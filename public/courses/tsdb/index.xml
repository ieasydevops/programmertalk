<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Overview | ProgrammerTalk</title>
    <link>https://meixinyun.github.io/programmertalk/courses/tsdb/</link>
      <atom:link href="https://meixinyun.github.io/programmertalk/courses/tsdb/index.xml" rel="self" type="application/rss+xml" />
    <description>Overview</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 09 Sep 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://meixinyun.github.io/programmertalk/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Overview</title>
      <link>https://meixinyun.github.io/programmertalk/courses/tsdb/</link>
    </image>
    
    <item>
      <title>Prometheus Tsdb 分析</title>
      <link>https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/promethesutsdb/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/promethesutsdb/</guid>
      <description>&lt;h2 id=&#34;数据模型&#34;&gt;数据模型&lt;/h2&gt;
&lt;h3 id=&#34;时间序列数据&#34;&gt;时间序列数据&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;    identifier -&amp;gt; (t0, v0), (t1, v1), (t2, v2), (t3, v3), ....
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;每个数据点都是一个由时间戳和值组成的元组。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Identifier&lt;/strong&gt;: metric name + set {(k,v)} &lt;br&gt;
其中，metric name 也可以标示为: &lt;strong&gt;name&lt;/strong&gt; = metric_name&lt;/p&gt;
&lt;p&gt;一个序列的数据点标示符（identifier）: 是由指标名称在加上一组唯一的多维度的标签集合构成。
典型的标示符结构如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    requests_total{path=&amp;quot;/status&amp;quot;, method=&amp;quot;GET&amp;quot;, instance=”10.0.0.1:80”}
    requests_total{path=&amp;quot;/status&amp;quot;, method=&amp;quot;POST&amp;quot;, instance=”10.0.0.3:80”}
    requests_total{path=&amp;quot;/&amp;quot;, method=&amp;quot;GET&amp;quot;, instance=”10.0.0.2:80”}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;指标名称也可以作为一组特殊的 ( k,v)，上述的标示符存储数据模型标示如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    {__name__=&amp;quot;requests_total&amp;quot;, path=&amp;quot;/status&amp;quot;, method=&amp;quot;GET&amp;quot;, instance=”10.0.0.1:80”}
    {__name__=&amp;quot;requests_total&amp;quot;, path=&amp;quot;/status&amp;quot;, method=&amp;quot;POST&amp;quot;, instance=”10.0.0.3:80”}
    {__name__=&amp;quot;requests_total&amp;quot;, path=&amp;quot;/&amp;quot;, method=&amp;quot;GET&amp;quot;, instance=”10.0.0.2:80”}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;数据模型立体分析&#34;&gt;数据模型立体分析&lt;/h3&gt;
&lt;p&gt;数据序列的数据模型，可基于一个二位坐标展开立体分析。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;series
  ^   
  │   . . . . . . . . . . . . . . . . .   . . . . .   {__name__=&amp;quot;request_total&amp;quot;, method=&amp;quot;GET&amp;quot;}
  │     . . . . . . . . . . . . . . . . . . . . . .   {__name__=&amp;quot;request_total&amp;quot;, method=&amp;quot;POST&amp;quot;}
  │         . . . . . . .
  │       . . .     . . . . . . . . . . . . . . . .                  ... 
  │     . . . . . . . . . . . . . . . . .   . . . .   
  │     . . . . . . . . . .   . . . . . . . . . . .   {__name__=&amp;quot;errors_total&amp;quot;, method=&amp;quot;POST&amp;quot;}
  │           . . .   . . . . . . . . .   . . . . .   {__name__=&amp;quot;errors_total&amp;quot;, method=&amp;quot;GET&amp;quot;}
  │         . . . . . . . . .       . . . . .
  │       . . .     . . . . . . . . . . . . . . . .                  ... 
  │     . . . . . . . . . . . . . . . .   . . . . 
  v
    &amp;lt;-------------------- time ---------------------&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;水平方向&lt;/strong&gt;: 时间以及该时间对应的垂直方向上的维度标示符构成的标示符空间&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;垂直方向&lt;/strong&gt;: 多个标签构成的维度空间&lt;/p&gt;
&lt;p&gt;由于prometheus的数据源，是周期性的采集一组时序数据的当前值。每个批量采集的实体对象称之为一个target，（比如，采集某个机器上的所有的基础监控指标）。因此，数据写入
模式是，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;垂直方向多维度并发写。
比如一次采集一个机器的磁盘，内存，网络，等多个维度的数据，这些数据对应的是同一时刻点上某个机器的指标值。从维度方向看，是并发的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多个target独立
多个target，即多个机器数据采集之间是相互独立的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;读写特性分析&#34;&gt;读写特性分析&lt;/h3&gt;
&lt;h4 id=&#34;写特性&#34;&gt;写特性&lt;/h4&gt;
&lt;p&gt;每秒采集数成千上万的监控对象，这些样本数据写入模式是完全垂直高度并发。分散的写入单个数据点到磁盘非常缓慢，需要实现按顺序写入更大的数据块。即便写入采用SSD磁盘，也只能每次写入4KiB 或更大的页，不能修改单个字节。写一个16字节的样本同写一个完整的4KiB 的页没有区别。 而且不仅存在
&lt;a href=&#34;https://en.wikipedia.org/wiki/Write_amplification&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;写入放大&lt;/a&gt;，很快会
&lt;a href=&#34;http://codecapsule.com/2014/02/12/coding-for-ssds-part-1-introduction-and-table-of-contents/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;损坏硬件&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;结论：

&lt;a href=&#34;https://fabxc.org/tsdb/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;顺序和批量写入是spinning disks和SSD的理想写入模式。 这是一个应该遵循的简单规则。&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;读特性&#34;&gt;读特性&lt;/h4&gt;
&lt;p&gt;如上文的分析，查询通用的场景：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;查询单个序列某个特定时间的数据&lt;/li&gt;
&lt;li&gt;在 N（N=～10000）个序列里查询一个单个的数据点&lt;/li&gt;
&lt;li&gt;在单个序列里查询几周的数据点&lt;/li&gt;
&lt;li&gt;甚至在10000个序列里查询几周的数据点&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此在我们的二维平面上，查询既不是完全垂直的，也不是水平的，而是二者的矩形组合。
Recording Ruel （即针对特定复杂聚合数据的查询做预计算），可以减轻已知的一些查询方面的问题，但是仍然不是临时查询（ad-hoc queries）的一个通用解决方案，这些查询也必须能很好的进行下去. 在理想情况下，相同序列的样本数据将会被顺序存储，这样一来我们便可以用尽可能少的读来扫描得到它们。 在上层，我们只需要知道这个序列可以访问的所有数据点的开始位置。&lt;/p&gt;
&lt;p&gt;结论：

&lt;a href=&#34;https://fabxc.org/tsdb/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;在将收集的数据写入磁盘的理想模式和为服务的查询操作提供更显著有效的存储格式之间显然存在着强烈的冲突。这是我们的时间序列数据库要解决的根本问题&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;h3 id=&#34;v2-及以前的解决方案&#34;&gt;V2 及以前的解决方案&lt;/h3&gt;
&lt;p&gt;V2 版本的核心点是基于 Per Series Per File. 即每个时间序列创建一个文件，此文件
会按顺序存放这个序列的所有采样数据。 由于时间序列数据的特性&lt;strong&gt;顺序和批量写&lt;/strong&gt; 是理想模式，因此，在写入引入了chunk 的概念。在内存中，构建了批量大小为1KiB的
chunk,当这个chunk 块被填满后，将他们追加到对应的序列文件中。
此方案，解决了大部分问题，并支持 Gorilla的高效压缩算法。&lt;/p&gt;
&lt;p&gt;数据写入示意图如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   ┌──────────┬─────────┬┬─────────┬─────────┐           series A
   └──────────┴─────────┴┴─────────┴─────────┘
          ┌──────────┬─────────┬┬─────────┬─────────┐    series B
          └──────────┴─────────┴┴─────────┴─────────┘ 
                              . . .
 ┌──────────┬─────────┬─────────┬─────────┬┬─────────┐   series XYZ
 └──────────┴─────────┴─────────┴─────────┴┴─────────┘ 
   chunk 1    chunk 2   chunk 3     ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;V2 版本的数据结构存在的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;序列分流&amp;rdquo;（series churn）会导致维护的文件数据多于实际采集数据的序列数量。随着时间的推移，最终导致 inode 耗尽的问题&lt;/li&gt;
&lt;li&gt;即便做了分块，每秒也会产生数以千计的数据块并且准备好被持久化。这仍然需要每秒完成几千次单独的磁盘写操作。尽管，可以通过批量写多个chunks 来缓解部分压力，但这发而会增加等待持久化数据的总内存占用量&lt;/li&gt;
&lt;li&gt;保持打开所有文件来读取和写入是不可行的。特别是因为在24小时后超过99%的数据便不再会被查询。如果它还是被查询到的话，我们就不得不打开数千个文件，查找和读取相关的数据点到内存，然后再重新关闭它们。而这样做会导致很高的查询延迟。&lt;/li&gt;
&lt;li&gt;过期数据的删除，带来写放大。旧数据必须得被清理掉，而且数据需要从数百万的文件前被抹除。这意味着删除实际上是写密集型操作。此外，循环地在这数百万的文件里穿梭然后分析它们，会让这个过程常常耗费数个小时。在完成时有可能还需要重新开始。删除旧文件将会给你的SSD带来进一步的写入放大！&lt;/li&gt;
&lt;li&gt;应用服务崩溃，引发数据丢失，基于checkpoint 恢复，也会带来漫长的启动周期。
当前堆积的数据块只能放在内存里。如果应用崩溃的话，数据将会丢失。为了避免这种情况，它会定期地保存内存状态的检查点（Checkpoint）到磁盘，这可能比我们愿意接受的数据丢失窗口要长得多。从检查点恢复估计也会花上几分钟，造成痛苦而漫长的重启周期。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;每个时间序列对应一个单个文件的方式使得单个查询很容易就击垮Prometheus的进程。而当所要查询的数据没有缓存到内存时，被查询序列的文件会被打开，然后包含相关数据点的数据块会被读取到内存里。倘若数据量超过了可用内存，Prometheus会因为OOM被杀死而退出。&lt;/p&gt;
&lt;h3 id=&#34;v3-版本的解决方案&#34;&gt;V3 版本的解决方案&lt;/h3&gt;
&lt;h4 id=&#34;v3-宏观设计&#34;&gt;V3 宏观设计&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ tree ./data
./data
├── b-000001
│   ├── chunks
│   │   ├── 000001
│   │   ├── 000002
│   │   └── 000003
│   ├── index
│   └── meta.json
├── b-000004
│   ├── chunks
│   │   └── 000001
│   ├── index
│   └── meta.json
├── b-000005
│   ├── chunks
│   │   └── 000001
│   ├── index
│   └── meta.json
└── b-000006
    ├── meta.json
    └── wal
        ├── 000001
        ├── 000002
        └── 000003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;先看下V3 宏观的数据结构。有一些编号的Block，都有一个 b- 前缀，每个Block ,都维护一个包含索引的文件以及一个包含 chunks 目录，chunks 目录包含了多个 chunk 文件。chunk 文件类似V2 版本的 chunk，这样可以用非常低的成本来读取一个时间窗口里的序列数据，并且允许我们采用相同的有效压缩算法，很显然，这里不再是每个序列对应一个单个文件，取而代之的是，几个文件包含许多 序列的 chunks。&amp;ldquo;index&amp;quot;文件，包含了大量的黑魔法，允许我们找出标签，它们可能的值，整个时间序列，以及存放数据的 chunk 。&lt;/p&gt;
&lt;p&gt;关键的两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为何有几个目录包含 index 和 chunk 的文件布局呢？&lt;/li&gt;
&lt;li&gt;为何最后一个包含一个 wal 目录？&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;很多小的数据库many-little-databases&#34;&gt;很多小的数据库（Many Little Databases）&lt;/h4&gt;
&lt;p&gt;将序列数据的水平维度，即时间空间分割成非重叠的block。每个block当成一个完全独立的数据库，包含其时间窗口的所有时间序列数据。因此，它有自己的索引和一组chunk 文件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../../tsdb/chapter2/prometheustsdb/images/littledatabase.png&#34; alt=&#34;littledatabase&#34;&gt;&lt;/p&gt;
&lt;p&gt;每个block 的数据是不可变的。当然，对于最新采集的数据，我们必须能够将对应的序列和采样数据写入到最近的block里。对于最新的block , 所有的新数据被写入在内存数据库
中，它像持久化的block 一样提供了查询的特性。内存数据结构能更有效的更新。为了防止数据丢失，所有传入的数据也会写入一个临时的WAL，即“wal”目录中的一组文件，在重新启动时可以从中重新填充内存中的数据库。&lt;/p&gt;
&lt;p&gt;所有这些文件都带有它们自己的序列化格式，这与我们所期望的一样：许多标志、偏移量、变量和CRC32校验和。&lt;/p&gt;
&lt;p&gt;这种布局允许我们对与查询的时间范围相关的所有 block 发出查询.每个 block 的部分结果合并在一起，形成整体结果。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;This layout allows us to fan out queries to all blocks relevant to the queried time 
range. The partial results from each block are merged back together to form the 
overall result.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这种水平分区添加了一些强大的功能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;当查询一个时间范围时，我们可以很容易地排除掉在次时间区间之外的block.
简单的解决了序列分流(seriers churn) 的问题&amp;ndash;因为减少了数据集的起始时间。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当完成一个block ,就可以 通过顺序的写入几个大文件来持久化 内存中的数据库，
从而避免了写放大，并且对 SSDs 盘 和 HDDs 盘一样友好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;保留了 V2 版本中好的一个特性，即最近查询较多的 chunks, 作为热数据而存内存中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更棒的是，不用在绑定固定的1KiB 的 chunk 大小来以更好做磁盘的数据对齐。可以选择对对个数据点和选择的压缩格式最匹配的任何大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;删除过期数据变得更廉价和及时。在之前，删除过期数据，必须要分析多达数亿个文件，
可能需要几小时才能聚合，而现在只需要删除当一个目录就可以。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;v3-版本中的关键技术点&#34;&gt;V3 版本中的关键技术点&lt;/h2&gt;
&lt;h3 id=&#34;mmap&#34;&gt;mmap&lt;/h3&gt;
&lt;p&gt;从上百万个小文件到几个大文件的转化，让我们用很小的开销就能保持所有的文件打开。这使我们解锁了 
&lt;a href=&#34;&#34;&gt;mmap（2)&lt;/a&gt;的使用。mmap 是一个系统调用，可以通过虚拟内存区建立到磁盘文件的映射，可以简单看作一个交换空间。
这意味着我们可以将数据库中的所有内容视为内存中的内容，而不占用任何物理RAM。只有当我们访问数据库文件中的某些数据时，操作系统才会从磁盘做数据懒加载。&lt;/p&gt;
&lt;p&gt;在机器的内存资源充足的时，prometheus 可以积极的将数据缓存到内存中，在内存紧张，
其它应用需要时内存时候，prometheus缓存的数据可以被驱逐出去，将内存返回给其它应用。直接将持久化数据装入RAM 比这种查询更多持久化的数据更容易造成我们进程的OOM。
内存的缓存大小变得完全自适应，只有真正查询时才会加载数据。&lt;/p&gt;
&lt;h3 id=&#34;压缩compaction&#34;&gt;压缩(Compaction)&lt;/h3&gt;
&lt;p&gt;存储服务，周期的切割出一个新的block, 并将前一个block （当前已经完成的）写入磁盘。只有当block 成功持久化后，用于还原内存block 的WAL 才会被删除。&lt;/p&gt;
&lt;p&gt;为了避免在内存中积累过多的数据，我们希望每个block都保持合理的大小.(通常默认2小时)
当查询多个block时候，我们需要将结果合并到一个整体结果中。这个合并过程必然要付出代价。一周范围的查询，合并的block不应超过80+个部分结果。&lt;/p&gt;
&lt;p&gt;为了兼顾二者,（ 1. 内存block太大就会造成索引占用过多内存，2. block 块太小，会导致查询聚合的block 过多）我们引入了压缩，Compaction.&lt;/p&gt;
&lt;p&gt;Compaction 描述了获取一个或多个数据block,并将其写入一个可能更大的数据Block的过程。在此期间，可以修改现有数据，比如删除，或重构样本的chunk 来提升查询性能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../../tsdb/chapter2/prometheustsdb/images/compactionAB.png&#34; alt=&#34;compactionAB&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图，可以有两种策略的压缩，chunk [1,2,3,4] 有序，
一种将 [1,2],[3,4] 做合并，或者 [1,2,3],[4]  合并&lt;/p&gt;
&lt;p&gt;合并后，显著的降低了查询时的聚合成本，因为需要合并的部分结果更少。&lt;/p&gt;
&lt;h3 id=&#34;保留策略retention&#34;&gt;保留策略(Retention)&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt; ┌────────────┐  ┌────┼─────┐  ┌───────────┐  ┌───────────┐
 │ 1          │  │ 2  |     │  │ 3         │  │ 4         │   . . .
 └────────────┘  └────┼─────┘  └───────────┘  └───────────┘
                      |
                      |
             retention boundary

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;V2 版本中，删除过期数据会对CPU，内存，磁盘都带来压力，在此版本中，删除变得很简单，只需要删除过期的目录即可。如上图，block 1 可以删除，block 2 需要等到过期时间完成覆盖block 后才可以删除。&lt;/p&gt;
&lt;p&gt;由于我们持续做Compaction, 当数据越旧，block 可能变得越大。因此，必须设置一个上限，防止block 不会扩展到整个数据库，而丧失我们设计初衷带来的好处。&lt;/p&gt;
&lt;p&gt;这个值究竟设置多大何合适呢？当将最大块大小设置为总保留时间窗口的10%。
如上图中，block 2，总的block 大小设置为保留窗口的 10% ，在保留窗口外部的block大小也被限制在了10% 以内。&lt;/p&gt;
&lt;p&gt;简而言之，通过block 的方式，将非常昂贵的保留删除操作变得非常廉价。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;note: 在内存中批量处理数据，通过WAL 追踪，周期的刷入磁盘，定期合并的模式
非常普遍。
无论数据的领域细节如何，我们看到的好处几乎普遍适用。遵循这种方法的开源数据库
如：LevelDB,Cassandra,Influxdb,Hbase等。
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;索引index&#34;&gt;索引(Index)&lt;/h3&gt;
&lt;h4 id=&#34;动机&#34;&gt;动机&lt;/h4&gt;
&lt;p&gt;研究存储改进的最初动机来自 &amp;ldquo;序列流失&amp;rdquo; (Seriers Chrun) 带来的问题。
基于 block 的布局减少了服务查询时必须考虑的序列总数&amp;ndash;因为大部分的查询总是对近期数据的查询，然而跨越整个时间范围的查询仍然很慢。
从时间序列查询的特性分析可知，大部分的查询场景需要： 一个更有效的倒排索引。&lt;/p&gt;
&lt;h4 id=&#34;查询复杂度分析&#34;&gt;查询复杂度分析&lt;/h4&gt;
&lt;p&gt;倒排索引，提供了基于数据项的子集快速查找数据项的能力。举例来说，如果想查询带有标签
app = &amp;ldquo;nginx&amp;quot;的序列，不需要遍历所有的序列，挨个检查是否他们是否包含这个标签，
而只是维护一个 以 app = &amp;ldquo;nginx&amp;rdquo; 这个标签的一个集合，判断给定的序列是否在这个集合内。&lt;/p&gt;
&lt;p&gt;为此，每个序列都被分配了一个唯一的ID，通过这个ID可以在恒定的时间内检索它，即
O(1)。在这种情况下，ID 是我们的 正排索引。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: If the series with IDs 10, 29, and 9 contain the label app=&amp;quot;nginx&amp;rdquo;, the inverted index for the label “nginx” is the simple list [10, 29, 9], which can be used to quickly retrieve all series containing the label. Even if there were 20 billion further series, it would not affect the speed of this lookup.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果n是序列的总数，m是给定查询的结果大小，那么使用索引的查询的复杂性现在是O(m)。查询按其检索的数据量（m）而不是搜索的数据体（n), m 通常比 n 要小得多.&lt;/p&gt;
&lt;h5 id=&#34;组合标签问题&#34;&gt;组合标签问题&lt;/h5&gt;
&lt;p&gt;如何我们的序列总数为N，需要查询的M个标签的组合条件，通常的算法复杂度为 O(N^N),
假设M个标签&lt;/p&gt;
&lt;p&gt;问题:
若我们需要查询M个标签组合是否在N个序列中，通常的算法复杂度，O(N^M),如果有倒排索引，则变为O(M^M),最差情况，每个M的集合和N重合，则
O(N^N)。&lt;/p&gt;
&lt;p&gt;解决方案:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;如何我们的序列是 ID 是全局有序的,就会变的很有趣
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假设：我们需要查找服务名为 foo 的 http 请求数的指标，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; __name__ =  &amp;quot;requests_total &amp;quot;  AND app = &amp;quot;foo&amp;quot;,

 这两个标签对应的倒排索引如下图：
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;__name__=&amp;quot;requests_total&amp;quot;   -&amp;gt;   [ 9999, 1000, 1001, 2000000, 2000001, 2000002, 2000003 ]
     app=&amp;quot;foo&amp;quot;              -&amp;gt;   [ 1, 3, 10, 11, 12, 100, 311, 320, 1000, 1001, 10002 ]

             intersection   =&amp;gt;   [ 1000, 1001 ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;他们的交集会非常小。上述问题转变为在两个集合中，找到交集。我们可以通过在每个列表的开头设置一个光标，并总是在较小的数字处前进一个来找到它。当两个数字相等时，我们将数字加到结果中，并向前推进两个游标。
这种方式，总的时间成本为 O（2n）= O (n),因为我们只在其中一个列表中前进。&lt;/p&gt;
&lt;p&gt;以上方案，推广到M个标签组合场景，最坏情况下的时间复杂度为 O(M*N),
而之前是 O（M^N）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么不对标签进行压缩？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;以上的规程其实是规范搜索引擎的一个简化版本，几乎所有的全文搜索引擎都在用这种方式。
每个序列描述符都被视为一个简短的“文档”，每个标签（名称+固定值）都被视为其中的一个“单词”。我们可以忽略搜索引擎索引中通常会遇到的许多附加数据，例如单词位置和频率数据。&lt;/p&gt;
&lt;p&gt;对于改善实际运行时的方法，似乎有着无尽的研究，通常会对输入数据做出一些假设。有很多技术可以压缩倒排索引，这些倒排索引都有各自的优点和缺点。由于我们的“文档”很小，而且“单词”在所有系列中都有很大的重复性，压缩变得几乎无关紧要。例如，一个拥有约440万个系列、约12个标签的真实数据集的唯一标签不到5000个。对于我们最初的存储版本，我们坚持基本的方法，不进行压缩，只添&lt;strong&gt;加了一些简单的调整，以跳过大范围的非相交id。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;虽然保持id的排序听起来很简单，但它并不总是保持不变的。例如，V2存储将序列的Hash作为id分配给新序列，因此无法有效地构建排序的反向索引。&lt;/p&gt;
&lt;p&gt;另一项艰巨的任务是&lt;strong&gt;在数据被删除或更新时修改磁盘上的索引&lt;/strong&gt;。通常，最简单的方法是简单地重新计算和重写它们，但是这样做的同时需要保持数据库的可查询性和一致性。
V3存储正是通过为每个块提供一个单独的不可变索引来实现这一点的，该索引只在压缩时通过重写进行修改。只需要更新完全保存在内存中的可变块的索引。&lt;/p&gt;
&lt;h2 id=&#34;扩展阅读&#34;&gt;扩展阅读&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://fabxc.org/tsdb/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Writing a Time Series Database from Scratch&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>内存模型</title>
      <link>https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/memorylayout/memorylayout/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/memorylayout/memorylayout/</guid>
      <description>&lt;h2 id=&#34;目标&#34;&gt;目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;核心类图&lt;/li&gt;
&lt;li&gt;设计思路分析&lt;/li&gt;
&lt;li&gt;客户端视角-数据的写入流程&lt;/li&gt;
&lt;li&gt;DB初始化流程&lt;/li&gt;
&lt;li&gt;关键内存对象分析&lt;/li&gt;
&lt;li&gt;总结&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;核心类图&#34;&gt;核心类图&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram
Appender &amp;lt;|-- headerAppender : Implementation

&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Appender
Appender : Add(l labels.Labels, t int64, v float64) (uint64, error)
Appender : AddFast(ref uint64, t int64, v float64) error
Appender : Commit() error
Appender : Rollback() error


class DB {
  &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
  ...
	lockf fileutil.Releaser
	metrics   *dbMetrics
	opts      *Options
  ...
  compactor Compactor
  blocks []*Block
	head *Head
  ...
}


class Head {
  &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
  ...
	wal        *wal.WAL
	series     *stripeSeries
  ...
	symbols    map[string]struct
  ...
  ref          uint64
	lset         labels.Labels
  ...
}


class memSeries {
  &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
  ...
	chunks       []*memChunk
	headChunk    *memChunk
  ...
	app    chunkenc.Appender
  ...
  postings *index.MemPostings
  ...
}



class dbAppender {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  Appender
  db *DB
}


class  stripeSeries {
  &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
  series [stripeSize]map[uint64]*memSeries
	hashes [stripeSize]seriesHashmap
	locks  [stripeSize]stripeLock
}



class  Block {
  &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
  ...
  meta BlockMeta
	chunkr     ChunkReader
	indexr     IndexReader
	tombstones TombstoneReader
  ...
}


memSeries --* stripeSeries

stripeSeries --* Head

Block --* DB
Head --* DB
Appender --* dbAppender
DB --* dbAppender

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;设计思路分析&#34;&gt;设计思路分析&lt;/h2&gt;
&lt;p&gt;Appender 实现对时间序列采样数据写操作的抽象和封装，提供Add/AddFast/Commit/Rollback的核心能力&lt;/p&gt;
&lt;p&gt;通过DB 这个类，实现对落在某个时间窗口内的时间序列数据的读写操作。
通过 DB + Appender 的组合，就能提供时序数据库的写入逻辑，
可以通过 BD + Query 的组合，完成时间序列数据库的读逻辑。
可以看到，系统的顶层设计非常优秀，秉持了高内聚，低耦合的原则。&lt;/p&gt;
&lt;p&gt;DB内部通过Head,Block,Compactor 等关键几个抽象类，完成数据内存模型的的封装。
Head 及其相关的类，实现了时间序列数据在内存中的读写，存储。
Block实现了数据块和磁盘文件目录的映射，而Compactor抽象了数据写入磁盘的逻辑，包括，写入计划，写操作，以及压缩合并。具体是通过 leavelCompactor 实现。&lt;/p&gt;
&lt;p&gt;headerAppender 是数据写入内存的主要承担者，通过 stripeSeries 来构建全局序列缓存，通过类似分段Hash 的方式，降低读写锁竞争，提升效率。
单个序列的内存模型是memSeries, 单个序列的数据最终是要落盘的，对应了时间序列数据的真实data的存储，
通过chunk，以及mmapchunk的抽象，实现内存chunk和磁盘chunk的映射，并通过 mmap的机制，实现chunk
数据的懒加载。
对于chunk 的写入，抽象出了 chunk appender ,并通过 XOR chunkappend 的实现类，实现了 gorilla 的
delta of delta 的压缩算法，完成对数据的压缩存储。而且这部分也是低耦合的，只要我们轻易的增加一个
新的 chunkappend 实现累，就可以做到灵活的切换到其它的压缩算法上去。&lt;/p&gt;
&lt;p&gt;数据采样点，在写入内存的同时，会创建一个全局唯一的ID，作为正排索引，提高我们数据查询效率。序列写入的同时，会构建全局的标签键值对的符号表，每个标签对应那些Value，以及 倒排索引。核心的倒排索引数据结构是
*index.MemPostings, 核心的Map结构是一个大的二层Map，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Map &amp;lt;LabelKey, Map &amp;lt;LabelValue,[]IDs&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一层Map的键是序列中的标签名称，第二层的Map 的键是 第一层标签对应的Value，而值则是有该标签键，值对
对应的所有的序列ID列表，而且每次更新时，都会保持ID是有序的。&lt;/p&gt;
&lt;p&gt;Head 的另一个核心对象是WAL，通过WAL解决了当数据尚未写入磁盘时，宕机引发的数据丢失问题。WAL在appedner Commit的时候被触发，实现了对Append的序列和采样数据的WAL.&lt;/p&gt;
&lt;h2 id=&#34;客户端视角-数据的写入流程&#34;&gt;客户端视角-数据的写入流程&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;流程图&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD

A[DBInit] --&amp;gt;|new dbAppender| B(Appender)

B2(数据采集器) --&amp;gt;|DataSamples InComming| B

B --&amp;gt; C{Has ID}

C --&amp;gt;|No| D[Add]

D --&amp;gt;|Return ID| C

C --&amp;gt;|Yes| E[AddFast]

E --&amp;gt; F{Commit}

F--&amp;gt;|Yes|G[appender Commit]

G --&amp;gt; |失败|G1(Rollback) 

&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;代码实现&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2.1 DB 初始化&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
storage, err := tsdb.Open(dir, l, nil, &amp;amp;tsdb.Options{
		RetentionDuration: 15 * 24 * 60 * 60 * 1000, // 15 days in milliseconds
		BlockRanges:       tsdb.ExponentialBlockRanges(2*60*60*1000, 5, 3),
	})

storage 即一个DB的实例

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.2 数据摄入完成，创建一个新的Appender&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例化一个 Appender
app := storage.Appender()

// Appender 创建一个新的Appender 实例，这个实例和给定的db 是绑定的
func (db *DB) Appender() Appender {
	return dbAppender{db: db, Appender: db.head.Appender()}
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.3 写入数据&lt;/p&gt;
&lt;p&gt;对于一个新的序列调用 Add, 对于已知的序列，调用AddFast&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if s.ref == nil {

	  ref, err := app.Add(s.labels, ts, float64(s.value))
         s.ref = &amp;amp;ref

	...
} else {

    app.AddFast(*s.ref, ts, float64(s.value)

    ...

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.4 提交/回滚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;app.Commit() Or  app.Rollback()

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;服务端视角-db初始化&#34;&gt;服务端视角-DB初始化&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;caller=db.go:571 msg=&amp;quot;DBInit 1,打开文件，初始化存储目录,log,Block Ragne等，修复坏的索引版本&amp;quot;
caller=db.go:573 msg=&amp;quot;DBInit 2,初始化DB实例&amp;quot;
caller=db.go:575 msg=&amp;quot;DBInit 3,监控指标初始化&amp;quot;
caller=db.go:586 msg=&amp;quot;DBInit 4,DB 文件加锁&amp;quot;
caller=db.go:599 msg=&amp;quot;DBInit 5,创建一个LeveledCompactor&amp;quot;
caller=db.go:609 msg=&amp;quot;DBInit 6,Wal Log初始化&amp;quot;
caller=db.go:626 msg=&amp;quot;DBInit 7,创建 Head&amp;quot;
caller=db.go:633 msg=&amp;quot;DBInit 8,db reload&amp;quot;
caller=db.go:640 msg=&amp;quot;DBInit 9,db blocks 初始化，获取最小的有效时间&amp;quot;
caller=db.go:650 msg=&amp;quot;DBInit 10,db 核心内存结构Head 初始化&amp;quot;
caller=db.go:660 msg=&amp;quot;DBInit 11,启动db主线程&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，步骤10 最为核心，10中完成了时间序列内存正排，倒排索引数据结构的初始化，让我们进入其中，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;caller=head.go:646 msg=&amp;quot;HeadInit 1，重放磁盘的WAL和磁盘内存可映射chunk（如果有的话)&amp;quot;
caller=head.go:659 msg=&amp;quot;HeadInit 1.1，获取WAL的最后一个checkpoint&amp;quot;
caller=head.go:686 msg=&amp;quot;HeadInit 1.2，获取WAL的最后一个Segment&amp;quot;
caller=head.go:693 msg=&amp;quot;HeadInit 1.3，从最近的checkpoint 回填segment&amp;quot;
caller=head.go:702 msg=&amp;quot;HeadInit 1.4，加载WAL&amp;quot;
caller=head.go:713 msg=&amp;quot;WAL replay completed&amp;quot; duration=744.825µs

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;关键内存对象分析&#34;&gt;关键内存对象分析&lt;/h2&gt;
&lt;p&gt;内存数据对象主要关注两个维度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单个时间序列的数据如何存储和查找？&lt;/li&gt;
&lt;li&gt;多个数据序列数据如何缓存，倒排索引如何构建？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先看单个时间序列数据模型&lt;/p&gt;
&lt;h3 id=&#34;内存序列对象-memseries&#34;&gt;内存序列对象 memSeries&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram

class memSeries {
  &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
  sync.Mutex
  
  ref          uint64         //全局唯一ID
  lset         labels.Labels  //标签集合
  chunks       []*memChunk    //chunk 的映射集合
  headChunk    *memChunk    
  chunkRange   int64
  firstChunkID int
  nextAt        int64  // 生成下一个chunk 的切割时间点
  sampleBuf     [4]sample
  app chunkenc.Appender

  appendable(t int64, v float64) error 
  append(t int64, v float64) (success, chunkCreated bool)
  cut(mint int64) *memChunk

  chunk(id int) *memChunk
  truncateChunksBefore(mint int64) (removed int) 
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;内存序列的，数据模型如上图，核心的概念是chunk, 提供的append 和 cut 等操作，新的时间采样点数据来后，会append,
等到一定的时间戳口就被cut 或 truncate 。chunk之间通过位数固定的步长，chunkRange,通过chunkID 和chunkRange 可以
非常快速的定位到所需查询的chunk。&lt;/p&gt;
&lt;h3 id=&#34;内存序列对象-chunk&#34;&gt;内存序列对象 chunk&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram
Appender &amp;lt;|-- xorAppender : 实现
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Appender
Appender : Append(t int64, v float64)

Iterator &amp;lt;|-- xorIterator : 实现
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Iterator
Iterator : At() (int64, float64)
Iterator : Err() error
Iterator : Next() bool



Chunk &amp;lt;|-- XORChunk : 实现
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Chunk
Chunk : Bytes() []byte
Chunk : Encoding() Encoding
Chunk : Appender() (Appender, error)
Chunk : Iterator(Iterator) Iterator
Chunk : NumSamples() int

class XORChunk {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  b bstream
  Encoding()
  Appender()
  Iterator(it Iterator)
}

XORChunk --&amp;gt; xorIterator
XORChunk --&amp;gt; xorAppender

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;chunk 是对单个序列，在段的时间维度上数据模型的抽象。提供的核心功能是Append 和  Iterator. 其中，XORChunk 实现了
gorila 的数据压缩算法。&lt;/p&gt;
&lt;p&gt;在看多个时间序列数据模型&lt;/p&gt;
&lt;h3 id=&#34;内存序列对象-head&#34;&gt;内存序列对象 Head&lt;/h3&gt;
&lt;h4 id=&#34;head-的类图结构&#34;&gt;Head 的类图结构&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram

class Head {
  &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
  ...
	wal        *wal.WAL 
  ...
  series *stripeSeries
  ...
  symbols map[string]struct
  values  map[string]stringset // Label names to possible values.
  ...
  postings *index.MemPostings // Postings lists for terms.
  tombstones *tombstones.MemTombstones
  ...
  chunkDiskMapper *chunks.ChunkDiskMapper

  Appender() storage.Appender // 初始化hadderAppender 并返回
  appender() *headAppender
  Chunks() (ChunkReader, error) // returns a ChunkReader against the block
  getOrCreate(hash uint64, lset labels.Labels) (*memSeries, bool, error)

}


&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;head-的核心对象及其关系&#34;&gt;Head 的核心对象及其关系&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram

class Head {
    &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
   
}


class headAppender {
    &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
    head         *Head
    sampleSeries []*memSeries
    series       []record.RefSeries
   	samples      []record.RefSample

}




class WAL
class stripeSeries
class MemPostings
class MemTombstones
class ChunkDiskMapper




Head --|&amp;gt; stripeSeries
Head --|&amp;gt; MemPostings
Head --|&amp;gt; MemTombstones
Head --|&amp;gt; ChunkDiskMapper


Head --* headAppender
memSeries --* headAppender

&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;各个主要对象的功能分析&#34;&gt;各个主要对象的功能分析&lt;/h4&gt;
&lt;p&gt;headAppender 组合了 内存序列对象(memSeries) 和 Head 对象，并实现了 Appender 接口，是时间序列数据写入逻辑
的核心承载者。
Head 通过 stripeSeries,MemPosting,MemTomstones，以及ChunkDiskMapper 完成以下子功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;全局有序的正排索引 -  stripeSeries&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram

class stripeSeries {
    &amp;lt;&amp;lt;class&amp;gt;&amp;gt;
    size                    int   //默认 2^14 个段
    series                  []map[uint64]*memSeries // 全局内存序列hash 表
    hashes                  []seriesHashmap  // 序列标签hash 值为Key，[]memSeries为Value,为了防止hash冲突
    locks                   []stripeLock  // 分段锁
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;stripeSeries 顾名思义，将序列分成了很多条带，降低锁竞争。 初始化时，默认将序列划分为 2^14 个条带。&lt;/p&gt;
&lt;p&gt;对于新增一个数据点，如何将该数据写入到内存中的呢？&lt;/p&gt;
&lt;p&gt;根据序列标签集计算Hash -&amp;gt; 根据Hash值到全局Hash表中 (stripeSeries) 获取对应的序列 -&amp;gt; 将数据写入单个
序列内存对象 memSeries 中。&lt;/p&gt;
&lt;p&gt;根据序列标签集获取Hash值的计算逻辑如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// lset.Hash(),基于采样点的所有标签构建Hash值
func (ls Labels) Hash() uint64 {
    b := make([]byte, 0, 1024)
    for _, v := range ls {
        b = append(b, v.Name...)
        b = append(b, sep)
        b = append(b, v.Value...)
        b = append(b, sep)
    }
    return xxhash.Sum64(b)
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据hash值，获取内存序列&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
s, created, err := a.head.getOrCreate(lset.Hash(), lset)
// 从全局分段序列中获取内存序列
func (h *Head) getOrCreate(hash uint64, lset labels.Labels) (*memSeries, bool, error) {
            ...
        s := h.series.getByHash(hash, lset)
            ...
}

从分段的HashMap中获取对应的内存序列

func (s *stripeSeries) getByHash(hash uint64, lset labels.Labels) *memSeries {

    i := hash &amp;amp; uint64(s.size-1)  // 获取对应stripe位置
    s.locks[i].RLock()  
    series := s.hashes[i].get(hash, lset)
    s.locks[i].RUnlock()

    return series
}


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于我们内存的序列范围可能非常大，为了提升效率，降低锁的竞争，将该序列分段，通过一次hash 可以定位到一个较小的区间，为了解决hash 冲突，维护了一个小的hashmap。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;倒排索引 - MemPostings&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;倒排索引 MemPostings,是个大的二层Map,标签名称Key，为第一层Map的Key，Value为第二层Map的Key，第二层Map的Value则为包含次标签的序列 ID的一个数组。&lt;/p&gt;
&lt;p&gt;简化的数据结构描述:  Map&amp;lt;LabelKey,Map&amp;lt;LabelValue,[]Ids&amp;gt;&lt;/p&gt;
&lt;p&gt;正是该倒排索引，解决了组合标签过滤查询时查询效率问题，查询性能从 O（M^N） - &amp;gt;  O( M*N ) 。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// MemPostings holds postings list for series ID per label pair.
// They may be written to out of order.
type MemPostings struct {
    mtx     sync.RWMutex
    m       map[string]map[string][]uint64  // 标签名称Key，为第一层Map的Key，Value为第二层Map的Key，第二层Map的Value则为包含次标签的序列 ID的一个数组。
    ordered bool
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;为了提升查询效率，数据在写入时，会时刻保持某个标签键值对所在的序列列表 IDs 是有序的。其实现逻辑也很简单，如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
 ...
    // id is the given ID
    list :=append(list,id)
    ...
    for i := len(list) - 1; i &amp;gt;= 1; i-- {
        if list[i] &amp;gt;= list[i-1] {
            break
        }
        list[i], list[i-1] = list[i-1], list[i]
}

&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;预写日志组件 WAL&lt;/p&gt;
&lt;p&gt;新增的序列，首先会写入内存对象，之后，当 append.Commit 时，会触发WAL写入。后面会有单独章节来分析WAL的实现。
值得一提的是，WAL针对 head 中的 Series 和 Sample 写Log&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;磁盘到内存的 chunk 映射 - chunkDiskMapper&lt;/p&gt;
&lt;p&gt;chunkDiskMapper 基于 mmap 的技术，实现来磁盘chunk到内存chunk  的映射. Head 的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
loadMmappedChunks() (map[uint64][]*mmappedChunk, error)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;方法，实现来数据的初始化。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;本文，从相对宏观的视角分析了 prometheus tsdb 的内存数据模型。&lt;/p&gt;
&lt;p&gt;1 . 整体的设计遵循高内聚，低耦合的方式原则。&lt;/p&gt;
&lt;p&gt;基于核心的三大领域模型，DB，Appender ,Query 实现。DB是对数据层封装，Appender 是对数据写入操作抽象，
Query是对数据查询操作抽象。Appender + DB 构成了tsdb 的总体写入框架&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;内存模型从两个角度看，单个序列数据存储模型由 memSeries 承载，多个序列存储模型有Head ,Block ,TomStone等承载。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据采样写入时候，构建了正排，索引。&lt;/p&gt;
&lt;p&gt;正向索引基于分段hash 的原理，减少锁竞争，通过二次 hashmap 解决hash冲突。
每个采样数据写入时，会判断是否为新增，若新增则会为期分配全局唯一的ID&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据采样写入时候，同时构建了全局的倒排索引，以及符号表&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;倒排索引是一个二层Map表, 结构为 &amp;lt;序列标签名称，&amp;lt;序列标签Value，此标签键值对所在的序列ID列表&amp;raquo;, 其中每个标签键值对对应的ID列表为有序的&lt;/p&gt;
&lt;p&gt;也就录了每个标签可能对应那些Value值。&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;通过 chunkDiskMapper 实现内存chunk 到磁盘chunk的映射&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过WAL 的技术解决在服务宕机时，部分尚未落入磁盘数据的恢复。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在后面的章节将分析，数据在磁盘的存储布局，以及内存数据到磁盘存的迁移过程。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>磁盘存储模型</title>
      <link>https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/disklayout/disylayout/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/disklayout/disylayout/</guid>
      <description>&lt;h2 id=&#34;目录&#34;&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;磁盘存储概览&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Block 存储布局&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chunk 存储布局&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Index 存储布局&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WAL 存储布局&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;磁盘存储结构概览&#34;&gt;磁盘存储结构概览&lt;/h2&gt;
&lt;h3 id=&#34;heading&#34;&gt;&lt;/h3&gt;
&lt;h2 id=&#34;各个组件存储布局&#34;&gt;各个组件存储布局&lt;/h2&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>WAL</title>
      <link>https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/wal/wal/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/wal/wal/</guid>
      <description>&lt;h2 id=&#34;目录&#34;&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;概览&lt;/li&gt;
&lt;li&gt;WAL管理&lt;/li&gt;
&lt;li&gt;WAL Read/Writer&lt;/li&gt;
&lt;li&gt;Log file 格式&lt;/li&gt;
&lt;li&gt;Record 文件格式&lt;/li&gt;
&lt;li&gt;格式的优缺点&lt;/li&gt;
&lt;li&gt;WAL的实现&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;概览&#34;&gt;概览&lt;/h2&gt;
&lt;p&gt;Write ahead log (WAL) 以日志文件的形式将内存表的操作顺序的持久化到存储介质上。在失败的时候，WAL文件 通过利用这些日志文件重建内存表，来恢复数据库到之前一致的状态。当一个内存表被安全的刷写到持久化介质后，对应的WLA日志会逐步的归档和淘汰，最终
归档的日志经过一个时间后会从磁盘清理掉。&lt;/p&gt;
&lt;h2 id=&#34;wal管理&#34;&gt;WAL管理&lt;/h2&gt;
&lt;p&gt;WAL文件是有在WAL目录下递增的序列号生成，为了能恢复到database的状态，这些文件需要按照序列号来读。WAL管理器，
提供了将WAL文件作为单个单元读取的抽象。在内部，它使用Reader或Writer抽象来打开和读取文件。&lt;/p&gt;
&lt;h2 id=&#34;wal-readerwriter&#34;&gt;WAL Reader/Writer&lt;/h2&gt;
&lt;p&gt;Writer 为向日志记录提供了一个抽象。存储媒介特定的内部细节，通过WriteableFile 接口屏蔽掉。
类似地，Reader提供了 从特定日志文件中顺序读取日志记录的抽象。内部存储媒介的详细信息有 SequentailFile 接口处理。&lt;/p&gt;
&lt;h2 id=&#34;log-file-格式&#34;&gt;Log File 格式&lt;/h2&gt;
&lt;p&gt;日志文件有一系列可变长度的记录组成。记录按kBlockSize分组。如果某个记录无法放入剩余空间，则剩余空间将填充null数据。
Writer 以kBlockSize为单位进行读/写&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;       +-----+-------------+--+----+----------+------+-- ... ----+
 File  | r0  |        r1   |P | r2 |    r3    |  r4  |           |
       +-----+-------------+--+----+----------+------+-- ... ----+
       &amp;lt;--- kBlockSize ------&amp;gt;|&amp;lt;-- kBlockSize ------&amp;gt;|

  rn = variable size records
  P = Padding

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;record-文件格式&#34;&gt;Record 文件格式&lt;/h2&gt;
&lt;p&gt;Record 的布局格式有两种： Legacy 和 Recyclable&lt;/p&gt;
&lt;h3 id=&#34;the-legacy-record-format&#34;&gt;The Legacy Record Format&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;+---------+-----------+-----------+--- ... ---+
|CRC (4B) | Size (2B) | Type (1B) | Payload   |
+---------+-----------+-----------+--- ... ---+

CRC = 32bit hash computed over the payload using CRC
Size = Length of the payload data
Type = Type of record
       (kZeroType, kFullType, kFirstType, kLastType, kMiddleType )
       The type is used to group a bunch of records together to represent
       blocks that are larger than kBlockSize
Payload = Byte stream as long as specified by the payload size

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;recyclable-record-format&#34;&gt;Recyclable Record Format&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;+---------+-----------+-----------+----------------+--- ... ---+
|CRC (4B) | Size (2B) | Type (1B) | Log number (4B)| Payload   |
+---------+-----------+-----------+----------------+--- ... ---+
Same as above, with the addition of
Log number = 32bit log file number, so that we can distinguish between
records written by the most recent log writer vs a previous one.

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;record-format-details-for-legacy-format&#34;&gt;Record Format Details For Legacy Format&lt;/h2&gt;
&lt;p&gt;日志文件内容是一个32KB块的序列。唯一的例外是文件的尾部可能包含部分块。&lt;/p&gt;
&lt;p&gt;每一个block 包含一系列记录组成:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
block := record* trailer?
record :=
  checksum: uint32	// crc32c of type and data[]
  length: uint16
  type: uint8		// One of FULL, FIRST, MIDDLE, LAST 
  data: uint8[length]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;记录永远不会在块的最后6个字节内开始（因为它不适合）。这里的任何剩余字节都构成了尾部，它必须完全由零字节组成，并且必须被读者跳过。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果当前的block 还剩下 7 个 bytes, 当新增一个非零长度的record时，写入的 writer 必须先发出一个记录(其中包含零字节
的用户数据)以填充block 尾部的 7 个 bytes ,然后在后续的block 中，发出用户的所有数据&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;用户的记录的数据类型如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FULL == 1
FIRST == 2
MIDDLE == 3
LAST == 4

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以后可以有更多的数据类型，一些 Readers 可能会跳过他们不理解的recored, 也有一些Readers 或报告 这些数据被忽略。&lt;/p&gt;
&lt;p&gt;Full record 或包含整个用户记录。&lt;/p&gt;
&lt;p&gt;FIRST, MIDDLE, LAST, 是用于用户记录的类型，这些类型被分割为多个片段（通常是因为block 的边界）。
FIRST: user record 的第一个片段的类型
LAST:  user record 的最后一个片段的类型
MID: 是用户记录的所有内部片段的类型。&lt;/p&gt;
&lt;p&gt;举例:&lt;/p&gt;
&lt;p&gt;一序列用户的 records&lt;/p&gt;
&lt;p&gt;A: length 1000&lt;/p&gt;
&lt;p&gt;B: length  97270&lt;/p&gt;
&lt;p&gt;C: length 8000&lt;/p&gt;
&lt;p&gt;A 会作为完整记录存储在第一个block中
B 会拆分成三个block. 第一个fragment 占据第一个block 剩余的部分
第二个 fragment 会占据第二个block 的所有
第三个 fragment 会占据第三个block 的前半部分，这将在第三个block中留下6个bytes  的空闲空间，该block 作为
尾部留空
C 作为完整的记录存在第四个block 中&lt;/p&gt;
&lt;h2 id=&#34;优势&#34;&gt;优势&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;不需要任何启发式 resyncing  ， 只需要转到下一个block 边界 ，扫描。如果又损坏，请跳到
下一个block。作为一个附带的好处，当一个日志文件的部分内容作为recode 潜入到另一个日志文件中时，我们
不会感到困惑。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Splitting at approximate boundaries (e.g., for mapreduce) is simple:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;找到下一个block 并跳过记录，直到找到完整的或第一个记录为止。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;对于大的记录，我们不需要额外的缓冲。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;缺点:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;不能打包小的 records. 这个可以通过添加新的类型来解决，这是当前实现的一个缺点&lt;/li&gt;
&lt;li&gt;不能压缩。同样，这个可以通过增加记录的类型来解决这个问题&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;wal的实现&#34;&gt;WAL的实现&lt;/h2&gt;
&lt;h3 id=&#34;wla-文件格式&#34;&gt;WLA 文件格式&lt;/h3&gt;
&lt;p&gt;WAL  按编号和顺序的段操作&lt;/p&gt;
&lt;p&gt;·000000·
·000001·
·000002·
·000003·&lt;/p&gt;
&lt;p&gt;默认最大128M，&lt;/p&gt;
&lt;p&gt;以32KB的页数写入一个段。只有最近一段的最后一页可能是不完整的。WAL记录是一个不透明的字节片，如果超过当前页面的剩余空间，
,它被拆分成子记录。在 segment 边界，记录永远不会分开，如果单个记录超过默认段大小，则将创建更大尺寸的segment.&lt;/p&gt;
&lt;p&gt;Prometheus tsdb 的WAL格式如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;┌───────────┬──────────┬────────────┬──────────────┐
│ type &amp;lt;1b&amp;gt; │ len &amp;lt;2b&amp;gt; │ CRC32 &amp;lt;4b&amp;gt; │ data &amp;lt;bytes&amp;gt; │
└───────────┴──────────┴────────────┴──────────────┘



&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;类型标签又如下几种状态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;0&lt;/code&gt;: rest of page will be empty&lt;/li&gt;
&lt;li&gt;&lt;code&gt;1&lt;/code&gt;: a full record encoded in a single fragment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;2&lt;/code&gt;: first fragment of a record&lt;/li&gt;
&lt;li&gt;&lt;code&gt;3&lt;/code&gt;: middle fragment of a record&lt;/li&gt;
&lt;li&gt;&lt;code&gt;4&lt;/code&gt;: final fragment of a record&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;记录的编码格式&#34;&gt;记录的编码格式&lt;/h3&gt;
&lt;p&gt;分三种类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;序列记录&lt;/li&gt;
&lt;li&gt;样本记录&lt;/li&gt;
&lt;li&gt;Tombstone 类型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Series records encode the labels that identifies a series and its unique ID.&lt;/p&gt;
&lt;h4 id=&#34;序列记录&#34;&gt;序列记录&lt;/h4&gt;
&lt;p&gt;一个序列记录，会包含该序列的标签和唯一ID&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;┌────────────────────────────────────────────┐
│ type = 1 &amp;lt;1b&amp;gt;                              │
├────────────────────────────────────────────┤
│ ┌─────────┬──────────────────────────────┐ │
│ │ id &amp;lt;8b&amp;gt; │ n = len(labels) &amp;lt;uvarint&amp;gt;    │ │
│ ├─────────┴────────────┬─────────────────┤ │
│ │ len(str_1) &amp;lt;uvarint&amp;gt; │ str_1 &amp;lt;bytes&amp;gt;   │ │
│ ├──────────────────────┴─────────────────┤ │
│ │  ...                                   │ │
│ ├───────────────────────┬────────────────┤ │
│ │ len(str_2n) &amp;lt;uvarint&amp;gt; │ str_2n &amp;lt;bytes&amp;gt; │ │
│ └───────────────────────┴────────────────┘ │
│                  . . .                     │
└────────────────────────────────────────────┘
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;采集样本记录&#34;&gt;采集样本记录&lt;/h4&gt;
&lt;p&gt;采样数据样本记录 主要包含了 三元组 &lt;code&gt;（序列ID，时间戳，序列值Value）&lt;/code&gt;
序列的索引ID和时间戳  被编码为 w.r.t
第一个行存储了启始的ID 和启始的时间戳。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;┌──────────────────────────────────────────────────────────────────┐
│ type = 2 &amp;lt;1b&amp;gt;                                                    │
├──────────────────────────────────────────────────────────────────┤
│ ┌────────────────────┬───────────────────────────┐               │
│ │ id &amp;lt;8b&amp;gt;            │ timestamp &amp;lt;8b&amp;gt;            │               │
│ └────────────────────┴───────────────────────────┘               │
│ ┌────────────────────┬───────────────────────────┬─────────────┐ │
│ │ id_delta &amp;lt;uvarint&amp;gt; │ timestamp_delta &amp;lt;uvarint&amp;gt; │ value &amp;lt;8b&amp;gt;  │ │
│ └────────────────────┴───────────────────────────┴─────────────┘ │
│                              . . .                               │
└──────────────────────────────────────────────────────────────────┘
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;tombstone记录&#34;&gt;Tombstone记录&lt;/h4&gt;
&lt;p&gt;Tombstone records encode tombstones as a list of triples &lt;code&gt;(series_id, min_time, max_time)&lt;/code&gt;
and specify an interval for which samples of a series got deleted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;┌─────────────────────────────────────────────────────┐
│ type = 3 &amp;lt;1b&amp;gt;                                       │
├─────────────────────────────────────────────────────┤
│ ┌─────────┬───────────────────┬───────────────────┐ │
│ │ id &amp;lt;8b&amp;gt; │ min_time &amp;lt;varint&amp;gt; │ max_time &amp;lt;varint&amp;gt; │ │
│ └─────────┴───────────────────┴───────────────────┘ │
│                        . . .                        │
└─────────────────────────────────────────────────────┘
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;wla-顶层接口设计&#34;&gt;WLA 顶层接口设计&lt;/h3&gt;
&lt;p&gt;数据结构设计&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
// WAL is a write ahead log that stores records in segment files.
// It must be read from start to end once before logging new data.
// If an error occurs during read, the repair procedure must be called
// before it&#39;s safe to do further writes.
//
// Segments are written to in pages of 32KB, with records possibly split
// across page boundaries.
// Records are never split across segments to allow full segments to be
// safely truncated. It also ensures that torn writes never corrupt records
// beyond the most recent segment.
type WAL struct {
	dir         string
	logger      log.Logger
	segmentSize int
	mtx         sync.RWMutex
	segment     *Segment // Active segment.
	donePages   int      // Pages written to the segment.
	page        *page    // Active page.
	stopc       chan chan struct{}
	actorc      chan func()
	closed      bool // To allow calling Close() more than once without blocking.
	compress    bool
	snappyBuf   []byte

	metrics *walMetrics
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;段-segment&#34;&gt;段 Segment&lt;/h3&gt;
&lt;p&gt;默认128M，&lt;/p&gt;
&lt;h3 id=&#34;页-page&#34;&gt;页 Page&lt;/h3&gt;
&lt;p&gt;页是内存的buffer , 用来向磁盘批量刷数据。
如果 Records 比一个页大，此记录会被分割，并单独的刷写到磁盘。&lt;/p&gt;
&lt;p&gt;磁盘的刷写动作在如下情况被触发：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当一个记录不适合当前的page 大小&lt;/li&gt;
&lt;li&gt;或剩下的空间放不下下一个记录&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;
// page is an in memory buffer used to batch disk writes.
// Records bigger than the page size are split and flushed separately.
// A flush is triggered when a single records doesn&#39;t fit the page size or
// when the next record can&#39;t fit in the remaining free page space.
type page struct {
	alloc   int
	flushed int
	buf     [pageSize]byte
}


&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
