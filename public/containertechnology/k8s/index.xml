<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Overview | ProgrammerTalk</title>
    <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/</link>
      <atom:link href="https://meixinyun.github.io/programmertalk/containertechnology/k8s/index.xml" rel="self" type="application/rss+xml" />
    <description>Overview</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 09 Sep 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://meixinyun.github.io/programmertalk/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Overview</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/</link>
    </image>
    
    <item>
      <title>K8S体系架构</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/fundamentals/fundamentals/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/fundamentals/fundamentals/</guid>
      <description>&lt;h2 id=&#34;k8s-基本工作原理&#34;&gt;k8s 基本工作原理&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/fundamentals/k8s-work.png&#34; alt=&#34;k8s-work&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>作业任务系统</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/job/job/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/job/job/</guid>
      <description>&lt;h2 id=&#34;作业系统的背景及需求&#34;&gt;作业系统的背景及需求&lt;/h2&gt;
&lt;p&gt;K8s 里面，最小的调度单元是 Pod，可以直接通过 Pod 来运行任务进程，但面临以下几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们如何保证 Pod 内进程正确的结束？&lt;/li&gt;
&lt;li&gt;如何保证进程运行失败后重试？&lt;/li&gt;
&lt;li&gt;如何管理多个任务，且任务之间有依赖关系？&lt;/li&gt;
&lt;li&gt;如何并行地运行任务，并管理任务的队列大小？&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;k8s对作业系统的抽象-job&#34;&gt;K8s对作业系统的抽象-Job&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;kubernetes 的 Job 是一个管理任务的控制器，它可以创建一个或多个 Pod 来指定 Pod 的数量，并可以监控它是否成功地运行或终止；&lt;/li&gt;
&lt;li&gt;我们可以根据 Pod 的状态来给 Job 设置重置的方式及重试的次数；&lt;/li&gt;
&lt;li&gt;根据依赖关系，保证上一个任务运行完成之后再运行下一个任务；&lt;/li&gt;
&lt;li&gt;还可以控制任务的并行度，根据并行度来确保 Pod 运行过程中的并行次数和总体完成大小&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;k8s-作业系统&#34;&gt;K8s 作业系统&lt;/h2&gt;
&lt;h3 id=&#34;功能&#34;&gt;功能&lt;/h3&gt;
&lt;p&gt;restartPolicy解析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Never: Job 需要重新运行&lt;/li&gt;
&lt;li&gt;OnFailure: 失败的时候再运行，再重试可以用&lt;/li&gt;
&lt;li&gt;Always: 不论什么情况下都重新运行时&lt;/li&gt;
&lt;li&gt;backoffLimit: 就是来保证一个 Job 到底能重试多少次&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;架构设计&#34;&gt;架构设计&lt;/h3&gt;
&lt;h4 id=&#34;deamset-管理模式&#34;&gt;DeamSet 管理模式&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/job/jobcontrolmode.png&#34; alt=&#34;jobcontrolmode&#34;&gt;&lt;/p&gt;
&lt;!-- Job Controller 负责根据配置创建相对应的 pod
Job Controller 跟踪 Job 的状态，及时地根据我们提交的一些配置重试或者继续创建
Job Controller 会自动添加Label来跟踪对应的pod,并根据配置并行或串行创建Pod --&gt;
&lt;h4 id=&#34;deamset-控制器&#34;&gt;DeamSet 控制器&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/job/jobctroller.png&#34; alt=&#34;jobcontrolmode&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>云原生技术概览</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/cncfoverview/cncfoverview/</link>
      <pubDate>Sun, 05 May 2019 01:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/cncfoverview/cncfoverview/</guid>
      <description>&lt;h2 id=&#34;云原生技术发展历史&#34;&gt;云原生技术发展历史&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/cncfoverview/cncfhis.png&#34; alt=&#34;cncfhis&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;云原声技术范畴&#34;&gt;云原声技术范畴&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/cncfoverview/techoverview.png&#34; alt=&#34;techview&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;关键技术点&#34;&gt;关键技术点&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/cncfoverview/techkeypoint.png&#34; alt=&#34;techview&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>应用配管</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/appconf/appconf/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/appconf/appconf/</guid>
      <description>&lt;h2 id=&#34;问题的背景&#34;&gt;问题的背景&lt;/h2&gt;
&lt;p&gt;依托镜像定义运行的Container，Pod，还需要解决如下问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;不可变基础设施（容器）的可变配置&lt;/li&gt;
&lt;li&gt;敏感信息的存储和使用（如秘密，Token）&lt;/li&gt;
&lt;li&gt;集群中Pod自我的身份认证&lt;/li&gt;
&lt;li&gt;容器的运行安全管控&lt;/li&gt;
&lt;li&gt;容器启动前置条件校验&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram

资源配置 --&amp;gt; 容器
安全管控 --&amp;gt; 容器
前置校验 --&amp;gt; 容器

容器 --&amp;gt; 可变配置
容器 --&amp;gt; 铭感信息
容器 --&amp;gt; 身份认证

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;pod的配置管理&#34;&gt;POD的配置管理&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/podconf.png&#34; alt=&#34;k8s-work&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;配置文件详解&#34;&gt;配置文件详解&lt;/h2&gt;
&lt;h2 id=&#34;configmap&#34;&gt;ConfigMap&lt;/h2&gt;
&lt;h3 id=&#34;configmap-介绍&#34;&gt;ConfigMap 介绍&lt;/h3&gt;
&lt;p&gt;管理容器运行所需要的：
配置文件
环境变量
命令行参数
用于解耦，容器镜像和可变配置
保障 工作负载的可移植性&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind ConfigMap
metadata:
    labels:
      app: flannel
      tier: node
    name: kube-flannel-cfg
    namespace: kube-system
data:
  cni-conf.json: {
    &amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
    &amp;quot;delegate&amp;quot;: {
      &amp;quot;isDefaultGateway&amp;quot;: true
    }
  }
  net-conf.json: {
    &amp;quot;Network&amp;quot;:&amp;quot;172.27.0.0/16&amp;quot;,
    &amp;quot;Backend&amp;quot;: {
      &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot;
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/configmapintruducton.png&#34; alt=&#34;k8s-work&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;configmap-创建&#34;&gt;ConfigMap 创建&lt;/h3&gt;
&lt;p&gt;创建命令： kubectl create configmap [NAME][DATA]&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; kubectl create configmap kube-flannel-cfg --from-file=config-prod-container/configmap/cni-conf.json -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/configmapcreate.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;configmap-使用&#34;&gt;ConfigMap 使用&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/configmapuse.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;configmap-使用的注意事项&#34;&gt;ConfigMap 使用的注意事项&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/confattention.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;secrete&#34;&gt;Secrete&lt;/h2&gt;
&lt;h3 id=&#34;secrete-介绍&#34;&gt;Secrete 介绍&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/secrete.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;secrete-创建&#34;&gt;Secrete 创建&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/secretecreate.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;secrete-使用&#34;&gt;Secrete 使用&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/secreteused.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;secrete-注意事项&#34;&gt;Secrete 注意事项&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/secreteattantion.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;serviceaccount&#34;&gt;ServiceAccount&lt;/h2&gt;
&lt;h3 id=&#34;应用场景&#34;&gt;应用场景&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/serviceaccount.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;案例分析&#34;&gt;案例分析&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/serviceaccountexample.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;容器资源管理&#34;&gt;容器资源管理&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/resouce.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;podqos配置&#34;&gt;Pod(QoS)配置&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/podQos.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;security-context&#34;&gt;Security Context&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appconf/securecontext.png&#34; alt=&#34;k8s-configmap&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;initcontainer&#34;&gt;InitContainer&lt;/h2&gt;
&lt;h3 id=&#34;initcontainer介绍&#34;&gt;InitContainer介绍&lt;/h3&gt;
&lt;p&gt;InitContainer 和普通 container 的区别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;InitContainer 首先会比普通 container 先启动，并且直到所有的 InitContainer 执行成功后，普通 container 才会被启动&lt;/li&gt;
&lt;li&gt;InitContainer 之间是按定义的次序去启动执行的，执行成功一个之后再执行第二个，而普通的 container 是并发启动的&lt;/li&gt;
&lt;li&gt;InitContainer 执行成功后就结束退出，而普通容器可能会一直在执行。它可能是一个 longtime 的，或者说失败了会重启，这个也是 InitContainer 和普通 container 不同的地方&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;initcontainer-用途&#34;&gt;InitContainer 用途&lt;/h3&gt;
&lt;p&gt;用于普通Container启动前的初始化(如配置文件准备)或普通Container启动的前置条件检验&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>应用持久化</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/pv/pv/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/pv/pv/</guid>
      <description>&lt;h2 id=&#34;目标&#34;&gt;目标&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;k8s Volume 使用场景&lt;/li&gt;
&lt;li&gt;PVC/PV/StorageClass 基本操作和概念呢解析&lt;/li&gt;
&lt;li&gt;PVC/PV 设计与实现原理&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;存储基本概念介绍&#34;&gt;存储基本概念介绍&lt;/h2&gt;
&lt;h3 id=&#34;术语表&#34;&gt;术语表&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;术语&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;描述&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;简称&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Volumes&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;存储卷&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;V&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Pod Volumes&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pod存储卷&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PV&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Persistent Volumes&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;持久化卷&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PV&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;PersistentVolumeClaim&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;持久化存储卷申明&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PVC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;in-tree&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;网络存储实现的代码在k8s仓库中&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;in-tree&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;out-of-tree&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;网络存储实现, 通过从抽象接口将不通的存储&lt;br&gt; driver实现从k8s代码仓库中玻璃&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;out-of-tree&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;container storage interface&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;K8s社区后面对存储插件实现(out of tree)的官方推荐方式&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSI&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;持久化存储的业务场景&#34;&gt;持久化存储的业务场景&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;如果 pod 中的某一个容器在运行时异常退出，被 kubelet 重新拉起之后，如何保证之前容器产生的重要数据没有丢失？&lt;/li&gt;
&lt;li&gt;如果同一个 pod 中的多个容器想要共享数据，应该如何去做？&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pod-volumes-的三种类型&#34;&gt;Pod Volumes 的三种类型：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;本地存储，常用的有 emptydir/hostpath；&lt;/li&gt;
&lt;li&gt;网络存储：网络存储当前的实现方式有两种，一种是 in-tree，它的实现的代码是放在 K8s 代码仓库中的，随着k8s对存储类型支持的增多，这种方式会给k8s本身的维护和发展带来很大的负担；而第二种实现方式是 out-of-tree，它的实现其实是给 K8s 本身解耦的，通过抽象接口将不同存储的driver实现从k8s代码仓库中剥离，因此out-of-tree 是后面社区主推的一种实现网络存储插件的方式；&lt;/li&gt;
&lt;li&gt;Projected Volumes：它其实是将一些配置信息，如 secret/configmap 用卷的形式挂载在容器中，让容器中的程序可以通过POSIX接口来访问配置数据；&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pod-volumes-存在的问题&#34;&gt;Pod Volumes 存在的问题&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/pvbasic.png&#34; alt=&#34;pv basic&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;场景一：pod 重建销毁，如用 Deployment 管理的 pod，在做镜像升级的过程中，会产生新的 pod并且删除旧的 pod ，那新旧 pod 之间如何复用数据？&lt;/li&gt;
&lt;li&gt;场景二：宿主机宕机的时候，要把上面的 pod 迁移，这个时候 StatefulSet 管理的 pod，其实已经实现了带卷迁移的语义。这时通过 Pod Volumes 显然是做不到的；&lt;/li&gt;
&lt;li&gt;场景三：多个 pod 之间，如果想要共享数据，应该如何去声明呢？我们知道，同一个 pod 中多个容器想共享数据，可以借助 Pod Volumes 来解决；当多个 pod 想共享数据时，Pod Volumes 就很难去表达这种语义；&lt;/li&gt;
&lt;li&gt;场景四：如果要想对数据卷做一些功能扩展性，如：snapshot、resize 这些功能，又应该如何去做呢？&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pod-volumes-的解决方案--persistent-volumes&#34;&gt;Pod Volumes 的解决方案- Persistent Volumes&lt;/h3&gt;
&lt;p&gt;将存储和计算分离，通过不同的组件来管理存储资源和计算资源，然后解耦 pod 和 Volume 之间生命周期的关联
。这样，当把 pod 删除之后，它使用的PV仍然存在，还可以被新建的 pod 复用。&lt;/p&gt;
&lt;h3 id=&#34;persistent-volumes-的接口描述-pvc&#34;&gt;Persistent Volumes 的接口描述 PVC&lt;/h3&gt;
&lt;p&gt;用户在使用持久化存储时，真正关心的问题是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用的存储是可以被多个node共享还是只能单node独占访问(注意是node level而不是pod level)？&lt;/li&gt;
&lt;li&gt;只读还是读写访问？
而不关心，与存储相关的实现细节。
这就使得我们需要抽象一层接口层，屏蔽掉存储实现细节的信息。这就是PVC的来历&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;为什么需要设计PVC?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;职责分离。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  PVX中只用声明自己需要的存储大小，access mode (单node独占还是多node 共享，只读还是
  读写访问？)等业务真正关系的存储需求，PV和其对应的后段存储信息则交给cluster admin
  统一运维和管控，安全访问策略更容易控制。
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PVC简化了User对存储的需求，PV才是存储实际信息的载体&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  通过kube-controller-manager中的PersistentVolumeController将PVC和合适的PV bound
  到一起，从而满足实际的存储需求
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PVC类似接口，PV类似接口对应的实现&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- ![pv basic](../../../k8s/pv/whypvc.png) --&gt;
&lt;h3 id=&#34;pvc-和-pv的两种bound-关系&#34;&gt;PVC 和 PV的两种Bound 关系&lt;/h3&gt;
&lt;p&gt;正如前文分析，PV 和PVC 类似实现类和接口的关系，那么在实际中两者的关系就有静态绑定，和动态绑定两种
接下来，分析两种关系。&lt;/p&gt;
&lt;h4 id=&#34;static-volume-provisioning-静态bound&#34;&gt;Static Volume Provisioning (静态Bound)&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/staticprovisiong.png&#34; alt=&#34;pv basic&#34;&gt;&lt;/p&gt;
&lt;p&gt;实现过程&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;由集群管理员（cluster admin）事先去规划这个集群中的用户会怎样使用存储，它会先预分配一些存储，也就是预先创建一些 PV；&lt;/li&gt;
&lt;li&gt;然后用户在提交自己的存储需求（也就是 PVC）的时候，K8s 内部相关组件会帮助它把 PVC 和 PV 做绑定；3. 用户再通过 pod 去使用存储的时候，就可以通过 PVC 找到相应的 PV&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;静态绑定的方式，
需要先设置好模版，对资源池的划分比较粗糙。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;举例：如果用户需要的是 20G，然而集群管理员在分配的时候可能有 80G 、100G 的，但没有 20G的，这样就很
难满足用户的真实需求，也会造成资源浪费
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;dynamic-provisioning动态态bound&#34;&gt;Dynamic Provisioning(动态态Bound)&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/dynamicprovisiong.png&#34; alt=&#34;pv basic&#34;&gt;&lt;/p&gt;
&lt;p&gt;实现过程&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;集群管理员不预先创建PV，而提供模版文件（StorageClass)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; StorageClass 表示创建某一类型存储（块存储，文件存储等）所需的一些参数。用户在提交自身存储需求
 (PVC)时，在PVC中指定使用的存储模版（StorageClass）
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;集群中的管控组件，会结合 PVC 和 StorageClass 的信息, 动态的生成
用户所需的PV，将PVC和PV绑定后，Pod 即可以使用PV了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;通过 StorageClass 配置生成存储所需要的存储模板，再结合用户的需求动态创建 PV 对象，做到按需分
配，在没有增加用户使用难度的同时也解放了集群管理员的运维工作
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;pvcpvstorageclass-基本操作和概念解析&#34;&gt;PVC/PV/StorageClass 基本操作和概念解析&lt;/h2&gt;
&lt;h3 id=&#34;pod-volumes-使用&#34;&gt;Pod Volumes 使用&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/podvolumes.png&#34; alt=&#34;pv basic&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过 .spec.volumes 申明pod 的 volumes 信息&lt;/li&gt;
&lt;li&gt;通过 .spec.containers.volumesMounts 申明 container
如何使用pod 的volumes 信息&lt;/li&gt;
&lt;li&gt;通过 .spec.containers.volumesMounts.subPath 隔离不通容器在同一个
volumes上数据存储的路径，实现多个container 共享同一个volumes&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;static-volume-provisioning-案例分析&#34;&gt;Static Volume Provisioning 案例分析&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Cluster Admin &amp;amp; User&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/staticprovisiongcase.png&#34; alt=&#34;pv basic&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;管理员配置解析&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/clusteradminop.png&#34; alt=&#34;管理员配置解析&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;用户配置解析&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/userop.png&#34; alt=&#34;用户PV配置解析&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;dynamic-volume-provisioning-案例分析&#34;&gt;Dynamic Volume Provisioning 案例分析&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;系统管理员&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/dynamicclusteradminop.png&#34; alt=&#34;动态PV用户配置解析&#34;&gt;&lt;/p&gt;
&lt;p&gt;1.1 系统管理员不再预分配 PV，而只是创建一个模板文件 &amp;ndash; StorageClass&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  StorageClass 的信息：
  第一：provisioner, 即那个存储插件。
      存储插件provisioner对应创建PV的具体实现
  第二：参数。
      k8s创建存储的时候，需要指定的一些细节参数。如：regionld、zoneld、fsType 和它的类型，
      ReclaimPolicy：使用方使用结束、Pod 及 PVC 被删除后，这块 PV 应该怎么处理
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;用户如何用&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/dynamicuserop.png&#34; alt=&#34;动态PV用户配置解析&#34;&gt;&lt;/p&gt;
&lt;p&gt;2.1 PVC 新加一个字段-StorageClassName&lt;/p&gt;
&lt;p&gt;2.2 用户提交完PVC之后，K8s 集群中的相关组件就会根据PVC以及StorageClass动态生成 PV,并和当前PVC绑定&lt;/p&gt;
&lt;p&gt;2.3 之后用户在提交自己的 yaml 时，PVC找到PV，并把它挂载到相应的容器中&lt;/p&gt;
&lt;h3 id=&#34;pv-spec-重要字段解析&#34;&gt;PV Spec 重要字段解析&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/keywordsparser.png&#34; alt=&#34;动态PV用户配置解析&#34;&gt;&lt;/p&gt;
&lt;p&gt;用户在提交PVC的时候,最重要的两个字段 —— Capacity 和 AccessModes。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Capacity&lt;/em&gt;: 存储对象的大小；
&lt;em&gt;AccessModes&lt;/em&gt; : PV三种使用方式:
1. 单 node 读写访问；
2. 多个 node 只读访问，是常见的一种数据的共享方式；
3. 多个 node 上读写访问。&lt;/p&gt;
&lt;p&gt;在提交 PVC 后，k8s 集群中的相关组件是如何去找到合适的 PV 呢？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先它是通过为 PV 建立的 AccessModes 索引找到所有能够满足用户的 PVC 里面的 AccessModes 要求的 PV list，&lt;/li&gt;
&lt;li&gt;然后根据PVC的 Capacity，StorageClassName, Label Selector 进一步筛选 PV，&lt;/li&gt;
&lt;li&gt;最小适合原则筛选
如果满足条件的 PV 有多个，选择 PV 的 size 最小的，accessmodes 列表最短的 PV&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在PV 使用完毕后如何释放回收？&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ReclaimPolicy&lt;/em&gt;:常见的有二种方式:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;delete: PVC 被删除之后，PV 也会被删除；&lt;/li&gt;
&lt;li&gt;Retain: 就是保留，保留之后，后面这个 PV 需要管理员来手动处理。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;StorageClassName：动态 Provisioning 时必须指定的一个字段,来指定到底用哪一个模板文件来生成 PV；&lt;/p&gt;
&lt;p&gt;NodeAffinity：创建出来的PV，它能被哪些node去挂载使用，
其实是有限制的。然后通过 NodeAffinity 来声明对node的限制，这样其实对 使用该PV的pod调度也有限制。
pod 必须要调度到这些能访问 PV 的 node 上，才能使用这块 PV&lt;/p&gt;
&lt;h3 id=&#34;pv-状态图&#34;&gt;PV 状态图&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram

CreatePV --&amp;gt; Pending 
Pending --&amp;gt; Available 
Available --&amp;gt; Bound

Bound --&amp;gt; Released
Released --&amp;gt; Deleted
Released --&amp;gt; Failed

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解释：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先在创建PV对象后，它会处在短暂的pending 状态；
等真正的 PV 创建好之后，它就处在 available 状态&lt;/li&gt;
&lt;li&gt;用户在提交 PVC 之后，被 K8s 相关组件做完 bound（即：找到相应的 PV），这个时候 PV 和 PVC 就结合到一起了，此时两者都处在 bound 状态&lt;/li&gt;
&lt;li&gt;在使用完 PVC，将其删除后，这个 PV 就处在 released 状态&lt;/li&gt;
&lt;li&gt;当 PV 已经处在 released 状态下，它是没有办法直接回到 available 状态
想把已经 released 的 PV 复用，有两种方式
4.1 新建一个 PV 对象，然后把之前的 released 的 PV 的相关字段的信息填到新的 PV 对象里面，这样的话，这个 PV 就可以结合新的 PVC 了
4.2 是我们在删除 pod 之后，不要去删除 PVC 对象，这样给 PV 绑定的 PVC 还是存在的，下次 pod 使用的时候，就可以直接通过 PVC 去复用&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;pvcpv-设计与实现原理&#34;&gt;PVC/PV 设计与实现原理&lt;/h2&gt;
&lt;h3 id=&#34;架构设计&#34;&gt;架构设计&lt;/h3&gt;
&lt;h4 id=&#34;podpv-创建的流程&#34;&gt;Pod/PV 创建的流程&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/podcreateflow.png&#34; alt=&#34;动态PV用户配置解析&#34;&gt;&lt;/p&gt;
&lt;p&gt;CSI(container storage interface) 的实现可分为两大部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一部分是由k8s社区驱动实现的通用的部分，如图中 csi-provisioner和 csi-attacher controller；&lt;/li&gt;
&lt;li&gt;第二部分由云存储厂商实践的，对接云存储厂商的 OpenApi，主要是实现真正的 create/delete/mount/unmount 存储的相关操作，对应到上图中的csi-controller-server和csi-node-server。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户提交 yaml 之后，k8s内部的处理流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;用户在提交 PVCyaml 的时候，首先会在集群中生成一个 PVC 对象&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PVC 对象会被 csi-provisioner controller watch到，csi-provisioner 会结合 PVC 对象以及 PVC 对象中声明的 storageClass，通过 GRPC 调用 csi-controller-server&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;csi-controller-server，然后，到云存储服务这边去创建真正的存储，并最终创建出来 PV 对象&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;由集群中的 PV controller 将 PVC 和 PV 对象做 bound ， PV 就可以被使用了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用户在提交 pod 之后,首先会被调度器调度选中某一个合适的node&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;该 node 上面的 kubelet 在创建 pod 流程中会通过首先 csi-node-server 将我们之前创建的 PV 挂载到我们 pod 可以使用的路径&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;kubelet 开始  create &amp;amp;&amp;amp; start pod 中的所有 container&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;pvpvc-以及通过-csi-使用存储流程&#34;&gt;PV、PVC 以及通过 csi 使用存储流程&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/pv/pvstages.png&#34; alt=&#34;动态PV用户配置解析&#34;&gt;&lt;/p&gt;
&lt;p&gt;有三个阶段：&lt;/p&gt;
&lt;p&gt;第一:  create 阶段，主要是创建存储&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 用户提交完 PVC，由 csi-provisioner 创建存储，并生成 PV 对象，之后 PV controller 将 PVC 及生
 成的 PV 对象做 bound
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第二:  attach 阶段，就是将那块存储挂载到 node 上面(通常为将存储load到node的/dev下面)；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;用户在提交 pod yaml 的时候，首先会被调度选中某一个 合适的node，等 pod 的运行 node 被选出来之后，
会被 AD Controller watch 到 pod 选中的 node，它会去查找 pod 中使用了哪些 PV。然后它会生成一
个内部的对象叫 VolumeAttachment 对象，从而去触发 csi-attacher去调用csi-controller-server 
去做真正的 attache 操作，attach操作调到云存储厂商OpenAPI。这个 attach 操作就是将存储 attach到 
pod 将会运行的 node 上面
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第三:  mount 阶段，将对应的存储进一步挂载到 pod 可以使用的路径&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubelet 创建 pod的过程中，它在创建 pod 的过程中，首先要去做一个 mount，这里的 mount 操作是为了将
已经attach到这个 node 上面那块盘，进一步 mount 到 pod 可以使用的一个具体路径，之后 kubelet 才
开始创建并启动容器
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;介绍了 K8s Volume 的使用场景，以及本身局限性；&lt;/li&gt;
&lt;li&gt;通过介绍 K8s 的 PVC 和 PV 体系，说明 K8s 通过 PVC 和 PV 体系增强了 K8s Volumes 在多 Pod 共享/迁移/存储扩展等场景下的能力的必要性以及设计思想；&lt;/li&gt;
&lt;li&gt;通过介绍 PV（存储）的不同供给模式 (static and dynamic)，学习了如何通过不同方式为集群中的 Pod 供给所需的存储；&lt;/li&gt;
&lt;li&gt;通过 PVC&amp;amp;PV 在 K8s 中完整的处理流程，深入理解 PVC&amp;amp;PV 的工作原理。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>应用可观测性</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/appobserver/appobserver/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/appobserver/appobserver/</guid>
      <description>&lt;h2 id=&#34;目标&#34;&gt;目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;介绍一些整体需求的来源；&lt;/li&gt;
&lt;li&gt;介绍在 K8s 中 Liveness 和 Readiness 的使用方式；&lt;/li&gt;
&lt;li&gt;介绍在 K8s 中常见问题的诊断；&lt;/li&gt;
&lt;li&gt;应用的远程调试的方式；&lt;/li&gt;
&lt;li&gt;课程的总结与实践；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;需求背景&#34;&gt;需求背景&lt;/h2&gt;
&lt;h3 id=&#34;来源&#34;&gt;来源&lt;/h3&gt;
&lt;p&gt;应用迁移到 Kubernetes 之后，要如何去保障应用的健康与稳定呢？从两个方面&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提高应用的可观测性；&lt;/li&gt;
&lt;li&gt;提供应用可恢复能力；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可观测性，可在三个方面增强：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先是应用的健康状态上面，可以实时地进行观测&lt;/li&gt;
&lt;li&gt;可以获取应用的资源使用情况；&lt;/li&gt;
&lt;li&gt;可以拿到应用的实时日志，进行问题的诊断与分析&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;应用可观测性的手段livenessreadiness探针&#34;&gt;应用可观测性的手段（Liveness/Readiness）探针&lt;/h2&gt;
&lt;h3 id=&#34;二者比较&#34;&gt;二者比较&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appobserver/readnessliveness.png&#34; alt=&#34;app observer&#34;&gt;&lt;/p&gt;
&lt;!-- 

|   |  Liveness(存活探针) | Readness（就绪探针）
|:------|:-------|:-------|
| 介绍 | 用于判断容器是否存活，即Pod状态&lt;br&gt;是否为Runing,如果Liveness探针判
        断容器不健康，就会触发kubelet杀&lt;br&gt;掉容器,并根据配置的策略判断是否重启容器,如果默认不配置Liveness 探针，则认为返回值默认为成功 | 用于判断容器是否启动完成，&lt;br&gt;即Pod的Condition是否Ready,如果探测结果不成功，则会Pod从Endpoint中移除，直至下次判断成功，在将Pod挂回到Endpoint上|
| 检测失败 | 杀掉Pod| 切断上层流量到Pod|
| 使用场景| 支持重新拉起的应用|启动后无法立即对外服务的应用|
| 注意事项|不论是Liveness还是Readness 探针，选择合适的方式可以防止被误操作:&lt;br&gt; 1. 调大判断的超时阈值，防止在容器压力较高的情况下出现偶发超时&lt;br&gt;2. 调整判断次数阈值，3次的默认值在短周期下不一定是最佳实践 &lt;br&gt; 3. exec 的如果执行的是shell脚本判断，在容器中可能调用时间会非常长|
 --&gt;
&lt;h3 id=&#34;应用监控探测方式&#34;&gt;应用监控探测方式&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appobserver/detectway.png&#34; alt=&#34;app observer&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;应用监控探测文件描述&#34;&gt;应用监控探测文件描述&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appobserver/probesepc.png&#34; alt=&#34;app observer&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;k8s-中常见问题的诊断&#34;&gt;K8s 中常见问题的诊断&lt;/h2&gt;
&lt;h3 id=&#34;状态机制&#34;&gt;状态机制&lt;/h3&gt;
&lt;h3 id=&#34;常见应用异常&#34;&gt;常见应用异常&lt;/h3&gt;
&lt;h4 id=&#34;pod-停留在-pending&#34;&gt;Pod 停留在 Pending&lt;/h4&gt;
&lt;p&gt;第一个就是 pending 状态，pending 表示调度器没有进行介入。此时可以通过 kubectl describe pod 来查看相应的事件，如果由于资源或者说端口占用，或者是由于 node selector 造成 pod 无法调度的时候，可以在相应的事件里面看到相应的结果，这个结果里面会表示说有多少个不满足的 node，有多少是因为 CPU 不满足，有多少是由于 node 不满足，有多少是由于 tag 打标造成的不满足。&lt;/p&gt;
&lt;h4 id=&#34;pod-停留在-waiting&#34;&gt;Pod 停留在 waiting&lt;/h4&gt;
&lt;p&gt;那第二个状态就是 pod 可能会停留在 waiting 的状态，pod 的 states 处在 waiting 的时候，通常表示说这个 pod 的镜像没有正常拉取，原因可能是由于这个镜像是私有镜像，但是没有配置 Pod secret；那第二种是说可能由于这个镜像地址是不存在的，造成这个镜像拉取不下来；还有一个是说这个镜像可能是一个公网的镜像，造成镜像的拉取失败。&lt;/p&gt;
&lt;h4 id=&#34;pod-不断被拉取并且可以看到-crashing&#34;&gt;Pod 不断被拉取并且可以看到 crashing&lt;/h4&gt;
&lt;p&gt;第三种是 pod 不断被拉起，而且可以看到类似像 backoff。这个通常表示说 pod 已经被调度完成了，但是启动失败，那这个时候通常要关注的应该是这个应用自身的一个状态，并不是说配置是否正确、权限是否正确，此时需要查看的应该是 pod 的具体日志。&lt;/p&gt;
&lt;h4 id=&#34;service-无法正常的工作&#34;&gt;Service 无法正常的工作&lt;/h4&gt;
&lt;p&gt;最后一种就是 service 无法正常工作的时候，该怎么去判断呢？那比较常见的 service 出现问题的时候，是自己的使用上面出现了问题。因为 service 和底层的 pod 之间的关联关系是通过 selector 的方式来匹配的，也就是说 pod 上面配置了一些 label，然后 service 通过 match label 的方式和这个 pod 进行相互关联。如果这个 label 配置的有问题，可能会造成这个 service 无法找到后面的 endpoint，从而造成相应的 service 没有办法对外提供服务，那如果 service 出现异常的时候，第一个要看的是这个 service 后面是不是有一个真正的 endpoint，其次来看这个 endpoint 是否可以对外提供正常的服务&lt;/p&gt;
&lt;h4 id=&#34;常见应用异常总结&#34;&gt;常见应用异常总结&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appobserver/fqa.png&#34; alt=&#34;app observer&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;应用远程调试&#34;&gt;应用远程调试&lt;/h2&gt;
&lt;h3 id=&#34;pod-远程调试&#34;&gt;Pod 远程调试&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;进入一个正在运行的Pod&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec -it pod-name /bin/bash
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进入一个正在运行包含多容器的Pod&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec -it pod-name -c container-name /bin/bash
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;servic-远程调试&#34;&gt;Servic 远程调试&lt;/h3&gt;
&lt;p&gt;实现方案，两种&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一: 将一个服务暴露到远程的一个集群之内，让远程集群内的一些应用来去调用本地的一个服务，这是一条反向的一个链路；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二: 让这个本地服务能够去调远程的服务，那么这是一条正向的链路。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;开源工具-telepresence&#34;&gt;开源工具 Telepresence&lt;/h3&gt;
&lt;p&gt;Telepresence 将本地的应用代理到集群中的一个Service 上&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  Telepresence -swap-deployment $DEPLOYMENT_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当本地开发的应用需要调用集群中的服务时:
可以使用Port-Forward将远程的应用代理到本地的端口&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  kubectl port-forward svc/app -n app-namespace
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;开源的调试工具---kubectl-debug&#34;&gt;开源的调试工具 - kubectl-debug&lt;/h3&gt;
&lt;h2 id=&#34;总结与实践&#34;&gt;总结与实践&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/appobserver/summery.png&#34; alt=&#34;appobserversummery&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes调度和资源管理</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/schedule/schedule/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/schedule/schedule/</guid>
      <description>&lt;h2 id=&#34;目录&#34;&gt;目录&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Kubernetes 的调度过程；&lt;/li&gt;
&lt;li&gt;Kubernetes 的基础调度能力（资源调度、关系调度）；&lt;/li&gt;
&lt;li&gt;Kubernetes 高级调度能力（优先级、抢占）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;kubernetes-调度过程&#34;&gt;Kubernetes 调度过程&lt;/h2&gt;
&lt;h3 id=&#34;调度过程概览&#34;&gt;调度过程概览&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/scheduledef.png&#34; alt=&#34;scheduledef&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;调度过程解析&#34;&gt;调度过程解析&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;用户通过 kube-ApiServer 提交描述Pod的yaml &lt;br&gt;&lt;/li&gt;
&lt;li&gt;ApiServer 会先把这个待创建的请求路由给我们的 webhooks 的 Controlles 进行校验&lt;br&gt;&lt;/li&gt;
&lt;li&gt;ApiServer 会在集群里面生成一个 pod，但此时生成的 pod，它的 nodeName 是空的，并且它的 phase 是 Pending 状态&lt;br&gt;&lt;/li&gt;
&lt;li&gt;kube-Scheduler 以及 kubelet 都能 watch 到这个 pod 的生成事件，kube-Scheduler 发现这个 pod 的 nodeName 是空的之后，会认为这个 pod 是处于未调度状态&lt;br&gt;&lt;/li&gt;
&lt;li&gt;kube-Scheduler 把这个 pod 拿到自己里面进行调度，通过一系列的调度算法，包括一系列的过滤和打分的算法后，Schedule 会选出一台最合适的节点，并且把这一台节点的名称绑定在这个 pod 的 spec 上，完成一次调度的过程&lt;br&gt;&lt;/li&gt;
&lt;li&gt;更新完 nodeName 之后，在 Node1 上的这台 kubelet 会 watch 到这个 pod 是属于自己节点上的一个 pod。它会把这个 pod 拿到节点上进行操作，包括创建一些容器 storage 以及 network，最后等所有的资源都准备完成，kubelet 会把状态更新为 Running&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;调度的关键点&#34;&gt;调度的关键点&lt;/h3&gt;
&lt;p&gt;调度的核心：就是把 pod 放到&lt;strong&gt;合适&lt;/strong&gt;的 node 上。
何为合适？&lt;/p&gt;
&lt;p&gt;1、首先要满足 pod 的资源要求;&lt;br&gt;
2、其次要满足 pod 的一些特殊关系的要求;&lt;br&gt;
3、再次要满足 node 的一些限制条件的要求;&lt;br&gt;
4、最后还要做到整个集群资源的合理利用。&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Kubernetes 是怎么做到满足这些 pod 和 node 的要求的？&lt;/p&gt;
&lt;h2 id=&#34;kubernetes基础调度力&#34;&gt;Kubernetes基础调度力&lt;/h2&gt;
&lt;h3 id=&#34;调度能力划分&#34;&gt;调度能力划分&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;资源调度&lt;br&gt;
Kubernetes 基本的一些 Resources 的配置方式，还有 Qos 的概念，以及 Resource Quota 的概念和使用方式&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;关系调度&lt;br&gt;
在关系调度上，介绍两种关系场景:&lt;br&gt;
2.1 pod 和 pod 之间的关系场景，包括怎么去亲和一个 pod，怎么去互斥一个 pod? &lt;br&gt;
2.2 pod 和 node 之间的关系场景，包括怎么去亲和一个 node，以及有一些 node 怎么去限制 pod 调度上来。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;如何满足pod资源要求&#34;&gt;如何满足Pod资源要求&lt;/h2&gt;
&lt;h3 id=&#34;资源配置方法&#34;&gt;资源配置方法&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/resourceconf.png&#34; alt=&#34;scheduledef&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;pod资源类型&#34;&gt;Pod资源类型&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;cpu&lt;/li&gt;
&lt;li&gt;memory&lt;/li&gt;
&lt;li&gt;ephemeral-storage&lt;/li&gt;
&lt;li&gt;extended-resource: nvidia.com/gpu&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;resources&#34;&gt;resources&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;request: 对这个 pod 基本保底的一些资源要求&lt;/li&gt;
&lt;li&gt;limit: 代表的是对这个 pod 可用能力上限的一种限制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由 request/limit 来引出 Qos概念&lt;/p&gt;
&lt;h3 id=&#34;资源qos类型&#34;&gt;资源QoS类型&lt;/h3&gt;
&lt;h4 id=&#34;qos-quality-of-service&#34;&gt;Qos: Quality of Service&lt;/h4&gt;
&lt;p&gt;其实是 Kubernetes 用来表达一个 pod 在资源能力上的服务质量的标准,Kubernetes 提供了三类的 Qos Class:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;第一类是 Guaranteed:它是一类高的 Qos Class，一般用 Guaranteed 来为一些需要资源保障能力的 pod 进行配置&lt;/li&gt;
&lt;li&gt;第二类是 Burstable，它其实是中等的一个 Qos label，一般会为一些希望有弹性能力的 pod 来配置 Burstable；&lt;/li&gt;
&lt;li&gt;第三类是 BestEffort，通过名字我们也知道，它是一种尽力而为式的服务质量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;问题：K8s中用户没法指定自己的 pod 是属于哪一类 Qos，而是通过 request 和 limit 的组合来自动地映射上 Qos Class&lt;/p&gt;
&lt;h4 id=&#34;pod-qos配置&#34;&gt;Pod QoS配置&lt;/h4&gt;
&lt;p&gt;如何通过 request 和 limit 的组合来确定我们想要的 QoS Level&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteed: CPU/Memory 必须 request==limit，其他的资源可以不相等&lt;/li&gt;
&lt;li&gt;Burstable: CPU/Memory request 和 limit 不相等&lt;/li&gt;
&lt;li&gt;BestEffort: 所有资源request/limit 必须都不填&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;资源qos用法&#34;&gt;资源QoS用法&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/podguaranteed.png&#34; alt=&#34;podguaranteed&#34;&gt;
&lt;img src=&#34;../../../k8s/schedule/podburstable.png&#34; alt=&#34;podburstable&#34;&gt;
&lt;img src=&#34;../../../k8s/schedule/podbesteffort.png&#34; alt=&#34;podbesteffort&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;资源和qos-关系&#34;&gt;资源和QoS 关系&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/resourceandqos.png&#34; alt=&#34;resourceandqos&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;调度器只会使用 request 进行调度&lt;/li&gt;
&lt;li&gt;不同的 QoS 表现不相同,&lt;/li&gt;
&lt;li&gt;开启 kubelet cpu-manager-policy=static 特性时，
如果它的 request 是一个整数，它会对 Guaranteed Pod 进行绑核&lt;/li&gt;
&lt;li&gt;非整数的 Guaranteed/Burstable/BestEffort，它们的 CPU 会放在一块，组成一个 CPU share pool，然后它们会根据不同的权重划分时间片来使用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外，memory 上也会按照不同的 Qos 进行划分:OOMScore&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteed，它会配置默认的 -998 的 OOMScore&lt;/li&gt;
&lt;li&gt;Burstable,会根据内存设计的大小和节点的关系来分配 2-999 的 OOMScore&lt;/li&gt;
&lt;li&gt;BestEffort 会固定分配 1000 的 OOMScore&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OOMScore 得分越高的话，在物理机出现 OOM 的时候会优先被 kill 掉&lt;/p&gt;
&lt;p&gt;eviction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;会优先考虑驱逐 BestEffort 的 pod&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;资源-quota&#34;&gt;资源 Quota&lt;/h3&gt;
&lt;p&gt;假如集群是由多个人同时提交的，或者是多个业务同时在使用，我们肯定要限制某个业务或某个人提交的总量，防止整个集群的资源都会被使用掉，导致另一个业务没有资源使用&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/resourcequota.png&#34; alt=&#34;resourcequota&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;解析&#34;&gt;解析&lt;/h4&gt;
&lt;p&gt;ResourceQuota: 限制 namespace 资源用量
上图右侧的 yaml 所示,spec 包括了一个 hard 和 scopeSelector&lt;br&gt;
hard: 和 Resourcelist 很像,比 ResourceList 更丰富一点,可以填写一些 Pod，这样可以限制 Pod 数量能力&lt;br&gt;
scopeSelector:为这个 Resource 方法定义更丰富的索引能力,
比如上面的例子中，索引出非 BestEffort 的 pod，限制的 cpu 是 1000 个，memory 是 200G，Pod 是 10 个，然后 Scope 除了提供 NotBestEffort，它还提供了更丰富的索引范围，包括 Terminating/Not Terminating，BestEffort/NotBestEffort，PriorityClass。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用 ResourceQuota 方法来做到限制每一个 namespace 的资源用量，从而保证其他用户的资源使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;小结&#34;&gt;小结&lt;/h3&gt;
&lt;p&gt;基础资源的使用方式，做到了如何满足 Pod 资源要求。下面做一个小结：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 要配置合理的资源要求&lt;/li&gt;
&lt;li&gt;CPU/Memory/EphemeralStorage/GPU&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过 Request 和 Limit 来为不同业务特点的 Pod 选择不同的 QoS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteed：敏感型，需要业务保障&lt;/li&gt;
&lt;li&gt;Burstable：次敏感型，需要弹性业务&lt;/li&gt;
&lt;li&gt;BestEffort：可容忍性业务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为每个 NS 配置 ResourceQuota 来防止过量使用，保障其他人的资源可用&lt;/p&gt;
&lt;h2 id=&#34;如何满足podnode-特殊关系条件要求&#34;&gt;如何满足Pod/Node 特殊关系/条件要求&lt;/h2&gt;
&lt;h3 id=&#34;如何满足pod-与pod-关系要求&#34;&gt;如何满足Pod 与Pod 关系要求&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;podAffinity/podAntiAffinity&lt;/li&gt;
&lt;li&gt;required /preferred&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;如何满足pod-与node-关系要求&#34;&gt;如何满足Pod 与Node 关系要求&lt;/h3&gt;
&lt;p&gt;主要又两类:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NodeSelector: 举例：必须要调度 Pod 到带了 k1: v1 标签的 Node 上，这时可以在 Pod 的 spec 中填写一个 nodeSelector 要求，比如 k1: v1。这样我的 Pod 就会强制调度到带了 k1: v1 标签的 Node 上&lt;/li&gt;
&lt;li&gt;NodeAffinity:&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;小结-1&#34;&gt;小结&lt;/h3&gt;
&lt;p&gt;首先假如有需求是处理 Pod 与 Pod 的时候，比如 Pod 和另一个 Pod 有亲和的关系或者是互斥的关系，可以给它们配置下面的参数：&lt;/p&gt;
&lt;p&gt;PodAffinity&lt;br&gt;
PodAntiAffinity&lt;/p&gt;
&lt;p&gt;假如存在 Pod 和 Node 有亲和关系，可以配置下面的参数：&lt;/p&gt;
&lt;p&gt;NodeSelector&lt;br&gt;
NodeAffinity&lt;/p&gt;
&lt;p&gt;假如有些 Node 是限制某些 Pod 调度的，比如说一些故障的 Node，或者说是一些特殊业务的 Node，可以配置下面的参数：&lt;/p&gt;
&lt;p&gt;Node &amp;ndash; Taints&lt;br&gt;
Pod &amp;ndash; Tolerations&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-高级调度能力&#34;&gt;Kubernetes 高级调度能力&lt;/h2&gt;
&lt;h3 id=&#34;优先级调度目的&#34;&gt;优先级调度目的&lt;/h3&gt;
&lt;h4 id=&#34;在资源不够情况下我们怎么做到集群的合理利用呢&#34;&gt;在&lt;strong&gt;资源不够情况下，我们怎么做到集群的合理利用呢？&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;先到先得策略 (FIFO) -简单、相对公平，上手快&lt;/li&gt;
&lt;li&gt;优先级策略 (Priority) - 符合日常公司业务特点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因为公司业务里面肯定是有高优先级的业务和低优先级的业务，所以优先级策略会比先到先得策略更能够符合日常公司业务特点&lt;/p&gt;
&lt;h4 id=&#34;priority-和-preemption&#34;&gt;Priority 和 Preemption&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;v 1.14 -stable&lt;/li&gt;
&lt;li&gt;Priority &amp;amp; Preemption default is On&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;优先级调度设置&#34;&gt;优先级调度设置&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/podprioritysetting.png&#34; alt=&#34;podprioritysetting&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;设置解析&#34;&gt;设置解析&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;需要创建一个 priorityClass&lt;br&gt;
1.1 是创建名为 high 的 priorityClass，它是高优先级，得分为 10000；&lt;br&gt;
1.2 然后还创建了一个 low 的 priorityClass，它的得分是 100。&lt;br&gt;&lt;/li&gt;
&lt;li&gt;为每个 Pod 配置上不同的 priorityClassName&lt;br&gt;
2.1 给 Pod1 配置上了 high，Pod2 上配置了 low priorityClassName&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;内置优先级配置&#34;&gt;内置优先级配置&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 内置了默认的优先级,DefaultpriorityWhenNoDefaultClassExistis,默认所有Pod此项均为0&lt;/li&gt;
&lt;li&gt;用户可配置最大优先级限制：HighestUserDefinablePriority = 10000000000(10 亿)&lt;/li&gt;
&lt;li&gt;系统级别优先级：SystemCriticalPriority = 20000000000(20 亿)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;优先级调度过程&#34;&gt;优先级调度过程&lt;/h3&gt;
&lt;h4 id=&#34;只触发优先级调度但是没有触发抢占调度的流程&#34;&gt;只触发优先级调度但是没有触发抢占调度的流程&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/podschedulewithoutpreemption.png&#34; alt=&#34;podschedulewithoutpreemption&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;优先级抢占过程&#34;&gt;优先级抢占过程&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/podpreemption.png&#34; alt=&#34;podpreemption&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;优先级抢占策略&#34;&gt;优先级抢占策略&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedule/preemptionstrategy.png&#34; alt=&#34;preemptionstrategy&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;一个 Pod 进入抢占的时候，会判断 Pod 是否拥有抢占的资格，有可能上次已经抢占过一次，如果符合抢占资格，它会先对所有的节点进行一次过滤，过滤出符合这次抢占要求的节点，如果不符合就过滤掉这批节点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从过滤剩下的节点中，挑选出合适的节点进行抢占。这次抢占的过程会模拟一次调度，也就是把上面优先级低的 Pod 先移除出去，再把待抢占的 Pod 尝试能否放置到此节点上。然后通过这个过程选出一批节点，进入下一个过程叫 ProcessPreemptionWithExtenders。这是一个扩展的钩子，用户可以在这里加一些自己抢占节点的策略，如果没有扩展的钩子，这里面是不做任何动作的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接下来的流程叫做 PickOneNodeForPreemption，就是从上面 selectNodeForPreemption list 里面挑选出最合适的一个节点，这是有一定的策略的&lt;br&gt;
3.1 优先选择打破 PDB 最少的节点；&lt;br&gt;
3.2 其次选择待抢占 Pods 中最大优先级最小的节点；&lt;br&gt;
3.3 再次选择待抢占 Pods 优先级加和最小的节点；&lt;br&gt;
3.4 接下来选择待抢占 Pods 数目最小的节点；&lt;br&gt;
3.5 最后选择拥有最晚启动 Pod 的节点&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;小结-2&#34;&gt;小结&lt;/h3&gt;
&lt;p&gt;调度的高级策略，在集群资源紧张的时候也能合理调度资源&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建自定义的一些优先级类别 (PriorityClass)；&lt;/li&gt;
&lt;li&gt;给不同类型 Pods 配置不同的优先级 (PriorityClassName)；&lt;/li&gt;
&lt;li&gt;通过组合不同类型 Pods 运行和优先级抢占让集群资源和调度弹性起来。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;参考阅读&#34;&gt;参考阅读&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://edu.aliyun.com/lesson_1651_17068?spm=5176.10731542.0.0.7b1920bemydBcn#_17068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ALi云原生课堂&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes调度和资源管理</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/schedulealgorithem/schedulealgo/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/schedulealgorithem/schedulealgo/</guid>
      <description>&lt;h2 id=&#34;目标&#34;&gt;目标&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Scheduler 架构&lt;/li&gt;
&lt;li&gt;Scheduler 算法实现
&lt;ul&gt;
&lt;li&gt;调度流程&lt;/li&gt;
&lt;li&gt;Predicates&lt;/li&gt;
&lt;li&gt;Priorities&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如何配置调度器&lt;/li&gt;
&lt;li&gt;Scheduler Extender&lt;/li&gt;
&lt;li&gt;Scheduler Framework&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;scheduler-架构&#34;&gt;Scheduler 架构&lt;/h2&gt;
&lt;h3 id=&#34;架构图&#34;&gt;架构图&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/schedulealgorithem/images/schedulerarc.png&#34; alt=&#34;schedulerarc&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;架构图解析&#34;&gt;架构图解析&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;调度器启动时会通过配置文件 File，或者是命令行参数，或者是配置好的 ConfigMap，来指定调度策略。指定要用哪些过滤器 (Predicates)、打分器 (Priorities) 以及要外挂哪些外部扩展的调度器 (Extenders)，和要使用的哪些 Schedule 的扩展点 (Plugins)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动的时候会通过 kube-apiserver 去 watch 相关的数据，通过 Informer 机制将调度需要的数据 ：Pod 数据、Node 数据、存储相关的数据，以及在抢占流程中需要的 PDB 数据，和打散算法需要的 Controller-Workload 数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过 Informer 去 watch 到需要等待的 Pod 数据，放到队列里面，通过调度算法流程里面，会一直循环从队列里面拿数据，然后经过调度流水线&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调度流水线 (Schedule Pipeline) 主要有三个组成部分：
4.1 调度器的调度流程
4.2 Wait 流程
4.3 Bind 流程&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从调度队列里面拿到一个 Pod 进入到 Schedule Theread 流程中，通过 Pre Filter&amp;ndash;Filter&amp;ndash;Post Filter&amp;ndash;Score(打分)-Reserve，最后 Reserve 对账本做预占用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基本调度流程结束后，会把这个任务提交给 Wait Thread 以及 Bind Thread，然后 Schedule Theread 继续执行流程，会从调度队列中拿到下一个 Pod 进行调度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调度完成后，会去更新调度缓存 (Schedule Cache)，如更新 Pod 数据的缓存，也会更新 Node 数据。以上就是大概的调度流程&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>网络模型基础</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network01/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network01/</guid>
      <description>&lt;h2 id=&#34;目标&#34;&gt;目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pod 网络联通性&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;li&gt;服务发现&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pod-网络联通性&#34;&gt;Pod 网络联通性&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>网络模型进阶</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network02/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network02/</guid>
      <description>&lt;h2 id=&#34;目标&#34;&gt;目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;kubernetes 网络的演化历史&lt;/li&gt;
&lt;li&gt;Pod 如何上网&lt;/li&gt;
&lt;li&gt;Service 如何工作&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;li&gt;思考问题&lt;/li&gt;
&lt;li&gt;总结&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-网络的演化历史&#34;&gt;kubernetes 网络的演化历史&lt;/h2&gt;
&lt;h3 id=&#34;早期docker网络的由来及弊端&#34;&gt;早期Docker网络的由来及弊端&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/dockernetworkbegin.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;p&gt;容器网络发端于 Docker 的网络。Docker 使用了一个比较简单的网络模型，即内部的网桥加内部的保留 IP.
这种设计的好处在于:
容器的网络和外部世界是解耦的，无需占用宿主机的 IP 或者宿主机的资源，完全是虚拟的。
它的设计初衷是: 当需要访问外部世界时，会采用 SNAT 这种方法来借用 Node 的 IP 去访问外面的服务。
比如容器需要对外提供服务的时候，所用的是 DNAT 技术，也就是在 Node 上开一个端口，然后通过 iptable 或者别的某些机制，把流导入到容器的进程上以达到目的。&lt;/p&gt;
&lt;p&gt;该模型的问题在于，外部网络无法区分哪些是容器的网络与流量、哪些是宿主机的网络与流量。比如，如果要做一个高可用的时候，172.16.1.1 和 172.16.1.2 是拥有同样功能的两个容器，此时我们需要将两者绑成一个 Group 对外提供服务，而这个时候我们发现从外部看来两者没有相同之处，它们的 IP 都是借用宿主机的端口，因此很难将两者归拢到一起。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-的解决方案&#34;&gt;kubernetes 的解决方案&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/dockernetworkk8s.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;p&gt;Kubernetes 提出了这样一种机制:即每一个 Pod，也就是一个功能聚集小团伙应有自己的“身份证”，或者说 ID。在 TCP 协议栈上，这个 ID 就是 IP。
这个 IP 是真正属于该 Pod 的，外部世界不管通过什么方法一定要给它。对这个 Pod IP 的访问就是真正对它的服务的访问，中间拒绝任何的变造。比如以 10.1.1.1 的 IP 去访问 10.1.2.1 的 Pod，结果到了 10.1.2.1 上发现，它实际上借用的是宿主机的 IP，而不是源 IP，这样是不被允许的。Pod 内部会要求共享这个 IP，从而解决了一些功能内聚的容器如何变成一个部署的原子的问题。&lt;/p&gt;
&lt;p&gt;Kubernetes 对怎么实现这个模型其实是没有什么限制的，用 underlay 网络来控制外部路由器进行导流是可以的；如果希望解耦，用 overlay 网络在底层网络之上再加一层叠加网，这样也是可以的。总之，只要达到模型所要求的目的即可。&lt;/p&gt;
&lt;h2 id=&#34;pod-如何上网&#34;&gt;Pod 如何上网&lt;/h2&gt;
&lt;p&gt;容器网络的网络包究竟是怎么传送的？可以从以下两个维度来看：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;协议层次&lt;/li&gt;
&lt;li&gt;网络拓扑&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/podnetwork.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;协议层次&#34;&gt;协议层次&lt;/h3&gt;
&lt;p&gt;它和 TCP 协议栈的概念是相同的，需要从两层、三层、四层一层层地摞上去，发包的时候从右往左，即先有应用数据，然后发到了 TCP 或者 UDP 的四层协议，继续向下传送，加上 IP 头，再加上 MAC 头就可以送出去了。收包的时候则按照相反的顺序，首先剥离 MAC 的头，再剥离 IP 的头，最后通过协议号在端口找到需要接收的进程。&lt;/p&gt;
&lt;h3 id=&#34;网络拓扑&#34;&gt;网络拓扑&lt;/h3&gt;
&lt;p&gt;一个容器的包所要解决的问题分为两步：第一步，如何从容器的空间 (c1) 跳到宿主机的空间 (infra)；第二步，如何从宿主机空间到达远端。&lt;/p&gt;
&lt;p&gt;容器网络的方案可以通过接入、流控、通道这三个层面来考虑&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一个是接入，就是说我们的容器和宿主机之间是使用哪一种机制做连接，比如 Veth + bridge、Veth + pair 这样的经典方式，也有利用高版本内核的新机制等其他方式（如 mac/IPvlan 等）,来把包送入到宿主机空间；&lt;/li&gt;
&lt;li&gt;第二个是流控，就是说我的这个方案要不要支持 Network Policy，如果支持的话又要用何种方式去实现。这里需要注意的是，我们的实现方式一定需要在数据路径必经的一个关节点上。如果数据路径不通过该 Hook 点，那就不会起作用；&lt;/li&gt;
&lt;li&gt;第三个是通道，即两个主机之间通过什么方式完成包的传输。我们有很多种方式，比如以路由的方式，具体又可分为 BGP 路由或者直接路由。还有各种各样的隧道技术等等。最终我们实现的目的就是一个容器内的包通过容器，经过接入层传到宿主机，再穿越宿主机的流控模块（如果有）到达通道送到对端。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;路由方案分析&#34;&gt;路由方案分析&lt;/h3&gt;
&lt;p&gt;一个最简单的路由方案：Flannel-host-gw&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/flannelhostgw.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个方案采用的是每个 Node 独占网段，每个 Subnet 会绑定在一个 Node 上，网关也设置在本地，或者说直接设在 cni0 这个网桥的内部端口上。
该方案的好处是管理简单，坏处就是无法跨 Node 迁移 Pod-这个 IP、网段已经是属于这个 Node 之后就无法迁移到别的 Node 上&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/hostgw.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个方案的精髓在于 route 表的设置，如上图所示。接下来为大家一一解读一下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一条很简单，我们在设置网卡的时候都会加上这一行。就是指定我的默认路由是通过哪个 IP 走掉，默认设备又是什么&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二条是对 Subnet 的一个规则反馈。就是说我的这个网段是 10.244.0.0，掩码是 24 位，它的网关地址就在网桥上，也就是 10.244.0.1。这就是说这个网段的每一个包都发到这个网桥的 IP 上；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第三条是对对端的一个反馈。如果你的网段是 10.244.1.0（上图右边的 Subnet），我们就把它的 Host 的网卡上的 IP (10.168.0.3) 作为网关。也就是说，如果数据包是往 10.244.1.0 这个网段发的，就请以 10.168.0.3 作为网关。后面的跟本次讲解没有什么关系。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再来看一下这个数据包到底是如何跑起来的？&lt;/p&gt;
&lt;p&gt;假设容器 (10.244.0.2) 想要发一个包给 10.244.1.3，那么它在本地产生了 TCP 或者 UDP 包之后，再依次填好对端 IP 地址、本地以太网的 MAC 地址作为源 MAC 以及对端 MAC。一般来说本地会设定一条默认路由，默认路由会把 cni0 上的 IP 作为它的默认网关，对端的 MAC 就是这个网关的 MAC 地址。然后这个包就可以发到桥上去了。如果网段在本桥上，那么通过 MAC 层的交换即可解决。&lt;/p&gt;
&lt;p&gt;这个例子中我们的 IP 并不属于本网段，因此网桥会将其上送到主机的协议栈去处理。主机协议栈恰好找到了对端的 MAC 地址。使用 10.168.0.3 作为它的网关，通过本地 ARP 探查后，我们得到了 10.168.0.3 的 MAC 地址。即通过协议栈层层组装，我们达到了目的，将 Dst-MAC 填为右图主机网卡的 MAC 地址，从而将包从主机的 eth0 发到对端的 eth0 上去。&lt;/p&gt;
&lt;p&gt;所以大家可以发现，这里有一个隐含的限制，上图中的 MAC 地址填好之后一定是能到达对端的，但如果这两个宿主机之间不是二层连接的，中间经过了一些网关、一些复杂的路由，那么这个 MAC 就不能直达，这种方案就是不能用的。当包到达了对端的 MAC 地址之后，发现这个包确实是给它的，但是 IP 又不是它自己的，就开始 Forward 流程，包上送到协议栈，之后再走一遍路由，刚好会发现 10.244.1.0/24 需要发到 10.244.1.1 这个网关上，从而到达了 cni0 网桥，它会找到 10.244.1.3 对应的 MAC 地址，再通过桥接机制，这个包就到达了对端容器。&lt;/p&gt;
&lt;p&gt;大家可以看到，整个过程总是二层、三层，发的时候又变成二层，再做路由，就是一个大环套小环。这是一个比较简单的方案，如果中间要走隧道，则可能会有一条 vxlan tunnel 的设备，此时就不填直接的路由，而填成对端的隧道号。&lt;/p&gt;
&lt;h2 id=&#34;service-如何工作&#34;&gt;Service 如何工作&lt;/h2&gt;
&lt;p&gt;Service 其实是一种负载均衡 (Load Balance) 的机制。&lt;/p&gt;
&lt;p&gt;我们认为它是一种用户侧(Client Side) 的负载均衡，也就是说 VIP 到 RIP 的转换在用户侧就已经完成了，并不需要集中式地到达某一个 NGINX 或者是一个 ELB 这样的组件来进行决策。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/servicework.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;p&gt;它的实现是这样的:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先是由一群 Pod 组成一组功能后端&lt;/li&gt;
&lt;li&gt;在前端上定义一个虚 IP 作为访问入口,会附赠一个 DNS 的域名，Client 先访问域名得到虚 IP 之后再转成实 IP&lt;/li&gt;
&lt;li&gt;Kube-proxy 则是整个机制的实现核心，它隐藏了大量的复杂性。它的工作机制是通过 apiserver 监控 Pod/Service 的变化(比如是不是新增了 Service、Pod）并将其反馈到本地的规则或者是用户态进程。并将其反馈到本地的规则或者是用户态进程&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;三步实现一个lvs-的service&#34;&gt;三步实现一个LVS 的Service&lt;/h3&gt;
&lt;p&gt;我们来实际做一个 LVS 版的 Service。LVS 是一个专门用于负载均衡的内核机制。它工作在第四层，性能会比用 iptable 实现好一些。&lt;/p&gt;
&lt;p&gt;假设我们是一个 Kube-proxy，拿到了一个 Service 的配置，如下图所示：它有一个 Cluster IP，在该 IP 上的端口是 9376，需要反馈到容器上的是 80 端口，还有三个可工作的 Pod，它们的 IP 分别是 10.1.2.3, 10.1.14.5, 10.1.3.8。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/servicepods.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;p&gt;它要做的事情就是：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/threesteps.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;p&gt;过程详解：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第 1 步，绑定 VIP 到本地（欺骗内核）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;首先需要让内核相信它拥有这样的一个虚 IP，这是 LVS 的工作机制所决定的，因为它工作在第四层，并不关心 IP 转发，只有它认为这个 IP 是自己的才会拆到 TCP 或 UDP 这一层。在第一步中，我们将该 IP 设到内核中，告诉内核它确实有这么一个 IP。实现的方法有很多，我们这里用的是 ip route 直接加 local 的方式，用 Dummy 哑设备上加 IP 的方式也是可以的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第 2 步，为这个虚 IP 创建一个 IPVS 的 virtual server；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;告诉它我需要为这个 IP 进行负载均衡分发，后面的参数就是一些分发策略等等。virtual server 的 IP 其实就是我们的 Cluster IP。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第 3 步，为这个 IPVS service 创建相应的 real server。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们需要为 virtual server 配置相应的 real server，就是真正提供服务的后端是什么。比如说我们刚才看到有三个 Pod，于是就把这三个的 IP 配到 virtual server 上，完全一一对应过来就可以了。Kube-proxy 工作跟这个也是类似的。只是它还需要去监控一些 Pod 的变化，比如 Pod 的数量变成 5 个了，那么规则就应变成 5 条。如果这里面某一个 Pod 死掉了或者被杀死了，那么就要相应地减掉一条。又或者整个 Service 被撤销了，那么这些规则就要全部删掉。所以它其实做的是一些管理层面的工作。&lt;/p&gt;
&lt;p&gt;三步实现LVS 服务全景图&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/threesteplvsservice.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;负载均衡还分内部外部&#34;&gt;负载均衡还分内部外部&lt;/h2&gt;
&lt;p&gt;Service 的类型，可以分为以下 4 类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP
集群内部的一个虚拟 IP，这个 IP 会绑定到一堆服务的 Group Pod 上面，这也是默认的服务方式。它的缺点是这种方式只能在 Node 内部也就是集群内部使用。&lt;/li&gt;
&lt;li&gt;NodePort
供集群外部调用。将 Service 承载在 Node 的静态端口上，端口号和 Service 一一对应，那么集群外的用户就可以通过 &lt;NodeIP&gt;:&lt;NodePort&gt; 的方式调用到 Service。&lt;/li&gt;
&lt;li&gt;LoadBalancer
给云厂商的扩展接口。像阿里云、亚马逊这样的云厂商都是有成熟的 LB 机制的，这些机制可能是由一个很大的集群实现的，为了不浪费这种能力，云厂商可通过这个接口进行扩展。它首先会自动创建 NodePort 和 ClusterIP 这两种机制，云厂商可以选择直接将 LB 挂到这两种机制上，或者两种都不用，直接把 Pod 的 RIP 挂到云厂商的 ELB 的后端也是可以的。&lt;/li&gt;
&lt;li&gt;ExternalName
摈弃内部机制，依赖外部设施，比如某个用户特别强，他觉得我们提供的都没什么用，就是要自己实现，此时一个 Service 会和一个域名一一对应起来，整个负载均衡的工作都是外部实现的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是一个实例。它灵活地应用了 ClusterIP、NodePort 等多种服务方式，又结合了云厂商的 ELB，变成了一个很灵活、极度伸缩、生产上真正可用的一套系统。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/lvsserviced.png&#34; alt=&#34;dockernetwork&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;用 ClusterIP 来做功能 Pod 的服务入口
大家可以看到，如果有三种 Pod 的话，就有三个 Service Cluster IP 作为它们的服务入口。这些方式都是 Client 端的，如何在 Server 端做一些控制呢？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;起一些 Ingress 的 Pod（Ingress 是 K8s 后来新增的一种服务，本质上还是一堆同质的 Pod），然后将这些 Pod 组织起来，暴露到一个 NodePort 的 IP，K8s 的工作到此就结束了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;任何一个用户访问 23456 端口的 Pod 就会访问到 Ingress 的服务，它的后面有一个 Controller，会把 Service IP 和 Ingress 的后端进行管理，最后会调到 ClusterIP，再调到我们的功能 Pod。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELB 去监听所有集群节点上的 23456 端口，只要在 23456 端口上有服务的，就认为有一个 Ingress 的实例在跑。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;整个的流量经过外部域名的一个解析跟分流到达了云厂商的 ELB，ELB 经过负载均衡并通过 NodePort 的方式到达 Ingress，Ingress 再通过 ClusterIP 调用到后台真正的 Pod。&lt;/p&gt;
&lt;h2 id=&#34;思考问题&#34;&gt;思考问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;容器层的网络究竟如何与宿主机网络共存？选择 underlay 网络还是 overlay 网络？是都变成 overlay 还是根据场景去区分？大家在工作中一般是怎么思考的？&lt;/li&gt;
&lt;li&gt;Service 还可以有怎样的实现？&lt;/li&gt;
&lt;li&gt;为什么一个容器编排系统要大力搞服务发现和负载均衡&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;要从根本上理解 Kubernetes 网络模型的演化来历，理解 PerPodPerIP 的用心在哪里；&lt;/li&gt;
&lt;li&gt;网络的事情万变不离其宗，按照模型从 4 层向下就是发包过程，反正层层剥离就是收包过程，容器网络也是如此；&lt;/li&gt;
&lt;li&gt;Ingress 等机制是在更高的层次上（服务&amp;lt;-&amp;gt;端口）方便大家部署集群对外服务，通过一个真正可用的部署实例，希望大家把 Ingress+Cluster IP + PodIP 等概念联合来看，理解社区出台新机制、新资源对象的思考。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>理解CNI 和 CNI插件</title>
      <link>https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network03/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network03/</guid>
      <description>&lt;h2 id=&#34;目标&#34;&gt;目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CNI 定义&lt;/li&gt;
&lt;li&gt;CNI如何选择&lt;/li&gt;
&lt;li&gt;开发自己的CNI插件&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;什么是cni&#34;&gt;什么是CNI&lt;/h2&gt;
&lt;p&gt;CNI: Container Network Interface，即容器网络的 API 接口&lt;/p&gt;
&lt;p&gt;CNI Plugin : 一系列实现 CNI API 接口的网络插件
CNI 插件主要解决的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置Pod网络空间&lt;/li&gt;
&lt;li&gt;打通Pod之间网络连同&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubelet 通过这个标准的 API 来调用不同的网络插件以实现不同的网络配置方式。&lt;/p&gt;
&lt;p&gt;常见的CNI接口包括：
Calico
Flannel
Terway
Weave Net
Contiv&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-中如何使用&#34;&gt;Kubernetes 中如何使用&lt;/h2&gt;
&lt;h3 id=&#34;kubernetes-cni-插件启用流程&#34;&gt;kubernetes CNI 插件启用流程&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;配置CNI配置文件（/etc/cni/net.d/XXXnet.conf）&lt;/li&gt;
&lt;li&gt;安装CNI 二进制插件(/opt/cni/bin/xxnet)&lt;/li&gt;
&lt;li&gt;在这个节点上创建Pod&lt;/li&gt;
&lt;li&gt;Kubelet 会根据CNI 配置文件执行CNI插件&lt;/li&gt;
&lt;li&gt;Pod网络配置初始化&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/podnetworkcreate.png&#34; alt=&#34;cnipluginstartflow&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;cni如何选择&#34;&gt;CNI如何选择&lt;/h2&gt;
&lt;p&gt;CNI 的实现类型，可分为三类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overlay, 隧道&lt;/li&gt;
&lt;li&gt;路由&lt;/li&gt;
&lt;li&gt;Underlay, 底层网络&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/cniimplements.png&#34; alt=&#34;cniimplements&#34;&gt;&lt;/p&gt;
&lt;p&gt;如何选择自己的插件呢？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/pluginselect.png&#34; alt=&#34;pluginselect&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;环境限制&#34;&gt;环境限制&lt;/h3&gt;
&lt;p&gt;不同环境中所支持的底层能力是不同的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;虚拟化环境（例如 OpenStack）中的网络限制较多，比如不允许机器之间直接通过二层协议访问，必须要带有 IP 地址这种三层的才能去做转发，限制某一个机器只能使用某些 IP 等。在这种被做了强限制的底层网络中，只能去选择 Overlay 的插件，常见的有 Flannel-vxlan, Calico-ipip, Weave 等等；&lt;/li&gt;
&lt;li&gt;物理机环境中底层网络的限制较少，比如说我们在同一个交换机下面直接做一个二层的通信。对于这种集群环境，我们可以选择 Underlay 或者路由模式的插件。Underlay 意味着我们可以直接在一个物理机上插多个网卡或者是在一些网卡上做硬件虚拟化；路由模式就是依赖于 Linux 的路由协议做一个打通。这样就避免了像 vxlan 的封包方式导致的性能降低。这种环境下我们可选的插件包括 clico-bgp, flannel-hostgw, sriov 等等；&lt;/li&gt;
&lt;li&gt;公有云环境也是虚拟化，因此底层限制也会较多。但每个公有云都会考虑适配容器，提升容器的性能，因此每家公有云可能都提供了一些 API 去配置一些额外的网卡或者路由这种能力。在公有云上，我们要尽量选择公有云厂商提供的 CNI 插件以达到兼容性和性能上的最优。比如 Aliyun 就提供了一个高性能的 Terway 插件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;功能需求&#34;&gt;功能需求&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;安全需求；
K8s 支持 NetworkPolicy，就是说我们可以通过 NetworkPolicy 的一些规则去支持“Pod 之间是否可以访问”这类策略。但不是每个 CNI 插件都支持 NetworkPolicy 的声明，如果大家有这个需求，可以选择支持 NetworkPolicy 的一些插件，比如 Calico, Weave 等等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否需要集群外的资源与集群内的资源互联互通；
大家的应用最初都是在虚拟机或者物理机上，容器化之后，应用无法一下就完成迁移，因此就需要传统的虚拟机或者物理机能跟容器的 IP 地址互通。为了实现这种互通，就需要两者之间有一些打通的方式或者直接位于同一层。此时可以选择 Underlay 的网络，比如 sriov 这种就是 Pod 和以前的虚拟机或者物理机在同一层。我们也可以使用 calico-bgp，此时它们虽然不在同一网段，但可以通过它去跟原有的路由器做一些 BGP 路由的一个发布，这样也可以打通虚拟机与容器。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;K8s 的服务发现与负载均衡的能力。
K8s 的服务发现与负载均衡就是我们前面所介绍的 K8s 的 Service，但并不是所有的 CNI 插件都能实现这两种能力。比如很多 Underlay 模式的插件，在 Pod 中的网卡是直接用的 Underlay 的硬件，或者通过硬件虚拟化插到容器中的，这个时候它的流量无法走到宿主机所在的命名空间，因此也无法应用 kube-proxy 在宿主机配置的规则。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种情况下，插件就无法访问到 K8s 的服务发现。因此大家如果需要服务发现与负载均衡，在选择 Underlay 的插件时就需要注意它们是否支持这两种能力。&lt;/p&gt;
&lt;h3 id=&#34;性能需求&#34;&gt;性能需求&lt;/h3&gt;
&lt;p&gt;我们可以从 Pod 的创建速度和 Pod 的网络性能来衡量不同插件的性能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pod 的创建速度&lt;/strong&gt;。当我们创建一组 Pod 时，比如业务高峰来了，需要紧急扩容，这时比如说我们扩容了 1000 个 Pod，就需要 CNI 插件创建并配置 1000 个网络资源。Overlay 和路由模式在这种情况下的创建速度是很快的，因为它是在机器里面又做了虚拟化，所以只需要调用内核接口就可以完成这些操作。但对于 Underlay 模式，由于需要创建一些底层的网络资源，所以整个 Pod 的创建速度相对会慢一些。因此对于经常需要紧急扩容或者创建大批量的 Pod 这些场景，我们应该尽量选择 Overlay 或者路由模式的网络插件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pod 的网络性能&lt;/strong&gt;。主要表现在两个 Pod 之间的网络转发、网络带宽、PPS 延迟等这些性能指标上。Overlay 模式的性能较差，因为它在节点上又做了一层虚拟化，还需要去封包，封包又会带来一些包头的损失、CPU 的消耗等，如果大家对网络性能的要求比较高，比如说机器学习、大数据这些场景就不适合使用 Overlay 模式。这种情形下我们通常选择 Underlay 或者路由模式的 CNI 插件。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;如何开发自己的cni插件&#34;&gt;如何开发自己的CNI插件&lt;/h2&gt;
&lt;p&gt;CNI 插件的实现通常包含两个部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;一个二进制的 CNI 插件去配置 Pod 网卡和 IP 地址。这一步配置完成之后相当于给 Pod 上插上了一条网线，就是说它已经有自己的 IP、有自己的网卡了；&lt;/li&gt;
&lt;li&gt;一个 Daemon 进程去管理 Pod 之间的网络打通。这一步相当于说将 Pod 真正连上网络，让 Pod 之间能够互相通信。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;给-pod-插上网线&#34;&gt;给 Pod 插上网线&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../../k8s/network/images/cndplugindev.png&#34; alt=&#34;cndplugindev&#34;&gt;&lt;/p&gt;
&lt;p&gt;步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;给 Pod 准备一个网卡&lt;/strong&gt;。通常我们会用一个 &amp;ldquo;veth&amp;rdquo; 这种虚拟网卡，一端放到 Pod 的网络空间，一端放到主机的网络空间，这样就实现了 Pod 与主机这两个命名空间的打通。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;给 Pod 分配IP地址&lt;/strong&gt;。这个 IP 地址在集群里需要是唯一的。如何保障集群里面给 Pod 分配的是个唯一的 IP 地址呢？
一般来说我们在创建整个集群的时候会指定 Pod 的一个大网段，按照每个节点去分配一个 Node 网段。比如说上图右侧创建的是一个 172.16 的网段，我们再按照每个节点去分配一个 /24 的段，这样就能保障每个节点上的地址是互不冲突的。然后每个 Pod 再从一个具体的节点上的网段中再去顺序分配具体的 IP 地址，比如 Pod1 分配到了 172.16.0.1，Pod2 分配到了 172.16.0.2，这样就实现了在节点里面 IP 地址分配的不冲突，并且不同的 Node 又分属不同的网段，因此不会冲突。这样就给 Pod 分配了集群里面一个唯一的 IP 地址。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;配置 Pod 的 IP 和路由&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;第一步，将分配到的 IP 地址配置给 Pod 的虚拟网卡；&lt;/li&gt;
&lt;li&gt;第二步，在 Pod 的网卡上配置集群网段的路由，令访问的流量都走到对应的 Pod 网卡上去，并且也会配置默认路由的网段到这个网卡上，也就是说走公网的流量也会走到这个网卡上进行路由；&lt;/li&gt;
&lt;li&gt;最后在宿主机上配置到 Pod 的 IP 地址的路由，指向到宿主机对端 veth1 这个虚拟网卡上。这样实现的是从 Pod 能够到宿主机上进行路由出去的，同时也实现了在宿主机上访问到 Pod 的 IP 地址也能路由到对应的 Pod 的网卡所对应的对端上去。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;给-pod-连上网络&#34;&gt;给 Pod 连上网络&lt;/h3&gt;
&lt;p&gt;刚才我们是给 Pod 插上网线，也就是给它配了 IP 地址以及路由表。那怎么打通 Pod 之间的通信呢？也就是让每一个 Pod 的 IP 地址在集群里面都能被访问到。&lt;/p&gt;
&lt;p&gt;一般我们是在 CNI Daemon 进程中去做这些网络打通的事情。通常来说是这样一个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先 CNI 在每个节点上运行的 Daemon 进程会学习到集群所有 Pod 的 IP 地址及其所在节点信息。学习的方式通常是通过监听 K8s APIServer，拿到现有 Pod 的 IP 地址以及节点，并且新的节点和新的 Pod 的创建的时候也能通知到每个 Daemon；&lt;/li&gt;
&lt;li&gt;拿到 Pod 以及 Node 的相关信息之后，再去配置网络进行打通。
&lt;ul&gt;
&lt;li&gt;首先 Daemon 会创建到整个集群所有节点的通道。这里的通道是个抽象概念，具体实现一般是通过 Overlay 隧道、阿里云上的 VPC 路由表、或者是自己机房里的 BGP 路由完成的；&lt;/li&gt;
&lt;li&gt;第二步是将所有 Pod 的 IP 地址跟上一步创建的通道关联起来。关联也是个抽象概念，具体的实现通常是通过 Linux 路由、fdb 转发表或者OVS 流表等完成的。Linux 路由可以设定某一个 IP 地址路由到哪个节点上去。fdb 转发表是 forwarding database 的缩写，就是把某个 Pod 的 IP转发到某一个节点的隧道端点上去（Overlay 网络）。OVS 流表是由Open vSwitch 实现的，它可以把 Pod 的 IP 转发到对应的节点上。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;课后思考实践&#34;&gt;课后思考实践&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在自己公司的网络环境中，选择哪种网络插件最适合？&lt;/li&gt;
&lt;li&gt;尝试自己实现一个 CNI 插件。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
