[{"authors":["admin"],"categories":null,"content":"Ace Mei is an experts of devops. His research interests include distributed system, devops, aiops.\n","date":1461110400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1555459200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://meixinyun.github.io/programmertalk/author/ace-mei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/author/ace-mei/","section":"authors","summary":"Ace Mei is an experts of devops. His research interests include distributed system, devops, aiops.","tags":null,"title":"Ace Mei","type":"authors"},{"authors":null,"categories":null,"content":"CI/CD 基于GitlabPipeline的CI/CD实现方案 基于GitOps的DevOps系统优化设计 CI/CD构建框架TekTon的深入剖析 ","date":1567987200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1567987200,"objectID":"1e35174be6d11ab3cb2ccfe9b82e62d5","permalink":"https://meixinyun.github.io/programmertalk/ee/cicd/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/programmertalk/ee/cicd/","section":"ee","summary":"持续集成","tags":null,"title":"CICD实现方案","type":"docs"},{"authors":null,"categories":null,"content":"Devops 的由来 落地Devops的核心技术 企业级Devops的实践之路  从零搭建CICD系统标准化交付流程  相关技术专题 学习资源汇总 总结 ","date":1567987200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1567987200,"objectID":"275940a52b399f3f55f2064dabcfd43f","permalink":"https://meixinyun.github.io/programmertalk/devops/setup/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/programmertalk/devops/setup/","section":"devops","summary":"Devops","tags":null,"title":"Devops OverView","type":"docs"},{"authors":null,"categories":null,"content":"敏捷协作平台 认知协调 [知识库/在线文档/事件追踪(Jira)/IM] 敏捷迭代的理念/流程 需求/项目 持续集成，持续发布(CI/CD) 研发效率（/框架/复用） 软件定义服务（流程，规范) 代码质量QA 基础代码框架/公共库 微服务复用/Serveless/FaaS 研发效能指标度量 ","date":1567987200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1567987200,"objectID":"c710aba99f8f8a1553bab1314cb36c43","permalink":"https://meixinyun.github.io/programmertalk/ee/engineeringefficiency/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/programmertalk/ee/engineeringefficiency/","section":"ee","summary":"工程效率体系建设","tags":null,"title":"工程效率体系","type":"docs"},{"authors":null,"categories":null,"content":"DockerSwarm 系统架构 Feature highlights   Cluster management integrated with Docker Engine: Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n  Decentralized design: Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n  Declarative service model: Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n  Scaling: For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n  Desired state reconciliation: The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state and your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager creates two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n  Multi-host networking: You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n  Service discovery: Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n  Load balancing: You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n  Secure by default: Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n  Rolling updates: At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service.\n  ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"4580f789cef000d9979103626b8f5240","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/dockerswarm/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/containertechnology/dockerswarm/","section":"containertechnology","summary":"DockerSwarm 学习笔记","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" 基础篇\n Go语言学习 Golang 指针 内存管理  进阶篇\n 并发编程 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"3da05e61e22bbb540d8f1ea25eb68afb","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/golang/","section":"courses","summary":"Golang Study","tags":null,"title":"GolangStudy","type":"docs"},{"authors":null,"categories":null,"content":"基础篇     云原生技术概览 K8S体系架构 作业管控系统 应用配置管理 应用持久化存储 应用可观测性 进阶篇     Kubernetes调度和资源管理 Kubernetes容器网络模型基础 Kubernetes容器网络模型进阶 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"afd3c8c42d41904d7a294c858321a066","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/containertechnology/k8s/","section":"containertechnology","summary":"k8s study notes","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"内存管理 7.1 概览 7.2 内存分配器 7.3 垃圾收集器 7.4 栈内存管理 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"d4535a0253d31dcf10715728670b8398","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/chapter7/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/golang/chapter7/","section":"courses","summary":"内存管理","tags":null,"title":"内存管理","type":"docs"},{"authors":null,"categories":null,"content":"时间序列数据库 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"72232713557edc63026df983c0079c34","permalink":"https://meixinyun.github.io/programmertalk/courses/tsdb/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/tsdb/","section":"courses","summary":"时间序列数据库的分析和设计","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"监控生态概览 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b7ae7ad3bce8631dddbf318734f97fa5","permalink":"https://meixinyun.github.io/programmertalk/courses/monitor/ecology/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/monitor/ecology/","section":"courses","summary":"监控生态","tags":null,"title":"服务质量评估体系生态","type":"docs"},{"authors":null,"categories":null,"content":"服务质量评估体系生态 产品 技术及架构 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"9885298b44c2f92928f18cd1134c13ab","permalink":"https://meixinyun.github.io/programmertalk/courses/monitor/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/monitor/","section":"courses","summary":"持续反馈系统-服务质量评估体系建设","tags":null,"title":"服务质量评估体系建设","type":"docs"},{"authors":null,"categories":null,"content":"服务健康监控红绿大盘 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"dc23a0697e37366a2849c12f703442f5","permalink":"https://meixinyun.github.io/programmertalk/courses/monitor/productization/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/monitor/productization/","section":"courses","summary":"监控产品化-易用性","tags":null,"title":"监控产品化","type":"docs"},{"authors":null,"categories":null,"content":"并发编程 Contex详解 Channel ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"5f4e9f2b52cbd1cdc24e1a23b9ceb183","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/chapter3/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/golang/chapter3/","section":"courses","summary":"运行时 并发编程","tags":null,"title":"并发编程","type":"docs"},{"authors":null,"categories":null,"content":"术语表    术语 简称 描述     Continuous Integration CI 持续集成，主要指在代码构建过程中持续地进行代码\n的集成、构建、以及自动化测试等   Continuous Deployment CD 持续部署。在代码构建完毕后，可以方便地将新版本部\n署上线,方便持续交付。   Continuous Delivery CD 持续交付   GitLab-CICD GitLab-CICD 配合GitLab使用的持续集成/部署系统   GitLab-Runner Runner 用来执行软件集成脚本的执行引擎    实现架构 ","date":1567987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567987200,"objectID":"4e7d4077a27d5cec0e596c838ca55262","permalink":"https://meixinyun.github.io/programmertalk/ee/cicd/gitlabpipeline/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/programmertalk/ee/cicd/gitlabpipeline/","section":"ee","summary":"GitlabPipeline","tags":null,"title":"基于GitlabPipeline实现CI/CD","type":"docs"},{"authors":null,"categories":null,"content":"GitOps ","date":1567987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567987200,"objectID":"da63b3776a8d58e869cfb412275be5ac","permalink":"https://meixinyun.github.io/programmertalk/ee/cicd/gitops/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/programmertalk/ee/cicd/gitops/","section":"ee","summary":"基于GitOps的实现","tags":null,"title":"基于GitOps的实现","type":"docs"},{"authors":null,"categories":null,"content":"目标   Tekton 的历史\n  Tekton 的设计分析\n  Tekton 详细设计\n  Tekton 实践案例\n  Tekton 源码分析\n  Tekton 问题汇总\n  Tekton 历史 Tekton 是一个谷歌开源的kubernetes原生CI/CD系统，功能强大且灵活。google cloud已经推出了基于Tekton的服务（https://cloud.google.com/Tekton/）\n其实Tekton的前身是Knative的build-pipeline项目，从名字可以看出这个项目是为了给build模块增加pipeline的功能，但是大家发现随着不同的功能加入到Knative build模块中，build模块越来越变得像一个通用的CI/CD系统，这已经脱离了Knative build设计的初衷，于是，索性将build-pipeline剥离出Knative，摇身一变成为Tekton，而Tekton也从此致力于提供全功能、标准化的原生kubernetesCI/CD解决方案。\nTekton虽然还是一个挺新的项目，但是已经成为 Continuous Delivery Foundation (CDF) 的四个初始项目之一，另外三个则是大名鼎鼎的Jenkins、Jenkins X、Spinnaker，实际上Tekton还可以作为插件集成到JenkinsX中。所以，如果你觉得Jenkins太重，没必要用Spinnaker这种专注于多云平台的CD，为了避免和Gitlab耦合不想用gitlab-ci，那么Tekton值得一试。\nTekton 的设计分析 Tekton目标   标准化你的 CI/CD 工具：Tekton 提供的开源组件可以跨供应商，语言和部署环境标准化 CI / CD 工具和流程。Tekton 提供的管道，版本，工作流程和其他 CI / CD 组件与行业规范一致，可以和你现有的 CI / CD 工具（如 Jenkins，Jenkins X，Skaffold 和 Knative 等）配合使用\n  内置用于 Kubernetes 的最佳实践：使用 Tekton 的内置最佳实践可以快速创建云原生 CI / CD 管道，目标是让开发人员创建和部署不可变镜像，管理基础架构的版本控制或执行更简单的回滚。 还可以利用 Tekton 的滚动部署，蓝 / 绿部署，金丝雀部署或 GitOps 工作流等高级部署模式。\n  Tekton 的核心概念   Task：顾名思义，task表示一个构建任务，task里可以定义一系列的steps，例如编译代码、构建镜像、推送镜像等，每个step实际由一个Pod执行。\n  TaskRun：task只是定义了一个模版，taskRun才真正代表了一次实际的运行，当然你也可以自己手动创建一个taskRun，taskRun创建出来之后，就会自动触发task描述的构建任务。\n  Pipeline：一个或多个task、PipelineResource以及各种定义参数的集合。\n  PipelineRun：类似task和taskRun的关系，pipelineRun也表示某一次实际运行的pipeline，下发一个pipelineRun CRD实例到kubernetes后，同样也会触发一次pipeline的构建。\n  PipelineResource：表示pipeline input资源，比如github上的源码，或者pipeline output资源，例如一个容器镜像或者构建生成的jar包等。\n  相关的概念   蓝绿部署： 是不停老版本，部署新版本然后进行测试，确认OK，将流量切到新版本，然后老版本同时也升级到新版本。\n  灰度： 是选择部分部署新版本，将部分流量引入到新版本，新老版本同时提供服务。等待灰度的版本OK，可全量覆盖老版本。\n  灰度是不同版本共存，蓝绿是新旧版本切换，2种模式的出发点不一样。\n","date":1567987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567987200,"objectID":"a796f1dfa730a1738a58ec3928b870ba","permalink":"https://meixinyun.github.io/programmertalk/ee/cicd/tekton/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/programmertalk/ee/cicd/tekton/","section":"ee","summary":"CI/CD构建框架TekTon的深入剖析","tags":null,"title":"CI/CD构建框架TekTon的深入剖析","type":"docs"},{"authors":null,"categories":null,"content":"k8s 基本工作原理 ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"fdcc1b34246149d9f78216a861f285b9","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/fundamentals/fundamentals/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/fundamentals/fundamentals/","section":"containertechnology","summary":"k8s 基本工作原理","tags":null,"title":"K8S体系架构","type":"docs"},{"authors":null,"categories":null,"content":"Context 的设计背景及原则 服务端处理请求逻辑 在Go服务端，每个传入的请求被一个自己的goroutine 处理。 请求处理器通常会开启额外的 goroutine 来访问后端服务 比如 database 或 RPC服务。每个请求的 goroutine 集合通常需要访问该请求特定的信息，比如用户的ID，授权token 以及 请求的 deadline. 当请求被取消或超时时，这个请求对应的goroutine 集能快速退出，系统可以回收它们使用的所有资源。\n流水线工作模式 使用原则   不要将Context 存储到一个结构体中，而应该是通过函数传参的方式传递。Context 应该当作第一个参数\n  不要传递一个空的 context 。当你不知道该传递何种Context时候，可以用 context.TODO\n  仅对传输进程和API的请求范围的数据使用上下文值，而不是将可选参数传递给函数。\n  相同的Context 可以传递给不同 goroutine中的函数，Context 对于多个goroutine同时使用是安全的。\n  Context 实现 Context 的类图结构 classDiagram Context \u0026lt;|-- emptyCtx : Implementation Context \u0026lt;|-- cancelCtx : Implementation Context \u0026lt;|-- timerCtx : Implementation Context \u0026lt;|-- valueCtx : Implementation \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Context Context : Deadline() (deadline time.Time, ok bool) Context : Done() \u0026lt;-chan struct{} Context : Err() error Context : Value(key interface{}) interface{} class CancelFunc{ \u0026lt;\u0026lt;func\u0026gt;\u0026gt; CancelFunc func() } CancelFunc --* WithCancel Context --* WithCancel  Context 接口申明 // A Context carries a deadline, a cancellation signal, and other values across // API boundaries. // // Context's methods may be called by multiple goroutines simultaneously. type Context interface { // Deadline returns the time when work done on behalf of this context // should be canceled. // Deadline returns ok==false when no deadline is set. // Successive calls to Deadline return the same results. Deadline() (deadline time.Time, ok bool) // Done returns a channel that's closed when work done on behalf of this // context should be canceled. Done may return nil if this context can // never be canceled. Successive calls to Done return the same value. // The close of the Done channel may happen asynchronously, // after the cancel function returns. // // WithCancel arranges for Done to be closed when cancel is called; // WithDeadline arranges for Done to be closed when the deadline expires; // WithTimeout arranges for Done to be closed when the timeout elapses. // Done is provided for use in select statements: Done() \u0026lt;-chan struct{} // If Done is not yet closed, Err returns nil. // If Done is closed, Err returns a non-nil error explaining why: // Canceled if the context was canceled // or DeadlineExceeded if the context's deadline passed. // After Err returns a non-nil error, successive calls to Err return the same error. Err() error // Value returns the value associated with this context for key, or nil // if no value is associated with key. Successive calls to Value with // the same key returns the same result. Value(key interface{}) interface{} }  接口分析 // Done is provided for use in select statements: Done() \u0026lt;-chan struct{}  Done 方法返回一个 struct{} chan , 该 channel 扮演了一个代表 context 运行函数的取消信号。 当该channel 关闭时，相应的函数应该放弃对应的工作并返回。\nContext 并不提供Cancel 方法，这和Done channel 只能被动接受的原因相同。 接受取消信号的函数，通常并不是 发送 cancel 信号 函数。尤其是,当一个父操作为子操作开启新的 goroutine 时，子操作不能取消父操作。\nErr() error  Err 函数会返回一个错误提示，为什么当前的context 被取消。\nDeadline() (deadline time.Time, ok bool)  Deadline 方法允许函数判断，是否值得开始工作。如果所剩的时间比较少，就不值得开启工作。在编码中， 也可以用用于设置I/O操作的超时时间\nValue(key interface{}) interface{}  Value 函数，允许上下文携带请求范围的数据。这些数据可以被多个子 goroutine 线程安全的访问。\nContext 的派生接口介绍 为了便于使用，Context 包提供了几个派生的Context，他们提供了Context语意，和一个 CancelFunc的回调函数。 当上下文被取消时，从它派生的所有上下文也将被取消。 函数原型如下：\n // A CancelFunc tells an operation to abandon its work. // A CancelFunc does not wait for the work to stop. // A CancelFunc may be called by multiple goroutines simultaneously. type CancelFunc func() WithCancel(parent Context) (ctx Context, cancel CancelFunc) WithDeadline(parent Context, d time.Time) (Context, CancelFunc) WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)  这几个函数，都接受一个上下文（parent Context)，并返回一个派生的上下文，和一个CancelFunc。调用CancelFunc将 触发以下操作：\n 取消输入context 派生的子context，以及该子context 派生的子孙 context 删除父对象对子对象的引用 停止任何关联的计时器。  Context 实现原理分析 Context 典型的使用场景，举例分析 Http 请求的场景 如下代码，模拟了一个一个HTTP Server 服务，处理客户端请求需要2秒时间，若在两秒内，用户取消了请求， 该请求应该立刻被返回。\n func main() { // Create an HTTP server that listens on port 8000 http.ListenAndServe(\u0026quot;:8000\u0026quot;, http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { ctx := r.Context() // This prints to STDOUT to show that processing has started fmt.Fprint(os.Stdout, \u0026quot;processing request\\n\u0026quot;) // We use `select` to execute a peice of code depending on which // channel receives a message first select { case \u0026lt;-time.After(2 * time.Second): // If we receive a message after 2 seconds // that means the request has been processed // We then write this as the response w.Write([]byte(\u0026quot;request processed\u0026quot;)) case \u0026lt;-ctx.Done(): // If the request gets cancelled, log it // to STDERR fmt.Fprint(os.Stderr, \u0026quot;request cancelled\\n\u0026quot;) } })) }  可以通过在2秒内关闭浏览器模拟取消请求，在控制台上就能看到 request cancelled 的输出。完整的代码在 这里。\n主动提交一个取消事件 在某些场景下，如果我们希望在满足特定条件下取消一个任务，可以通过提交一个cancel事件来实现。 下面的例子，展示这种场景\n func operation1(ctx context.Context) error { time.Sleep(100 * time.Millisecond) return errors.New(\u0026quot;failed\u0026quot;) } func operation2(ctx context.Context) { select { case \u0026lt;-time.After(500 * time.Millisecond): fmt.Println(\u0026quot;done\u0026quot;) case \u0026lt;-ctx.Done(): fmt.Println(\u0026quot;halted operation2\u0026quot;) } } func main() { ctx :=context.Background() ctx, cancel :=context.WithCancel(ctx) go func(){ err :=Operation1(ctx) if err != nil { cancel() } }() operation2(ctx) }  例子2，带有超时时间的context。\nfunc main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() go handle(ctx, 500*time.Millisecond) select { case \u0026lt;-ctx.Done(): fmt.Println(\u0026quot;main\u0026quot;, ctx.Err()) } } func handle(ctx context.Context, duration time.Duration) { select { case \u0026lt;-ctx.Done(): fmt.Println(\u0026quot;handle\u0026quot;, ctx.Err()) case \u0026lt;-time.After(duration): fmt.Println(\u0026quot;process request with\u0026quot;, duration) } }  一个容易犯错的例子  func doSomething() { ctx, cancel := context.WithCancel(ctx) defer cancel() someArg := \u0026quot;loremipsum\u0026quot; go doSomethingElse(context.Background(), someArg) }  doSomething 函数退出时，就会调用cancel(),此时，doSomething 会被迫终止， 在函数中调用另一个子进程时，应创建一个新的context\nPipline 的场景 所谓的Pipline 可以理解为： 通过channel 连在一起的一系列的状态，每个状态都有 一组goroutine 执行相同的函数。在每个阶段，这些 goroutine 执行如下操作：\n 从上游接受channel 获取值 对获取的数据执行一些操作， 将数据通过传出 channels下发  通过一个例子来说明：\n生成函数\nfunc gen(nums ...int) \u0026lt;-chan int { out := make(chan int) go func() { for _, n := range nums { out \u0026lt;- n } close(out) }() return out }  Context 总结 扩展阅读  Go Concurrency Patterns: Pipelines and cancellation\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"f2c829b7ce5c092eeaee7563ab694acc","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/chapter3/context/context/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/golang/chapter3/context/context/","section":"courses","summary":"并发编程 Context","tags":null,"title":"Contex详解","type":"docs"},{"authors":null,"categories":null,"content":"golang 指针 指针的基本概念  指针地址 指针类型  Go语言中指针的特点 Go语言为程序员提供了控制数据结构指针的能力，但是，并不能进行指针运算。\n为什么不能进行指针运算？   指针类型变量即拥有指针高效访问的特点，又不会发生指针偏移，从而避免了非法修改关键性数据的问题。\n  垃圾回收也比较容易对不会发生偏移的指针进行检索和回收。\n  指针的核心概念 指针（pointer）在Go语言中可以被拆分为两个核心概念：\n  类型指针，允许对这个指针类型的数据进行修改，传递数据可以直接使用指针，而无须拷贝数据，类型指针不能进行偏移和运算。\n  切片，由指向起始元素的原始指针、元素数量和容量组成。\n切片比原始指针具备更强大的特性，而且更为安全。切片在发生越界时，运行时会panic，并打出堆栈，而原 始指针只会崩溃。    GO 中的指针地址和指针类型 Go语言中使用在变量名前面添加\u0026amp;操作符（前缀）来获取变量的内存地址（取地址操作）\nptr :=\u0026amp;V // V 类型为T 其中 v 代表被取地址的变量，变量 v 的地址使用变量 ptr 进行接收，ptr 的类型为*T，称做 T 的指针类型，*代表指针。  golang 的关键字 make 和 new   make 的作用是 初始化内置的数据结构，slice,map,channel 1\n  new 的作用是 根据传入的类型，分配一片内存空间，并返回指向这篇内存空间的指针 2\n  new 关键字 先看 new 这个关键字，new 是一个内置的内存分配函数，区别与其它语言的 new, 它不初始化内存，只将其归零。也就是说，new（T）为类型为T的新项分配 zeroed storage 并返回其地址，即类型为 *T 的值。在go 中，它返回指向新分配的类型为T的零值的指针。\n It's a built-in function that allocates memory, but unlike its namesakes in some other languages it does not initialize the memory, it only zeros it. That is, new(T) allocates zeroed storage for a new item of type T and returns its address, a value of type *T. In Go terminology, it returns a pointer to a newly allocated zero value of type T. Since the memory returned by new is zeroed, it's helpful to arrange when designing your data structures that the zero value of each type can be used without further initialization. This means a user of the data structure can create one with new and get right to work. For example, the documentation for bytes.Buffer states that \u0026quot;the zero value for Buffer is an empty buffer ready to use.\u0026quot; Similarly, sync.Mutex does not have an explicit constructor or Init method. Instead, the zero value for a sync.Mutex is defined to be an unlocked mutex.  举例：\nThe zero-value-is-useful property works transitively,\n type SyncedBuffer struct { lock sync.Mutex buffer bytes.Buffer }  类型为 SyncedBuffer 的值 分配完，或申明完就可以立即使用，并不需要进一步的初始化\n p := new(SyncedBuffer) // type *SyncedBuffer var v SyncedBuffer // type SyncedBuffer  比较在C语言中的临时变量和GO语言中的临时变量指针 在C语言中，如下的写法\n#include \u0026lt;stdio.h\u0026gt; int *fun() //指针函数 （返回值是一个地址） { int a = 10; return \u0026amp;a; //返回变量a的地址 } int main() { int *b = NULL; b = fun(); printf(\u0026quot;%d\\n\u0026quot;, *b); } 编译时会有警告： main.c:14:12: warning: function returns address of local variable [-Wreturn-local-addr] $ ./a.out Segmentation fault (core dumped) //运行发生段错误  在 go 语言中，\npackage main import ( \u0026quot;fmt\u0026quot; ) func fun() *int { a := 10; return \u0026amp;a; //返回变量a的地址 } func main() { b :=fun() fmt.Printf(\u0026quot;%T,%p %d\\n\u0026quot;, b,b, *b) } go run a.go *int,0xc000018078 10 输出b 的类型为指针，内存地址为：0xc000018078,存储的值为10  new 和 make 关键字的区别\ngraph TD A[new] --\u0026gt;|-| B(pointer) make --\u0026gt;|-| D[slice] make --\u0026gt;|-| E[hash] make --\u0026gt;|-| F[channel]  构造器和符合语句（Constructors and composite literals ） 某些情况下，0值并不是我们想要的，如下例子：\nfunc NewFile(fd int, name string) *File { if fd \u0026lt; 0 { return nil } f := new(File) f.fd = fd f.name = name f.dirinfo = nil f.nepipe = 0 return f }  我们可以通过复合语句(composite literals)简化，\nfunc NewFile(fd int, name string) *File { if fd \u0026lt; 0 { return nil } f := File{fd, name, nil, 0} return \u0026amp;f }  上述例子，我们返回了一个临时变量，这个临时变量在函数返回后，在内存中的值依然存在。以上的语句可以进一步简化；\n return \u0026amp;File{fd, name, nil, 0}  复合文本的字段按顺序排列，并且必须全部存在。但是，通过将元素显式标记为字段：值对，初始值设定项可以按任意顺序显示，缺少的值保留为各自的零值。所以可以进一步简化：\nreturn \u0026amp;File{fd: fd, name: name}  如何复合构造的语句不包含 属性字段创建一个0值的类型指针，那么\nnew(File) 和 \u0026amp;File{}  两种表达方式是等效的。\nmake 关键字 内置函数 make (T , args) 和 new(T) 不同。make 只能创建 slices , map ,channel， make 会返回一个初始化 的类型为T的值（而非0值）。 这个区别的根本原因在于，这三种类型的数据结构，在使用前必须被初始化。\nPointer vs Values 对比分析如下两个函数定义，请解释他们的区别:\n// 定义一个新的 []byte 类型, type ByteSlice []byte // 绑定对应的Append函数， func (slice ByteSlice) Append(data []byte) []byte { ... } // 通过重新定义方法，将指向ByteSlice的指针作为其接收器来消除这种笨拙，这样方法就可以覆盖调用方的切片。 func (p *ByteSlice) Append(data []byte) { slice := *p // Body as above, without the return. *p = slice ... }  第一步，我们提供值传递的方法，构造了Append函数，但需要将 更新后的 slice返回， 第二个函数，将值传递转化为指针，这样可以直接对传入的slice进行修改，从而省掉返回值 其实，我们可以跟进一步，修改一些这个函数，为如下形式：\nfunc (p *ByteSlice) Write(data []byte) (n int, err error) { slice := *p // Again as above. *p = slice return len(data), nil }  类型 *ByteSlice, 符合标准的 io.Writer接口。因此可以直接输出\nvar b ByteSlice fmt.Fprintf(\u0026amp;b, \u0026quot;This hour has %d days\\n\u0026quot;, 7)  由于只有 *ByteSlice 满足 io.Writer， 因此我们传递 ByteSlice 的地址，(即: \u0026amp;b),\n指针和值的区别在于，值传递的方法，能被指针和值调用，但指针的方法，只能被指针类调用。 出现此规则的原因是：指针方法可以修改接收器；对值调用他们将\n参考阅读  allocation_new\n allocation_make\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"6431fcc38fb1bf7414318412fc93c196","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/golangpointer/golangpointer/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/golang/golangpointer/golangpointer/","section":"courses","summary":"Golang Pointer","tags":null,"title":"GolangStudy","type":"docs"},{"authors":null,"categories":null,"content":"数据模型 时间序列数据  identifier -\u0026gt; (t0, v0), (t1, v1), (t2, v2), (t3, v3), ....  每个数据点都是一个由时间戳和值组成的元组。\nIdentifier: metric name + set {(k,v)} 其中，metric name 也可以标示为: name = metric_name\n一个序列的数据点标示符（identifier）: 是由指标名称在加上一组唯一的多维度的标签集合构成。 典型的标示符结构如下：\n requests_total{path=\u0026quot;/status\u0026quot;, method=\u0026quot;GET\u0026quot;, instance=”10.0.0.1:80”} requests_total{path=\u0026quot;/status\u0026quot;, method=\u0026quot;POST\u0026quot;, instance=”10.0.0.3:80”} requests_total{path=\u0026quot;/\u0026quot;, method=\u0026quot;GET\u0026quot;, instance=”10.0.0.2:80”}  指标名称也可以作为一组特殊的 ( k,v)，上述的标示符存储数据模型标示如下：\n {__name__=\u0026quot;requests_total\u0026quot;, path=\u0026quot;/status\u0026quot;, method=\u0026quot;GET\u0026quot;, instance=”10.0.0.1:80”} {__name__=\u0026quot;requests_total\u0026quot;, path=\u0026quot;/status\u0026quot;, method=\u0026quot;POST\u0026quot;, instance=”10.0.0.3:80”} {__name__=\u0026quot;requests_total\u0026quot;, path=\u0026quot;/\u0026quot;, method=\u0026quot;GET\u0026quot;, instance=”10.0.0.2:80”}  数据模型立体分析 数据序列的数据模型，可基于一个二位坐标展开立体分析。\nseries ^ │ . . . . . . . . . . . . . . . . . . . . . . {__name__=\u0026quot;request_total\u0026quot;, method=\u0026quot;GET\u0026quot;} │ . . . . . . . . . . . . . . . . . . . . . . {__name__=\u0026quot;request_total\u0026quot;, method=\u0026quot;POST\u0026quot;} │ . . . . . . . │ . . . . . . . . . . . . . . . . . . . ... │ . . . . . . . . . . . . . . . . . . . . . │ . . . . . . . . . . . . . . . . . . . . . {__name__=\u0026quot;errors_total\u0026quot;, method=\u0026quot;POST\u0026quot;} │ . . . . . . . . . . . . . . . . . {__name__=\u0026quot;errors_total\u0026quot;, method=\u0026quot;GET\u0026quot;} │ . . . . . . . . . . . . . . │ . . . . . . . . . . . . . . . . . . . ... │ . . . . . . . . . . . . . . . . . . . . v \u0026lt;-------------------- time ---------------------\u0026gt;  水平方向: 时间以及该时间对应的垂直方向上的维度标示符构成的标示符空间\n垂直方向: 多个标签构成的维度空间\n由于prometheus的数据源，是周期性的采集一组时序数据的当前值。每个批量采集的实体对象称之为一个target，（比如，采集某个机器上的所有的基础监控指标）。因此，数据写入 模式是，\n  垂直方向多维度并发写。 比如一次采集一个机器的磁盘，内存，网络，等多个维度的数据，这些数据对应的是同一时刻点上某个机器的指标值。从维度方向看，是并发的。\n  多个target独立 多个target，即多个机器数据采集之间是相互独立的。\n  读写特性分析 写特性 每秒采集数成千上万的监控对象，这些样本数据写入模式是完全垂直高度并发。分散的写入单个数据点到磁盘非常缓慢，需要实现按顺序写入更大的数据块。即便写入采用SSD磁盘，也只能每次写入4KiB 或更大的页，不能修改单个字节。写一个16字节的样本同写一个完整的4KiB 的页没有区别。 而且不仅存在 写入放大，很快会 损坏硬件。\n结论： 顺序和批量写入是spinning disks和SSD的理想写入模式。 这是一个应该遵循的简单规则。\n读特性 如上文的分析，查询通用的场景：\n 查询单个序列某个特定时间的数据 在 N（N=～10000）个序列里查询一个单个的数据点 在单个序列里查询几周的数据点 甚至在10000个序列里查询几周的数据点  因此在我们的二维平面上，查询既不是完全垂直的，也不是水平的，而是二者的矩形组合。 Recording Ruel （即针对特定复杂聚合数据的查询做预计算），可以减轻已知的一些查询方面的问题，但是仍然不是临时查询（ad-hoc queries）的一个通用解决方案，这些查询也必须能很好的进行下去. 在理想情况下，相同序列的样本数据将会被顺序存储，这样一来我们便可以用尽可能少的读来扫描得到它们。 在上层，我们只需要知道这个序列可以访问的所有数据点的开始位置。\n结论： 在将收集的数据写入磁盘的理想模式和为服务的查询操作提供更显著有效的存储格式之间显然存在着强烈的冲突。这是我们的时间序列数据库要解决的根本问题\n解决方案 V2 及以前的解决方案 V2 版本的核心点是基于 Per Series Per File. 即每个时间序列创建一个文件，此文件 会按顺序存放这个序列的所有采样数据。 由于时间序列数据的特性顺序和批量写 是理想模式，因此，在写入引入了chunk 的概念。在内存中，构建了批量大小为1KiB的 chunk,当这个chunk 块被填满后，将他们追加到对应的序列文件中。 此方案，解决了大部分问题，并支持 Gorilla的高效压缩算法。\n数据写入示意图如下：\n ┌──────────┬─────────┬┬─────────┬─────────┐ series A └──────────┴─────────┴┴─────────┴─────────┘ ┌──────────┬─────────┬┬─────────┬─────────┐ series B └──────────┴─────────┴┴─────────┴─────────┘ . . . ┌──────────┬─────────┬─────────┬─────────┬┬─────────┐ series XYZ └──────────┴─────────┴─────────┴─────────┴┴─────────┘ chunk 1 chunk 2 chunk 3 ...  V2 版本的数据结构存在的问题：\n \u0026ldquo;序列分流\u0026rdquo;（series churn）会导致维护的文件数据多于实际采集数据的序列数量。随着时间的推移，最终导致 inode 耗尽的问题 即便做了分块，每秒也会产生数以千计的数据块并且准备好被持久化。这仍然需要每秒完成几千次单独的磁盘写操作。尽管，可以通过批量写多个chunks 来缓解部分压力，但这发而会增加等待持久化数据的总内存占用量 保持打开所有文件来读取和写入是不可行的。特别是因为在24小时后超过99%的数据便不再会被查询。如果它还是被查询到的话，我们就不得不打开数千个文件，查找和读取相关的数据点到内存，然后再重新关闭它们。而这样做会导致很高的查询延迟。 过期数据的删除，带来写放大。旧数据必须得被清理掉，而且数据需要从数百万的文件前被抹除。这意味着删除实际上是写密集型操作。此外，循环地在这数百万的文件里穿梭然后分析它们，会让这个过程常常耗费数个小时。在完成时有可能还需要重新开始。删除旧文件将会给你的SSD带来进一步的写入放大！ 应用服务崩溃，引发数据丢失，基于checkpoint 恢复，也会带来漫长的启动周期。 当前堆积的数据块只能放在内存里。如果应用崩溃的话，数据将会丢失。为了避免这种情况，它会定期地保存内存状态的检查点（Checkpoint）到磁盘，这可能比我们愿意接受的数据丢失窗口要长得多。从检查点恢复估计也会花上几分钟，造成痛苦而漫长的重启周期。  每个时间序列对应一个单个文件的方式使得单个查询很容易就击垮Prometheus的进程。而当所要查询的数据没有缓存到内存时，被查询序列的文件会被打开，然后包含相关数据点的数据块会被读取到内存里。倘若数据量超过了可用内存，Prometheus会因为OOM被杀死而退出。\nV3 版本的解决方案 V3 宏观设计 $ tree ./data ./data ├── b-000001 │ ├── chunks │ │ ├── 000001 │ │ ├── 000002 │ │ └── 000003 │ ├── index │ └── meta.json ├── b-000004 │ ├── chunks │ │ └── 000001 │ ├── index │ └── meta.json ├── b-000005 │ ├── chunks │ │ └── 000001 │ ├── index │ └── meta.json └── b-000006 ├── meta.json └── wal ├── 000001 ├── 000002 └── 000003  先看下V3 宏观的数据结构。有一些编号的Block，都有一个 b- 前缀，每个Block ,都维护一个包含索引的文件以及一个包含 chunks 目录，chunks 目录包含了多个 chunk 文件。chunk 文件类似V2 版本的 chunk，这样可以用非常低的成本来读取一个时间窗口里的序列数据，并且允许我们采用相同的有效压缩算法，很显然，这里不再是每个序列对应一个单个文件，取而代之的是，几个文件包含许多 序列的 chunks。\u0026ldquo;index\u0026quot;文件，包含了大量的黑魔法，允许我们找出标签，它们可能的值，整个时间序列，以及存放数据的 chunk 。\n关键的两个问题：\n 为何有几个目录包含 index 和 chunk 的文件布局呢？ 为何最后一个包含一个 wal 目录？  很多小的数据库（Many Little Databases） 将序列数据的水平维度，即时间空间分割成非重叠的block。每个block当成一个完全独立的数据库，包含其时间窗口的所有时间序列数据。因此，它有自己的索引和一组chunk 文件。\n每个block 的数据是不可变的。当然，对于最新采集的数据，我们必须能够将对应的序列和采样数据写入到最近的block里。对于最新的block , 所有的新数据被写入在内存数据库 中，它像持久化的block 一样提供了查询的特性。内存数据结构能更有效的更新。为了防止数据丢失，所有传入的数据也会写入一个临时的WAL，即“wal”目录中的一组文件，在重新启动时可以从中重新填充内存中的数据库。\n所有这些文件都带有它们自己的序列化格式，这与我们所期望的一样：许多标志、偏移量、变量和CRC32校验和。\n这种布局允许我们对与查询的时间范围相关的所有 block 发出查询.每个 block 的部分结果合并在一起，形成整体结果。\nThis layout allows us to fan out queries to all blocks relevant to the queried time range. The partial results from each block are merged back together to form the overall result.  这种水平分区添加了一些强大的功能：\n  当查询一个时间范围时，我们可以很容易地排除掉在次时间区间之外的block. 简单的解决了序列分流(seriers churn) 的问题\u0026ndash;因为减少了数据集的起始时间。\n  当完成一个block ,就可以 通过顺序的写入几个大文件来持久化 内存中的数据库， 从而避免了写放大，并且对 SSDs 盘 和 HDDs 盘一样友好。\n  保留了 V2 版本中好的一个特性，即最近查询较多的 chunks, 作为热数据而存内存中。\n  更棒的是，不用在绑定固定的1Ki\u0008B 的 chunk 大小来以更好做磁盘的数据对齐。可以选择对对个数据点和选择的压缩格式最匹配的任何大小。\n  删除过期数据变得更廉价和及时。在之前，删除过期数据，必须要分析多达数亿个文件， 可能需要几小时才能聚合，而现在只需要删除当一个目录就可以。\n  V3 版本中的关键技术点 mmap 从上百万个小文件到几个大文件的转化，让我们用很小的开销就能保持所有的文件打开。这使我们解锁了 mmap（2)的使用。mmap 是一个系统调用，可以通过虚拟内存区建立到磁盘文件的映射，可以简单看作一个交换空间。 这意味着我们可以将数据库中的所有内容视为内存中的内容，而不占用任何物理RAM。只有当我们访问数据库文件中的某些数据时，操作系统才会从磁盘做数据懒加载。\n在机器的内存资源充足的时，prometheus 可以积极的将数据缓存到内存中，在内存紧张， 其它应用需要时内存时候，prometheus缓存的数据可以被驱逐出去，将内存返回给其它应用。直接将持久化数据装入RAM 比这种查询更多持久化的数据更容易造成我们进程的OOM。 内存的缓存大小变得完全自适应，只有真正查询时才会加载数据。\n压缩(Compaction) 存储服务，周期的切割出一个新的block, 并将前一个block （当前已经完成的）写入磁盘。只有当block 成功持久化后，用于还原内存block 的WAL 才会被删除。\n为了避免在内存中积累过多的数据，我们希望每个block都保持合理的大小.(通常默认2小时) 当查询多个block时候，我们需要将结果合并到一个整体结果中。这个合并过程必然要付出代价。一周范围的查询，合并的block不应超过80+个部分结果。\n为了兼顾二者,（ 1. 内存block太大就会造成索引占用过多内存，2. block 块太小，会导致查询聚合的block 过多）我们引入了压缩，Compaction.\nCompaction 描述了获取一个或多个数据block,并将其写入一个可能更大的数据Block的过程。在此期间，可以修改现有数据，比如删除，或重构样本的chunk 来提升查询性能。\n如上图，可以有两种策略的压缩，chunk [1,2,3,4] 有序， 一种将 [1,2],[3,4] 做合并，或者 [1,2,3],[4] 合并\n合并后，显著的降低了查询时的聚合成本，因为需要合并的部分结果更少。\n保留策略(Retention)  ┌────────────┐ ┌────┼─────┐ ┌───────────┐ ┌───────────┐ │ 1 │ │ 2 | │ │ 3 │ │ 4 │ . . . └────────────┘ └────┼─────┘ └───────────┘ └───────────┘ | | retention boundary  V2 版本中，删除过期数据会对CPU，内存，磁盘都带来压力，在此版本中，删除变得很简单，只需要删除过期的目录即可。如上图，block 1 可以删除，block 2 需要等到过期时间完成覆盖block 后才可以删除。\n由于我们持续做Compaction, 当数据越旧，block 可能变得越大。因此，必须设置一个上限，防止block 不会扩展到整个数据库，而丧失我们设计初衷带来的好处。\n这个值究竟设置多大何合适呢？当将最大块大小设置为总保留时间窗口的10%。 如上图中，block 2，总的block 大小设置为保留窗口的 10% ，在保留窗口外部的block大小也被限制在了10% 以内。\n简而言之，通过block 的方式，将非常昂贵的保留删除操作变得非常廉价。\nnote: 在内存中批量处理数据，通过WAL 追踪，周期的刷入磁盘，定期合并的模式 非常普遍。 无论数据的领域细节如何，我们看到的好处几乎普遍适用。遵循这种方法的开源数据库 如：LevelDB,Cassandra,Influxdb,Hbase等。  索引(Index) 动机 研究存储改进的最初动机来自 \u0026ldquo;序列流失\u0026rdquo; (Seriers Chrun) 带来的问题。 基于 block 的布局减少了服务查询时必须考虑的序列总数\u0026ndash;因为大部分的查询总是对近期数据的查询，然而跨越整个时间范围的查询仍然很慢。 从时间序列查询的特性分析可知，大部分的查询场景需要： 一个更有效的倒排索引。\n查询复杂度分析 倒排索引，提供了基于数据项的子集快速查找数据项的能力。举例来说，如果想查询带有标签 app = \u0026ldquo;nginx\u0026quot;的序列，不需要遍历所有的序列，挨个检查是否他们是否包含这个标签， 而只是维护一个 以 app = \u0026ldquo;nginx\u0026rdquo; 这个标签的一个集合，判断给定的序列是否在这个集合内。\n为此，每个序列都被分配了一个唯一的ID，通过这个ID可以在恒定的时间内检索它，即 O(1)。在这种情况下，ID 是我们的 正排索引。\n Example: If the series with IDs 10, 29, and 9 contain the label app=\u0026quot;nginx\u0026rdquo;, the inverted index for the label “nginx” is the simple list [10, 29, 9], which can be used to quickly retrieve all series containing the label. Even if there were 20 billion further series, it would not affect the speed of this lookup.\n 如果n是序列的总数，m是给定查询的结果大小，那么使用索引的查询的复杂性现在是O(m)。查询按其检索的数据量（m）而不是搜索的数据体（n), m 通常比 n 要小得多.\n组合标签问题 如何我们的序列总数为N，需要查询的M个标签的组合条件，通常的算法复杂度为 O(N^N), 假设M个标签\n问题: 若我们需要查询M个标签组合是否在N个序列中，通常的算法复杂度，O(N^M),如果有倒排索引，则变为O(M^M),最差情况，每个M的集合和N重合，则 O(N^N)。\n解决方案:\n如何我们的序列是 ID 是全局有序的,就会变的很有趣  假设：我们需要查找服务名为 foo 的 http 请求数的指标，\n __name__ = \u0026quot;requests_total \u0026quot; AND app = \u0026quot;foo\u0026quot;, 这两个标签对应的倒排索引如下图：  __name__=\u0026quot;requests_total\u0026quot; -\u0026gt; [ 9999, 1000, 1001, 2000000, 2000001, 2000002, 2000003 ] app=\u0026quot;foo\u0026quot; -\u0026gt; [ 1, 3, 10, 11, 12, 100, 311, 320, 1000, 1001, 10002 ] intersection =\u0026gt; [ 1000, 1001 ]  他们的交集会非常小。上述问题转变为在两个集合中，找到交集。我们可以通过在每个列表的开头设置一个光标，并总是在较小的数字处前进一个来找到它。当两个数字相等时，我们将数字加到结果中，并向前推进两个游标。 这种方式，总的时间成本为 O（2n）= O (n),因为我们只在其中一个列表中前进。\n以上方案，推广到M个标签组合场景，最坏情况下的时间复杂度为 O(M*N), 而之前是 O（M^N）\n为什么不对标签进行压缩？\n以上的规程其实是规范搜索引擎的一个简化版本，几乎所有的全文搜索引擎都在用这种方式。 每个序列描述符都被视为一个简短的“文档”，每个标签（名称+固定值）都被视为其中的一个“单词”。我们可以忽略搜索引擎索引中通常会遇到的许多附加数据，例如单词位置和频率数据。\n对于改善实际运行时的方法，似乎有着无尽的研究，通常会对输入数据做出一些假设。有很多技术可以压缩倒排索引，这些倒排索引都有各自的优点和缺点。由于我们的“文档”很小，而且“单词”在所有系列中都有很大的重复性，压缩变得几乎无关紧要。例如，一个拥有约440万个系列、约12个标签的真实数据集的唯一标签不到5000个。对于我们最初的存储版本，我们坚持基本的方法，不进行压缩，只添加了一些简单的调整，以跳过大范围的非相交id。\n虽然保持id的排序听起来很简单，但它并不总是保持不变的。例如，V2存储将序列的Hash作为id分配给新序列，因此无法有效地构建排序的反向索引。\n另一项艰巨的任务是在数据被删除或更新时修改磁盘上的索引。通常，最简单的方法是简单地重新计算和重写它们，但是这样做的同时需要保持数据库的可查询性和一致性。 V3存储正是通过为每个块提供一个单独的不可变索引来实现这一点的，该索引只在压缩时通过重写进行修改。只需要更新完全保存在内存中的可变块的索引。\n扩展阅读  Writing a Time Series Database from Scratch\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2ad1d85ff47a546e23e6954c17aca569","permalink":"https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/promethesutsdb/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/courses/tsdb/chapter2/prometheustsdb/promethesutsdb/","section":"courses","summary":"数据模型 时间序列数据 identifier -\u0026gt; (t0, v0), (t1, v1), (t2, v2), (t3, v3), .... 每个数据点都是一个","tags":null,"title":"Prometheus Tsdb 分析","type":"docs"},{"authors":null,"categories":null,"content":"作业系统的背景及需求 K8s 里面，最小的调度单元是 Pod，可以直接通过 Pod 来运行任务进程，但面临以下几个问题：\n 我们如何保证 Pod 内进程正确的结束？ 如何保证进程运行失败后重试？ 如何管理多个任务，且任务之间有依赖关系？ 如何并行地运行任务，并管理任务的队列大小？  K8s对作业系统的抽象-Job  kubernetes 的 Job 是一个管理任务的控制器，它可以创建一个或多个 Pod 来指定 Pod 的数量，并可以监控它是否成功地运行或终止； 我们可以根据 Pod 的状态来给 Job 设置重置的方式及重试的次数； 根据依赖关系，保证上一个任务运行完成之后再运行下一个任务； 还可以控制任务的并行度，根据并行度来确保 Pod 运行过程中的并行次数和总体完成大小  K8s 作业系统 功能 restartPolicy解析：\n Never: Job 需要重新运行 OnFailure: 失败的时候再运行，再重试可以用 Always: 不论什么情况下都重新运行时 backoffLimit: 就是来保证一个 Job 到底能重试多少次  架构设计 DeamSet 管理模式 DeamSet 控制器 ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"d5f02eb67511cb4fa8e8e92da19392ce","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/job/job/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/job/job/","section":"containertechnology","summary":"作业系统的背景及需求 K8s 里面，最小的调度单元是 Pod，可以直接","tags":null,"title":"作业任务系统","type":"docs"},{"authors":null,"categories":null,"content":"目标 一个公司想要多，快，好，省的快速发展，离不开基础设施及技术的支撑。从业务视角来看，包括敏捷管理体系和 持久交付体系。敏捷管理体系会将公司的业务，和对应的研发有效的组织和结合起来。持久交付体系，包含软件全生命 周期的管理，以及服务的持续反馈和运营。\n持续反馈的载体为监控系统。监控系统，作为运维的质量把控的核心平台，核心就是为公司服务的稳定性保驾护航，第一时间发现 问题或问题的隐患，将故障扼杀在摇篮，或将其的影响面缩小的最小范围。为了实现这个目标，我们可以从两个维度来思考展开，即 产品和技术维度。\n产品维度 从产品维度来看，我们需要有宏观全局视角，也需要能灵活的下钻能力，实现多维度逐层展开，快速定位的能力。\n全局视角：大的来说可以包含三个维度视角\n 业务全链路拓扑，健康红绿盘  业务服务的全链路，能反应公司主要业务线的黄金链路上服务的健康状态，通过带有拓扑关系的红绿大盘来体现。 首级页面，若出现问题，可以下钻到对应的二级指标详情页。二级页面能体现当前关注 服务的上下游的健康红绿状态，以及自身黄金指标的状态的变迁历史。自身黄金指标的来源，分类三大类： Log 类，Tracing类，以及Metrics 类。通过统一的数据模型，将三者统一。至于关键的异常跃迁点： 需要有两个能力： 其一： 在此时间区域内，相关服务，或相关指标的趋势图 其二：在此时间去区间内，相关服务，资源，的异常事件能关联出来，突出的事件可以通过特殊颜色高亮展示，点击这些异常事件，可以下钻到对应的日志详情页，查看对应的日志。通常，\n 比较突出的值得关注事件包括：该时间点内的发布变更操作，配置变更事件，网络变更等信息 业务的问题排查链路包括：聚合后的高层指标信息，服务入口流量-\u0026gt;接入层负载-\u0026gt;核心服务-\u0026gt;核心服务的接口-\u0026gt;对应的实例-\u0026gt;对应的资源  网络全链路拓扑，及健康红绿盘  有一个入口的全局视角，能直观的反应一个集团的的网络拓扑流量走势，能通过健康红绿盘看到核心骨干网络的网络质量及数据， 如果有异常，能快速定位。\n 全局骨干网的流量拓扑，质量 关键业务节点之间的PingMash 矩阵，能直观的反应可能出现网络质量异常 专线，关键网络链路质量  从服务的运营视角，来看的线上核心服务的运营数据和对应资源水位及容量，核心节点的负载状态。  核心业务服务的关联的资源利用率情况，以及资源水位，实时负载和基线的动态关系。\n统一的配置管理：\n技术维度   统一数据模型。所有的数据都将可以转化为标准的 KV模型 提供统一的数据准入标准，但不限制准入的数据采集具体实现。事实上，数据的采集需要多方共建,但应该遵循统一的准入标准。\n  实现底层数据银行的中台，方便的打通各个业务之间的数据壁垒。\n  去中心化的架构，能实现任意节点的业务数据接入 所有的数据都是按需索取，摒弃了基于中心转发，数据动态路由的方式。\n  并提供统一灵活强大的查询引擎。支持受限类SQL的查询语法，普通的开发，业务人员既能快速入手，也能高度灵活的去定制自己所需的 业务报表类的数据\n  能更具业务数据的重要程度，实时性能，灵活的选择分层的存储媒介，缩减存储成本\n  运维周边生态 监控系统的基本模型：输入-\u0026gt;计算/决策-\u0026gt;输出。输入的来源可以是我们的业务质量信息，可以是运维其它系统的事件，通过 多维度的关联计算，或基于人工输入的计算规则，通过规则执行引擎得出结论，对当前的事情产生决策输出。输出的控制信号 可以对接运维的命令管道系统，从而对系统当前的状态做适度的调节。从输入-\u0026gt;计算/据测-\u0026gt;输出-\u0026gt;执行-\u0026gt;输入，完成一个闭环。\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"9e573b9be8480c04aa1fc6df2008bed9","permalink":"https://meixinyun.github.io/programmertalk/courses/monitor/ecology/ecology/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/courses/monitor/ecology/ecology/","section":"courses","summary":"目标 一个公司想要多，快，好，省的快速发展，离不开基础设施及技","tags":null,"title":"监控生态","type":"docs"},{"authors":null,"categories":null,"content":"目标  channel 的设计目标和设计原则 channel 的数据结构 channel 的数据收发逻辑 总结  channel 的设计目标和设计原则 设计目标 channel 本质上是一个高性能的无锁缓存管道。通过通信的方式，解决多个goroutine 之间的数据共享问题。\n 不要通过共享内存的方式进行通信，而是应该通过通信的方式共享内存  既然是内存channel，需要解决如下的问题：\n 数据的有序性，FIFO 性能，多个Goroutine 向同一个channel 收发数据时锁和调度的优化  设计原则 -FIFO 多个Goroutine 向同一个channel 收发数据的原则先进先出\n 先从 Channel 读取数据的 Goroutine 会先接收到数据； 先向 Channel 发送数据的 Goroutine 会得到先发送数据的权利；  但是 Go 语言稍早版本的实现却不是严格遵循这一语义的， runtime: make sure blocked channels run operations in FIFO order,中提出了有缓冲区的 Channel 在执行收发操作时没有遵循 FIFO 的规则,带缓冲区的channel 读写机制，\n 发送方会向缓冲区中写入数据，然后唤醒接收方，多个接收方会尝试从缓冲区中读取数据，如果没有读取到就会重新陷入休眠； 接收方会从缓冲区中读取数据，然后唤醒发送方，发送方会尝试向缓冲区写入数据，如果缓冲区已满就会重新陷入休眠；  这种基于重试的机制会导致 Channel 的处理不会遵循 FIFO 的原则。经过 runtime: simplify buffered channels 和 runtime: simplify chan ops, take 2 两个提交的修改，带缓冲区和不带缓冲区的 Channel 都会遵循先入先出对数据进行接收和发送3 4。\n设计原则-锁优化 一般来说，会通过 CAS(compare-and-swap 或者 compare-and-set)自旋锁的操作,来实现无锁队列。\n从本质上来说，Channel 是一个用于同步和通信的有锁队列，使用互斥锁解决程序中可能存在的线程竞争问题是很常见的，我们能很容易地实现有锁队列。但锁导致的休眠和唤醒会带来额外的上下文切换。\n1994 年的论文 Implementing lock-free queues 就研究了如何使用无锁的数据结构实现先进先出队列，而 Go 语言社区也在2014 年提出了无锁 Channel 的实现方案，该方案将 Channel 分成了以下三种类型8：\n 同步 Channel — 不需要缓冲区，发送方会直接将数据交给（Handoff）接收方； 异步 Channel — 基于环形缓存的传统生产者消费者模型； chan struct{} 类型的异步 Channel — struct{} 类型不占用内存空间，不需要实现缓冲区和直接发送（Handoff）的语义；  这个提案的目的也不是实现完全无锁的队列，只是在一些关键路径上通过无锁提升 Channel 的性能。社区中已经有无锁 Channel 的实现9，但是在实际的基准测试中，无锁队列在多核测试中的表现还需要进一步的改进10。\n因为目前通过 CAS 实现11的无锁 Channel 没有提供 FIFO 的特性，所以该提案暂时也被搁浅了12。\nchannel 的数据结构 channel 数据结构 classDiagram class hchan { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; ... uint qcount // Channel 中的元素个数 uint dataqsiz // Channel 中的循环队列的长度 buf unsafe.Pointer // Channel 的缓冲区数据指针 sendx uint // Channel 的发送操作处理到的位置 recvx uint // 接收操作处理的位置 recvq waitq // 由于缓冲区空间不足而阻塞的 Goroutine 列表 sendq waitq // list of send waiters ... } class sudog { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; } class waitq { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; first *sudog last *sudog } waitq --* hchan sudog --* waitq  type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 //当前 Channel 能够收发的元素的大小 elemtype *_type // element type //当前 Channel 能够收发的元素的类型 sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters closed uint32 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex }  hchan 是实现channel 的数据结构，包括一个环形buffer队列，该buffer 大小，收发元素对应的操作位置，以及channel的 元素类型和大小。 sendq 和 recvq 存储了当前 Channel 由于缓冲区空间不足而阻塞的 Goroutine 列表，这些等待队列使用双向链表 runtime.waitq 表示,\ntype waitq struct { first *sudog last *sudog }  sudog 代表了在某个channel 上收发数据而处于等待的goroutine。\nsudog 是为了解决 goroutine 和 同步对象之间多对多的关系，而抽象出的概念。 一个因此一个goroutine可能在多个等待列表中， 因此一个goroutine 对应多个 sudogs, 多个goroutine 可能等待同一个同步对象，因此，也会存在多个 sudog 对应 一个对象。 sudog 通过一个特殊池来分配. 通过 acquireSudog 和 releaseSudog 来分配和释放。\nchannel 初始化 Channel 的创建都会使用 make 关键字，编译器会将 make(chan int, 10) 表达式被转换成 OMAKE 类型的节点，并在类型检查阶段将 OMAKE 类型的节点转换成 OMAKECHAN 类型 根据 Channel 中收发元素的类型和缓冲区的大小初始化 runtime.hchan 结构体和缓冲区：\n上述代码根据 Channel 中收发元素的类型和缓冲区的大小初始化 runtime.hchan 结构体和缓冲区：\n如果当前 Channel 中不存在缓冲区，那么就只会为 runtime.hchan 分配一段内存空间； 如果当前 Channel 中存储的类型不是指针类型，就会为当前的 Channel 和底层的数组分配一块连续的内存空间； 在默认情况下会单独为 runtime.hchan 和缓冲区分配内存； 在函数的最后会统一更新 runtime.hchan 的 elemsize、elemtype 和 dataqsiz 几个字段\ngraph TD A[make] --\u0026gt;|编译器| B(OMAKECHAN) B --\u0026gt; C{是否带缓冲} C --\u0026gt;|带缓冲区的异步 Channel| D[-] C --\u0026gt;|不带缓冲区的同步 Channel| E[-] D --\u0026gt; F[runtime.makechan] E --\u0026gt; F[runtime.makechan] F --\u0026gt; G[ 根据 Channel 中收发元素的类型和缓冲区的大小初始化 runtime.hchan 结构体和缓冲区] G --\u0026gt;H{发送类型和缓冲区} H --\u0026gt; |Channel 不存在缓冲区| I1[为hchan 分配一段内存空间] H --\u0026gt; |Channel 中存储的类型不是指针类型| I2[为hchan 和底层数组分配一块连续的内存空间] H --\u0026gt; |默认| I3[为hchan 和缓冲区分配内存] I1 --\u0026gt; J[统一更新elemsize,elemtype 和 dataqsiz ] I2 --\u0026gt; J[统一更新elemsize,elemtype 和 dataqsiz] I3 --\u0026gt; J[统一更新elemsize,elemtype 和 dataqsiz ]  channel 的数据发送 使用 ch \u0026lt;- i 语句，编译器会将它解析成 OSEND 节点并在 cmd/compile/internal/gc.walkexpr 函数中转换成 runtime.chansend1：\nchansend1 会调用runtime.chansend 并传入Channel 和需要发送的数据。runtime.chansend 是向 Channel 中发送数据时最终会调用的函数，这个函数负责了发送数据的全部逻辑，如果我们在调用时将 block 参数设置成 true，那么就表示当前发送操作是一个阻塞操作：\n func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { lock(\u0026amp;c.lock) if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026quot;send on closed channel\u0026quot;)) }  在发送数据的逻辑执行之前会先为当前 Channel 加锁，防止发生竞争条件。如果 Channel 已经关闭，那么向该 Channel 发送数据时就会报\u0026quot;send on closed channel\u0026rdquo; 错误并中止程序。\n直接发送 如果目标 Channel 没有被关闭并且已经有处于读等待的 Goroutine，那么 runtime.chansend 函数会从接收队列 recvq 中取出最先陷入等待的 Goroutine 并直接向它发送数据：\nif sg := c.recvq.dequeue(); sg != nil { send(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true }  发送数据时会调用 runtime.send，该函数的执行可以分成两个部分：\n  调用 runtime.sendDirect 函数将发送的数据直接拷贝到 x = \u0026lt;-c 表达式中变量 x 所在的内存地址上；\n  调用 runtime.goready 将等待接收数据的 Goroutine 标记成可运行状态 Grunnable 并把该 Goroutine 放到发送方所在的处理器的 runnext 上等待执行，该处理器在下一次调度时就会立刻唤醒数据的接收方；\n  // send processes a send operation on an empty channel c. // The value ep sent by the sender is copied to the receiver sg. // The receiver is then woken up to go on its merry way. // Channel c must be empty and locked. send unlocks c with unlockf. // sg must already be dequeued from c. // ep must be non-nil and point to the heap or the caller's stack. func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { ... if sg.elem != nil { sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 { sg.releasetime = cputicks() } goready(gp, skip+1) }  缓冲区 如果创建的 Channel 包含缓冲区并且 Channel 中的数据没有装满，就会执行下面这段代码\n ... if c.qcount \u0026lt; c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled { raceacquire(qp) racerelease(qp) } typedmemmove(c.elemtype, qp, ep) c.sendx++ // 如果发送处理位置到达达到处理元素的最大值，就置为零，环形Buffer if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(\u0026amp;c.lock) return true } ...  首先会使用 chanbuf 计算出下一个可以存储数据的位置，然后通过 runtime.typedmemmove 将发送的数据拷贝到缓冲区中并增加 sendx 索引和 qcount 计数器。\n阻塞发送 当 Channel 没有接收者能够处理数据时，向 Channel 发送数据就会被下游阻塞，当然使用 select 关键字可以向 Channel 非阻塞地发送消息。向 Channel 阻塞地发送数据会执行下面的代码，我们可以简单梳理一下这段代码的逻辑：\nfunc chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if !block { unlock(\u0026amp;c.lock) return false } //调用 runtime.getg 获取发送数据使用的 Goroutine； gp := getg() mysg := acquireSudog() mysg.elem = ep mysg.g = gp mysg.c = c gp.waiting = mysg c.sendq.enqueue(mysg) goparkunlock(\u0026amp;c.lock, waitReasonChanSend, traceEvGoBlockSend, 3) gp.waiting = nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true }  小结   调用时将 block 参数设置成 true，那么就表示当前发送操作是一个阻塞操作\n  在发送数据的逻辑执行之前会先为当前 Channel 加锁，防止发生竞争条件, 如果 Channel 已经关闭，那么向该 Channel 发送数据时就会报\u0026quot;send on closed channel\u0026rdquo; 错误并中止程序\n  如果当前 Channel 的 recvq 上存在已经被阻塞的 Goroutine，那么会直接将数据发送给当前的 Goroutine 并将其设置成下一个运行的 Goroutine；\n  如果 Channel 存在缓冲区并且其中还有空闲的容量，我们就会直接将数据直接存储到当前缓冲区 sendx 所在的位置上；\n  如果不满足上面的两种情况，就会创建一个 runtime.sudog 结构并将其加入 Channel 的 sendq 队列中，当前 Goroutine 也会陷入阻塞等待其他的协程从 Channel 接收数据；\n  发送数据的过程中包含几个会触发 Goroutine 调度的时机：\n 发送数据时发现 Channel 上存在等待接收数据的 Goroutine，立刻设置处理器的 runnext 属性，但是并不会立刻触发调度； 发送数据时并没有找到接收方并且缓冲区已经满了，这时就会将自己加入 Channel 的 sendq 队列并调用 runtime.goparkunlock 触发 Goroutine 的调度让出处理器的使用权；  channel 的数据接收逻辑 Go 语言中可以使用两种不同的方式去接收 Channel 中的数据：\ni \u0026lt;- ch i, ok \u0026lt;- ch  这两种不同的方法经过编译器的处理都会变成 ORECV 类型的节点，后者会在类型检查阶段被转换成 OAS2RECV 类型。数据的接收操作遵循以下的路线图：\nstateDiagram [*] --\u0026gt; ch ch --\u0026gt; ORECV ORECV --\u0026gt; OAS2RECV ORECV --\u0026gt; chanrecv1 OAS2RECV --\u0026gt; chanrecv2 chanrecv1 --\u0026gt; chanrecv chanrecv2 --\u0026gt; chanrecv  虽然不同的接收方式会被转换成 runtime.chanrecv1 和 runtime.chanrecv2 两种不同函数的调用，但是这两个函数最终还是会调用 runtime.chanrecv。\n小结 我们梳理一下从 Channel 中接收数据时可能会发生的五种情况：\n如果 Channel 为空，那么就会直接调用 runtime.gopark 挂起当前 Goroutine； 如果 Channel 已经关闭并且缓冲区没有任何数据，runtime.chanrecv 函数会直接返回； 如果 Channel 的 sendq 队列中存在挂起的 Goroutine，就会将 recvx 索引所在的数据拷贝到接收变量所在的内存空间上并将 sendq 队列中 Goroutine 的数据拷贝到缓冲区； 如果 Channel 的缓冲区中包含数据就会直接读取 recvx 索引对应的数据； 在默认情况下会挂起当前的 Goroutine，将 runtime.sudog 结构加入 recvq 队列并陷入休眠等待调度器的唤醒；\n我们总结一下从 Channel 接收数据时，会触发 Goroutine 调度的两个时机：\n 当 Channel 为空时； 当缓冲区中不存在数据并且也不存在数据的发送者时；  总结 参考文献  Dmitry Vyukov. Jan, 2014. “Go channels on steroids” \n Ahmed W. A scalable lock-free channel.\n 8 Dmitry Vyukov. Jan, 2014. “Go channels on steroids”\n 9 Ahmed W. A scalable lock-free channel.\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"b0c5e0410bbcb13be41c70a6646a651b","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/chapter3/channel/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/golang/chapter3/channel/","section":"courses","summary":"并发编程 Channel","tags":null,"title":"Channel","type":"docs"},{"authors":null,"categories":null,"content":"概览 程序中的数据和变量都会被分配到程序所在的虚拟内存中，内存空间包含两个重要区域 — 栈区（Stack）和堆区（Heap）\n函数调用的参数、返回值以及局部变量大都会被分配到栈上，这部分内存会由编译器进行管理；\n堆中的对象由内存分配器分配并由垃圾收集器回收。\n设计原理 内存管理一般包含三个不同的组件，分别是用户程序（Mutator）、分配器（Allocator）和收集器（Collector） 1,当用户程序申请内存时，它会通过内存分配器申请新的内存，而分配器会负责从堆中初始化相应的内存区域。\nstateDiagram Mutator --\u0026gt; Allocator Allocator --\u0026gt; Heap Collector --\u0026gt; Heap  目标 了解内存分配器的分配方法以及Go语言内存分配器的分级分配方法，虚拟内存布局和地址空间\n分配方法 graph TD 内存分配 --\u0026gt; C{分配方法} C --\u0026gt;|线性分配| D[Sequential Allocator/Bump Allocator] C --\u0026gt;|链表分配| E[Free-List Allocator]  参考文献  1. Dmitry Soshnikov. Feb 2019. “Writing a Memory Allocator”\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"e08d4d174b11983315d928ef4007ae98","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/chapter7/overview/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/golang/chapter7/overview/","section":"courses","summary":"概览","tags":null,"title":"概览","type":"docs"},{"authors":null,"categories":null,"content":"云原生技术发展历史 云原声技术范畴 关键技术点 ","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557014400,"objectID":"41db44a413be1d9d8580ad964ee3bec8","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/cncfoverview/cncfoverview/","publishdate":"2019-05-05T01:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/cncfoverview/cncfoverview/","section":"containertechnology","summary":"云原生技术发展历史 云原声技术范畴 关键技术点","tags":null,"title":"云原生技术概览","type":"docs"},{"authors":null,"categories":null,"content":"目标  核心类图 设计思路分析 客户端视角-数据的写入流程 DB初始化流程 关键内存对象分析 总结  核心类图 classDiagram Appender \u0026lt;|-- headerAppender : Implementation \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Appender Appender : Add(l labels.Labels, t int64, v float64) (uint64, error) Appender : AddFast(ref uint64, t int64, v float64) error Appender : Commit() error Appender : Rollback() error class DB { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; ... lockf fileutil.Releaser metrics *dbMetrics opts *Options ... compactor Compactor blocks []*Block head *Head ... } class Head { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; ... wal *wal.WAL series *stripeSeries ... symbols map[string]struct ... ref uint64 lset labels.Labels ... } class memSeries { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; ... chunks []*memChunk headChunk *memChunk ... app chunkenc.Appender ... postings *index.MemPostings ... } class dbAppender { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; Appender db *DB } class stripeSeries { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; series [stripeSize]map[uint64]*memSeries hashes [stripeSize]seriesHashmap locks [stripeSize]stripeLock } class Block { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; ... meta BlockMeta chunkr ChunkReader indexr IndexReader tombstones TombstoneReader ... } memSeries --* stripeSeries stripeSeries --* Head Block --* DB Head --* DB Appender --* dbAppender DB --* dbAppender  设计思路分析 Appender 实现对时间序列采样数据写操作的抽象和封装，提供Add/AddFast/Commit/Rollback的核心能力\n通过DB 这个类，实现对落在某个时间窗口内的时间序列数据的读写操作。 通过 DB + Appender 的组合，就能提供时序数据库的写入逻辑， 可以通过 BD + Query 的组合，完成时间序列数据库的读逻辑。 可以看到，系统的顶层设计非常优秀，秉持了高内聚，低耦合的原则。\nDB内部通过Head,Block,Compactor 等关键几个抽象类，完成数据内存模型的的封装。 Head 及其相关的类，实现了时间序列数据在内存中的读写，存储。 Block实现了数据块和磁盘文件目录的映射，而Compactor抽象了数据写入磁盘的逻辑，包括，写入计划，写操作，以及压缩合并。具体是通过 leavelCompactor 实现。\nheaderAppender 是数据写入内存的主要承担者，通过 stripeSeries 来构建全局序列缓存，通过类似分段Hash 的方式，降低读写锁竞争，提升效率。 单个序列的内存模型是memSeries, 单个序列的数据最终是要落盘的，对应了时间序列数据的真实data的存储， 通过chunk，以及mmapchunk的抽象，实现内存chunk和磁盘chunk的映射，并通过 mmap的机制，实现chunk 数据的懒加载。 对于chunk 的写入，抽象出了 chunk appender ,并通过 XOR chunkappend 的实现类，实现了 gorilla 的 delta of delta 的压缩算法，完成对数据的压缩存储。而且这部分也是低耦合的，只要我们轻易的增加一个 新的 chunkappend 实现累，就可以做到灵活的切换到其它的压缩算法上去。\n数据采样点，在写入内存的同时，会创建一个全局唯一的ID，作为正排索引，提高我们数据查询效率。序列写入的同时，会构建全局的标签键值对的符号表，每个标签对应那些Value，以及 倒排索引。核心的倒排索引数据结构是 *index.MemPostings, 核心的Map结构是一个大的二层Map，\nMap \u0026lt;LabelKey, Map \u0026lt;LabelValue,[]IDs\u0026gt;\u0026gt;  第一层Map的键是序列中的标签名称，第二层的Map 的键是 第一层标签对应的Value，而值则是有该标签键，值对 对应的所有的序列ID列表，而且每次更新时，都会保持ID是有序的。\nHead 的另一个核心对象是WAL，通过WAL解决了当数据尚未写入磁盘时，宕机引发的数据丢失问题。WAL在appedner Commit的时候被触发，实现了对Append的序列和采样数据的WAL.\n客户端视角-数据的写入流程  流程图  graph TD A[DBInit] --\u0026gt;|new dbAppender| B(Appender) B2(数据采集器) --\u0026gt;|DataSamples InComming| B B --\u0026gt; C{Has ID} C --\u0026gt;|No| D[Add] D --\u0026gt;|Return ID| C C --\u0026gt;|Yes| E[AddFast] E --\u0026gt; F{Commit} F--\u0026gt;|Yes|G[appender Commit] G --\u0026gt; |失败|G1(Rollback)  代码实现  2.1 DB 初始化\n storage, err := tsdb.Open(dir, l, nil, \u0026amp;tsdb.Options{ RetentionDuration: 15 * 24 * 60 * 60 * 1000, // 15 days in milliseconds BlockRanges: tsdb.ExponentialBlockRanges(2*60*60*1000, 5, 3), }) storage 即一个DB的实例  2.2 数据摄入完成，创建一个新的Appender\n实例化一个 Appender app := storage.Appender() // Appender 创建一个新的Appender 实例，这个实例和给定的db 是绑定的 func (db *DB) Appender() Appender { return dbAppender{db: db, Appender: db.head.Appender()} }  2.3 写入数据\n对于一个新的序列调用 Add, 对于已知的序列，调用AddFast\nif s.ref == nil { ref, err := app.Add(s.labels, ts, float64(s.value)) s.ref = \u0026amp;ref ... } else { app.AddFast(*s.ref, ts, float64(s.value) ... }  2.4 提交/回滚\napp.Commit() Or app.Rollback()  服务端视角-DB初始化 caller=db.go:571 msg=\u0026quot;DBInit 1,打开文件，初始化存储目录,log,Block Ragne等，修复坏的索引版本\u0026quot; caller=db.go:573 msg=\u0026quot;DBInit 2,初始化DB实例\u0026quot; caller=db.go:575 msg=\u0026quot;DBInit 3,监控指标初始化\u0026quot; caller=db.go:586 msg=\u0026quot;DBInit 4,DB 文件加锁\u0026quot; caller=db.go:599 msg=\u0026quot;DBInit 5,创建一个LeveledCompactor\u0026quot; caller=db.go:609 msg=\u0026quot;DBInit 6,Wal Log初始化\u0026quot; caller=db.go:626 msg=\u0026quot;DBInit 7,创建 Head\u0026quot; caller=db.go:633 msg=\u0026quot;DBInit 8,db reload\u0026quot; caller=db.go:640 msg=\u0026quot;DBInit 9,db blocks 初始化，获取最小的有效时间\u0026quot; caller=db.go:650 msg=\u0026quot;DBInit 10,db 核心内存结构Head 初始化\u0026quot; caller=db.go:660 msg=\u0026quot;DBInit 11,启动db主线程\u0026quot;  其中，步骤10 最为核心，10中完成了时间序列内存正排，倒排索引数据结构的初始化，让我们进入其中，\ncaller=head.go:646 msg=\u0026quot;HeadInit 1，重放磁盘的WAL和磁盘内存可映射chunk（如果有的话)\u0026quot; caller=head.go:659 msg=\u0026quot;HeadInit 1.1，获取WAL的最后一个checkpoint\u0026quot; caller=head.go:686 msg=\u0026quot;HeadInit 1.2，获取WAL的最后一个Segment\u0026quot; caller=head.go:693 msg=\u0026quot;HeadInit 1.3，从最近的checkpoint 回填segment\u0026quot; caller=head.go:702 msg=\u0026quot;HeadInit 1.4，加载WAL\u0026quot; caller=head.go:713 msg=\u0026quot;WAL replay completed\u0026quot; duration=744.825µs  关键内存对象分析 内存数据对象主要关注两个维度：\n 单个时间序列的数据如何存储和查找？ 多个数据序列数据如何缓存，倒排索引如何构建？  先看单个时间序列数据模型\n内存序列对象 memSeries classDiagram class memSeries { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; sync.Mutex ref uint64 //全局唯一ID lset labels.Labels //标签集合 chunks []*memChunk //chunk 的映射集合 headChunk *memChunk chunkRange int64 firstChunkID int nextAt int64 // 生成下一个chunk 的切割时间点 sampleBuf [4]sample app chunkenc.Appender appendable(t int64, v float64) error append(t int64, v float64) (success, chunkCreated bool) cut(mint int64) *memChunk chunk(id int) *memChunk truncateChunksBefore(mint int64) (removed int) }  内存序列的，数据模型如上图，核心的概念是chunk, 提供的append 和 cut 等操作，新的时间采样点数据来后，会append, 等到一定的时间戳口就被cut 或 truncate 。chunk之间通过位数固定的步长，chunkRange,通过chunkID 和chunkRange 可以 非常快速的定位到所需查询的chunk。\n内存序列对象 chunk classDiagram Appender \u0026lt;|-- xorAppender : 实现 \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Appender Appender : Append(t int64, v float64) Iterator \u0026lt;|-- xorIterator : 实现 \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Iterator Iterator : At() (int64, float64) Iterator : Err() error Iterator : Next() bool Chunk \u0026lt;|-- XORChunk : 实现 \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Chunk Chunk : Bytes() []byte Chunk : Encoding() Encoding Chunk : Appender() (Appender, error) Chunk : Iterator(Iterator) Iterator Chunk : NumSamples() int class XORChunk { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; b bstream Encoding() Appender() Iterator(it Iterator) } XORChunk --\u0026gt; xorIterator XORChunk --\u0026gt; xorAppender  chunk 是对单个序列，在段的时间维度上数据模型的抽象。提供的核心功能是Append 和 Iterator. 其中，XORChunk 实现了 gorila 的数据压缩算法。\n在看多个时间序列数据模型\n内存序列对象 Head Head 的类图结构 classDiagram class Head { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; ... wal *wal.WAL ... series *stripeSeries ... symbols map[string]struct values map[string]stringset // Label names to possible values. ... postings *index.MemPostings // Postings lists for terms. tombstones *tombstones.MemTombstones ... chunkDiskMapper *chunks.ChunkDiskMapper Appender() storage.Appender // 初始化hadderAppender 并返回 appender() *headAppender Chunks() (ChunkReader, error) // returns a ChunkReader against the block getOrCreate(hash uint64, lset labels.Labels) (*memSeries, bool, error) }  Head 的核心对象及其关系 classDiagram class Head { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; } class headAppender { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; head *Head sampleSeries []*memSeries series []record.RefSeries samples []record.RefSample } class WAL class stripeSeries class MemPostings class MemTombstones class ChunkDiskMapper Head --|\u0026gt; stripeSeries Head --|\u0026gt; MemPostings Head --|\u0026gt; MemTombstones Head --|\u0026gt; ChunkDiskMapper Head --* headAppender memSeries --* headAppender  各个主要对象的功能分析 headAppender 组合了 内存序列对象(memSeries) 和 Head 对象，并实现了 Appender 接口，是时间序列数据写入逻辑 的核心承载者。 Head 通过 stripeSeries,MemPosting,MemTomstones，以及ChunkDiskMapper 完成以下子功能：\n 全局有序的正排索引 - stripeSeries  classDiagram class stripeSeries { \u0026lt;\u0026lt;class\u0026gt;\u0026gt; size int //默认 2^14 个段 series []map[uint64]*memSeries // 全局内存序列hash 表 hashes []seriesHashmap // 序列标签hash 值为Key，[]memSeries为Value,为了防止hash冲突 locks []stripeLock // 分段锁 ... }  stripeSeries 顾名思义，将序列分成了很多条带，降低锁竞争。 初始化时，默认将序列划分为 2^14 个条带。\n对于新增一个数据点，如何将该数据写入到内存中的呢？\n根据序列标签集计算Hash -\u0026gt; 根据Hash值到全局Hash表中 (stripeSeries) 获取对应的序列 -\u0026gt; 将数据写入单个 序列内存对象 memSeries 中。\n根据序列标签集获取Hash值的计算逻辑如下：\n// lset.Hash(),基于采样点的所有标签构建Hash值 func (ls Labels) Hash() uint64 { b := make([]byte, 0, 1024) for _, v := range ls { b = append(b, v.Name...) b = append(b, sep) b = append(b, v.Value...) b = append(b, sep) } return xxhash.Sum64(b) }  根据hash值，获取内存序列\n s, created, err := a.head.getOrCreate(lset.Hash(), lset) // 从全局分段序列中获取内存序列 func (h *Head) getOrCreate(hash uint64, lset labels.Labels) (*memSeries, bool, error) { ... s := h.series.getByHash(hash, lset) ... } 从分段的HashMap中获取对应的内存序列 func (s *stripeSeries) getByHash(hash uint64, lset labels.Labels) *memSeries { i := hash \u0026amp; uint64(s.size-1) // 获取对应stripe位置 s.locks[i].RLock() series := s.hashes[i].get(hash, lset) s.locks[i].RUnlock() return series }  由于我们内存的序列范围可能非常大，为了提升效率，降低锁的竞争，将该序列分段，通过一次hash 可以定位到一个较小的区间，为了解决hash 冲突，维护了一个小的hashmap。\n 倒排索引 - MemPostings  倒排索引 MemPostings,是个大的二层Map,标签名称Key，为第一层Map的Key，Value为第二层Map的Key，第二层Map的Value则为包含次标签的序列 ID的一个数组。\n简化的数据结构描述: Map\u0026lt;LabelKey,Map\u0026lt;LabelValue,[]Ids\u0026gt;\n正是该倒排索引，解决了组合标签过滤查询时查询效率问题，查询性能从 O（M^N） - \u0026gt; O( M*N ) 。\n// MemPostings holds postings list for series ID per label pair. // They may be written to out of order. type MemPostings struct { mtx sync.RWMutex m map[string]map[string][]uint64 // 标签名称Key，为第一层Map的Key，Value为第二层Map的Key，第二层Map的Value则为包含次标签的序列 ID的一个数组。 ordered bool }  为了提升查询效率，数据在写入时，会时刻保持某个标签键值对所在的序列列表 IDs 是有序的。其实现逻辑也很简单，如下：\n ... // id is the given ID list :=append(list,id) ... for i := len(list) - 1; i \u0026gt;= 1; i-- { if list[i] \u0026gt;= list[i-1] { break } list[i], list[i-1] = list[i-1], list[i] }    预写日志组件 WAL\n新增的序列，首先会写入内存对象，之后，当 append.Commit 时，会触发WAL写入。后面会有单独章节来分析WAL的实现。 值得一提的是，WAL针对 head 中的 Series 和 Sample 写Log\n  磁盘到内存的 chunk 映射 - chunkDiskMapper\nchunkDiskMapper 基于 mmap 的技术，实现来磁盘chunk到内存chunk 的映射. Head 的\n loadMmappedChunks() (map[uint64][]*mmappedChunk, error)  方法，实现来数据的初始化。\n  总结 本文，从相对宏观的视角分析了 prometheus tsdb 的内存数据模型。\n1 . 整体的设计遵循高内聚，低耦合的方式原则。\n基于核心的三大领域模型，DB，Appender ,Query 实现。DB是对数据层封装，Appender 是对数据写入操作抽象， Query是对数据查询操作抽象。Appender + DB 构成了tsdb 的总体写入框架\n 内存模型从两个角度看，单个序列数据存储模型由 memSeries 承载，多个序列存储模型有Head ,Block ,TomStone等承载。\n  数据采样写入时候，构建了正排，索引。\n正向索引基于分段hash 的原理，减少锁竞争，通过二次 hashmap 解决hash冲突。 每个采样数据写入时，会判断是否为新增，若新增则会为期分配全局唯一的ID\n  数据采样写入时候，同时构建了全局的倒排索引，以及符号表\n  倒排索引是一个二层Map表, 结构为 \u0026lt;序列标签名称，\u0026lt;序列标签Value，此标签键值对所在的序列ID列表\u0026raquo;, 其中每个标签键值对对应的ID列表为有序的\n也就录了每个标签可能对应那些Value值。\n 通过 chunkDiskMapper 实现内存chunk 到磁盘chunk的映射\n  通过WAL 的技术解决在服务宕机时，部分尚未落入磁盘数据的恢复。\n  在后面的章节将分析，数据在磁盘的存储布局，以及内存数据到磁盘存的迁移过程。\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"36b4564c499aa1d42e79b127f30b44dd","permalink":"https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/memorylayout/memorylayout/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/courses/tsdb/chapter2/prometheustsdb/memorylayout/memorylayout/","section":"courses","summary":"目标 核心类图 设计思路分析 客户端视角-数据的写入流程 DB初始化","tags":null,"title":"内存模型","type":"docs"},{"authors":null,"categories":null,"content":"问题的背景 依托镜像定义运行的Container，Pod，还需要解决如下问题：\n 不可变基础设施（容器）的可变配置 敏感信息的存储和使用（如秘密，Token） 集群中Pod自我的身份认证 容器的运行安全管控 容器启动前置条件校验  stateDiagram 资源配置 --\u0026gt; 容器 安全管控 --\u0026gt; 容器 前置校验 --\u0026gt; 容器 容器 --\u0026gt; 可变配置 容器 --\u0026gt; 铭感信息 容器 --\u0026gt; 身份认证  POD的配置管理 配置文件详解 ConfigMap ConfigMap 介绍 管理容器运行所需要的： 配置文件 环境变量 命令行参数 用于解耦，容器镜像和可变配置 保障 工作负载的可移植性\napiVersion: v1 kind ConfigMap metadata: labels: app: flannel tier: node name: kube-flannel-cfg namespace: kube-system data: cni-conf.json: { \u0026quot;name\u0026quot;: \u0026quot;cbr0\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;flannel\u0026quot;, \u0026quot;delegate\u0026quot;: { \u0026quot;isDefaultGateway\u0026quot;: true } } net-conf.json: { \u0026quot;Network\u0026quot;:\u0026quot;172.27.0.0/16\u0026quot;, \u0026quot;Backend\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;vxlan\u0026quot; } }  ConfigMap 创建 创建命令： kubectl create configmap [NAME][DATA]\n kubectl create configmap kube-flannel-cfg --from-file=config-prod-container/configmap/cni-conf.json -n kube-system  ConfigMap 使用 ConfigMap 使用的注意事项 Secrete Secrete 介绍 Secrete 创建 Secrete 使用 Secrete 注意事项 ServiceAccount 应用场景 案例分析 Resource 容器资源管理 Pod(QoS)配置 Security Context InitContainer InitContainer介绍 InitContainer 和普通 container 的区别：\n InitContainer 首先会比普通 container 先启动，并且直到所有的 InitContainer 执行成功后，普通 container 才会被启动 InitContainer 之间是按定义的次序去启动执行的，执行成功一个之后再执行第二个，而普通的 container 是并发启动的 InitContainer 执行成功后就结束退出，而普通容器可能会一直在执行。它可能是一个 longtime 的，或者说失败了会重启，这个也是 InitContainer 和普通 container 不同的地方  InitContainer 用途 用于普通Container启动前的初始化(如配置文件准备)或普通Container启动的前置条件检验\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"f4634246aa950272a4dc970375170001","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/appconf/appconf/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/appconf/appconf/","section":"containertechnology","summary":"问题的背景 依托镜像定义运行的Container，Pod，还需","tags":null,"title":"应用配管","type":"docs"},{"authors":null,"categories":null,"content":"目标 通常，由于业务的复杂性，导致关键服务的核心指标也会非常繁多，我们基于这些指标会构建多个监控大盘。问题是: 这些指标构成的大盘虽然非常详细，但不够直观，并不能一眼就知道当前那些系统存在什么样的问题。为此我们需要一个高度抽象的服务健康红绿盘。\n通常，我们看到的仪表盘是这样的：\n健康红绿盘的需求分析 基本假设 为了更直观，我们假设所有的观察者：\n 不知到每个图表的具体含义 不知道当系统处变得不健康时，图形应该是什么样子 不知道内部组件以及他们如何组合在一起 从未读过对应服务的代码  我们看下如下的仪表盘：\n从这个图中，我们可以直观的值得\n log_statistics_minutely 服务挂了，它应该在过去的90分钟内停止了 pupuet 服务可能在storage-31 存在问题，问题可能不是很严重。 其它的服务应该是健康的。  健康红绿盘应该具有的基本信息  服务名称：代表那个服务 状态：健康，警告，或严重（OK,WARNING,CRITICAL） 简短信息提示: 一个简单的提示，表明是什么原因导致当前的状态 操作码：服务处于当前这个状态的唯一标识符，通过该操作码能明确对应到对应的运维操作文档。此处是一个超级链接，能跳转到对应的运维操作文档。  红绿盘的颜色需要简单明了，不能太多，否则容易让你迷惑\n参考文献  optimize-your-monitoring-for-decision-making\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"4a9ecab5fb45ecdf0247d1e3b4b4ca3f","permalink":"https://meixinyun.github.io/programmertalk/courses/monitor/productization/healthybox/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/courses/monitor/productization/healthybox/","section":"courses","summary":"目标 通常，由于业务的复杂性，导致关键服务的核心指标也会非常繁","tags":null,"title":"应用健康红绿大盘","type":"docs"},{"authors":null,"categories":null,"content":"内存分配器 ","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"1c3b59f475a4c34e156d483e0a67e1ff","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/chapter7/allocator/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/programmertalk/courses/golang/chapter7/allocator/","section":"courses","summary":"内存分配器","tags":null,"title":"内存分配器","type":"docs"},{"authors":null,"categories":null,"content":"目录   磁盘存储概览\n  Block 存储布局\n  Chunk 存储布局\n  Index 存储布局\n  WAL 存储布局\n  总结\n  磁盘存储结构概览  各个组件存储布局 总结 ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"acf2af86ff08a996a0aaf5699daf28af","permalink":"https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/disklayout/disylayout/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/courses/tsdb/chapter2/prometheustsdb/disklayout/disylayout/","section":"courses","summary":"目录 磁盘存储概览 Block 存储布局 Chunk 存储布局 Index 存储布局 WAL 存储布局 总结","tags":null,"title":"磁盘存储模型","type":"docs"},{"authors":null,"categories":null,"content":"多Prometheus 实例HA 为了HA的目标，通常会部署多个Prometheus来采集同样的数据，但不希望对同样的指标存储多个副本，为此可以通过如下方式来实现：\n假设有个两个团队，每个运行自己的prometheus 实例，监控不通的服务。假设 Prometheus 为 T1 和 T2， 在HA的场景下，上报的数据为 T1.a, T1.b 和 T2.a ，T2.b 如果上报的数据中， T1.a 为Leader节点，T1.b 的数据将会被丢掉。如果经过一个周期（假设为30s） 会将Leader 切换到T1.b\n这意味着，如果T1.a 挂掉后几分钟，HA 采样处理器会切换,并选择T1.b作为Leader. 这种故障转移超时使我们一次只能接受来自单个副本的样本，但要确保在出现问题时不会丢弃太多数据。\n默认情况下，假设采集周期为15s， 在大多情况下，当副本的leader切换时，我们仅仅只会丢失一个采集周期的数据。\n对于 rate 类型的指标查询，一般rate 窗口应为采集周期的4倍，以考虑这些指标的故障转移场景， 比较15s 的采样周期，至少应计算1分钟内的速率。\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"f7a4c5658d1a8d7f521f42d9f193a249","permalink":"https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/cortex/ha/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/courses/tsdb/chapter2/cortex/ha/","section":"courses","summary":"多Prometheus 实例HA 为了HA的目标，通常会部署多个","tags":null,"title":"Cortex HA","type":"docs"},{"authors":null,"categories":null,"content":"目标  k8s Volume 使用场景 PVC/PV/StorageClass 基本操作和概念呢解析 PVC/PV 设计与实现原理  存储基本概念介绍 术语表    术语 描述 简称     Volumes 存储卷 V   Pod Volumes Pod存储卷 PV   Persistent Volumes 持久化卷 PV   PersistentVolumeClaim 持久化存储卷申明 PVC   in-tree 网络存储实现的代码在k8s仓库中 in-tree   out-of-tree 网络存储实现, 通过从抽象接口将不通的存储\ndriver实现从k8s代码仓库中玻璃 out-of-tree   container storage interface K8s社区后面对存储插件实现(out of tree)的官方推荐方式 CSI    持久化存储的业务场景  如果 pod 中的某一个容器在运行时异常退出，被 kubelet 重新拉起之后，如何保证之前容器产生的重要数据没有丢失？ 如果同一个 pod 中的多个容器想要共享数据，应该如何去做？  Pod Volumes 的三种类型：  本地存储，常用的有 emptydir/hostpath； 网络存储：网络存储当前的实现方式有两种，一种是 in-tree，它的实现的代码是放在 K8s 代码仓库中的，随着k8s对存储类型支持的增多，这种方式会给k8s本身的维护和发展带来很大的负担；而第二种实现方式是 out-of-tree，它的实现其实是给 K8s 本身解耦的，通过抽象接口将不同存储的driver实现从k8s代码仓库中剥离，因此out-of-tree 是后面社区主推的一种实现网络存储插件的方式； Projected Volumes：它其实是将一些配置信息，如 secret/configmap 用卷的形式挂载在容器中，让容器中的程序可以通过POSIX接口来访问配置数据；  Pod Volumes 存在的问题  场景一：pod 重建销毁，如用 Deployment 管理的 pod，在做镜像升级的过程中，会产生新的 pod并且删除旧的 pod ，那新旧 pod 之间如何复用数据？ 场景二：宿主机宕机的时候，要把上面的 pod 迁移，这个时候 StatefulSet 管理的 pod，其实已经实现了带卷迁移的语义。这时通过 Pod Volumes 显然是做不到的； 场景三：多个 pod 之间，如果想要共享数据，应该如何去声明呢？我们知道，同一个 pod 中多个容器想共享数据，可以借助 Pod Volumes 来解决；当多个 pod 想共享数据时，Pod Volumes 就很难去表达这种语义； 场景四：如果要想对数据卷做一些功能扩展性，如：snapshot、resize 这些功能，又应该如何去做呢？  Pod Volumes 的解决方案- Persistent Volumes 将存储和计算分离，通过不同的组件来管理存储资源和计算资源，然后解耦 pod 和 Volume 之间生命周期的关联 。这样，当把 pod 删除之后，它使用的PV仍然存在，还可以被新建的 pod 复用。\nPersistent Volumes 的接口描述 PVC 用户在使用持久化存储时，真正关心的问题是：\n 使用的存储是可以被多个node共享还是只能单node独占访问(注意是node level而不是pod level)？ 只读还是读写访问？ 而不关心，与存储相关的实现细节。 这就使得我们需要抽象一层接口层，屏蔽掉存储实现细节的信息。这就是PVC的来历  为什么需要设计PVC?\n  职责分离。\n PVX中只用声明自己需要的存储大小，access mode (单node独占还是多node 共享，只读还是 读写访问？)等业务真正关系的存储需求，PV和其对应的后段存储信息则交给cluster admin 统一运维和管控，安全访问策略更容易控制。    PVC简化了User对存储的需求，PV才是存储实际信息的载体\n 通过kube-controller-manager中的PersistentVolumeController将PVC和合适的PV bound 到一起，从而满足实际的存储需求    PVC类似接口，PV类似接口对应的实现\n  PVC 和 PV的两种Bound 关系 正如前文分析，PV 和PVC 类似实现类和接口的关系，那么在实际中两者的关系就有静态绑定，和动态绑定两种 接下来，分析两种关系。\nStatic Volume Provisioning (静态Bound) 实现过程\n 由集群管理员（cluster admin）事先去规划这个集群中的用户会怎样使用存储，它会先预分配一些存储，也就是预先创建一些 PV； 然后用户在提交自己的存储需求（也就是 PVC）的时候，K8s 内部相关组件会帮助它把 PVC 和 PV 做绑定；3. 用户再通过 pod 去使用存储的时候，就可以通过 PVC 找到相应的 PV  静态绑定的方式， 需要先设置好模版，对资源池的划分比较粗糙。\n举例：如果用户需要的是 20G，然而集群管理员在分配的时候可能有 80G 、100G 的，但没有 20G的，这样就很 难满足用户的真实需求，也会造成资源浪费  Dynamic Provisioning(动态态Bound) 实现过程\n  集群管理员不预先创建PV，而提供模版文件（StorageClass)\n StorageClass 表示创建某一类型存储（块存储，文件存储等）所需的一些参数。用户在提交自身存储需求 (PVC)时，在PVC中指定使用的存储模版（StorageClass）    集群中的管控组件，会结合 PVC 和 StorageClass 的信息, 动态的生成 用户所需的PV，将PVC和PV绑定后，Pod 即可以使用PV了。\n通过 StorageClass 配置生成存储所需要的存储模板，再结合用户的需求动态创建 PV 对象，做到按需分 配，在没有增加用户使用难度的同时也解放了集群管理员的运维工作    PVC/PV/StorageClass 基本操作和概念解析 Pod Volumes 使用  通过 .spec.volumes 申明pod 的 volumes 信息 通过 .spec.containers.volumesMounts 申明 container 如何使用pod 的volumes 信息 通过 .spec.containers.volumesMounts.subPath 隔离不通容器在同一个 volumes上数据存储的路径，实现多个container 共享同一个volumes  Static Volume Provisioning 案例分析  Cluster Admin \u0026amp; User  管理员配置解析  用户配置解析  Dynamic Volume Provisioning 案例分析  系统管理员  1.1 系统管理员不再预分配 PV，而只是创建一个模板文件 \u0026ndash; StorageClass\n StorageClass 的信息： 第一：provisioner, 即那个存储插件。 存储插件provisioner对应创建PV的具体实现 第二：参数。 k8s创建存储的时候，需要指定的一些细节参数。如：regionld、zoneld、fsType 和它的类型， ReclaimPolicy：使用方使用结束、Pod 及 PVC 被删除后，这块 PV 应该怎么处理  用户如何用  2.1 PVC 新加一个字段-StorageClassName\n2.2 用户提交完PVC之后，K8s 集群中的相关组件就会根据PVC以及StorageClass动态生成 PV,并和当前PVC绑定\n2.3 之后用户在提交自己的 yaml 时，PVC找到PV，并把它挂载到相应的容器中\nPV Spec 重要字段解析 用户在提交PVC的时候,最重要的两个字段 —— Capacity 和 AccessModes。\nCapacity: 存储对象的大小； AccessModes : PV三种使用方式: 1. 单 node 读写访问； 2. 多个 node 只读访问，是常见的一种数据的共享方式； 3. 多个 node 上读写访问。\n在提交 PVC 后，k8s 集群中的相关组件是如何去找到合适的 PV 呢？\n 首先它是通过为 PV 建立的 AccessModes 索引找到所有能够满足用户的 PVC 里面的 AccessModes 要求的 PV list， 然后根据PVC的 Capacity，StorageClassName, Label Selector 进一步筛选 PV， 最小适合原则筛选 如果满足条件的 PV 有多个，选择 PV 的 size 最小的，accessmodes 列表最短的 PV  在PV 使用完毕后如何释放回收？\nReclaimPolicy:常见的有二种方式:\n delete: PVC 被删除之后，PV 也会被删除； Retain: 就是保留，保留之后，后面这个 PV 需要管理员来手动处理。  StorageClassName：动态 Provisioning 时必须指定的一个字段,来指定到底用哪一个模板文件来生成 PV；\nNodeAffinity：创建出来的PV，它能被哪些node去挂载使用， 其实是有限制的。然后通过 NodeAffinity 来声明对node的限制，这样其实对 使用该PV的pod调度也有限制。 pod 必须要调度到这些能访问 PV 的 node 上，才能使用这块 PV\nPV 状态图 stateDiagram CreatePV --\u0026gt; Pending Pending --\u0026gt; Available Available --\u0026gt; Bound Bound --\u0026gt; Released Released --\u0026gt; Deleted Released --\u0026gt; Failed  解释：\n 首先在创建PV对象后，它会处在短暂的pending 状态； 等真正的 PV 创建好之后，它就处在 available 状态 用户在提交 PVC 之后，被 K8s 相关组件做完 bound（即：找到相应的 PV），这个时候 PV 和 PVC 就结合到一起了，此时两者都处在 bound 状态 在使用完 PVC，将其删除后，这个 PV 就处在 released 状态 当 PV 已经处在 released 状态下，它是没有办法直接回到 available 状态 想把已经 released 的 PV 复用，有两种方式 4.1 新建一个 PV 对象，然后把之前的 released 的 PV 的相关字段的信息填到新的 PV 对象里面，这样的话，这个 PV 就可以结合新的 PVC 了 4.2 是我们在删除 pod 之后，不要去删除 PVC 对象，这样给 PV 绑定的 PVC 还是存在的，下次 pod 使用的时候，就可以直接通过 PVC 去复用  PVC/PV 设计与实现原理 架构设计 Pod/PV 创建的流程 CSI(container storage interface) 的实现可分为两大部分：\n 第一部分是由k8s社区驱动实现的通用的部分，如图中 csi-provisioner和 csi-attacher controller； 第二部分由云存储厂商实践的，对接云存储厂商的 OpenApi，主要是实现真正的 create/delete/mount/unmount 存储的相关操作，对应到上图中的csi-controller-server和csi-node-server。  用户提交 yaml 之后，k8s内部的处理流程：\n  用户在提交 PVCyaml 的时候，首先会在集群中生成一个 PVC 对象\n  PVC 对象会被 csi-provisioner controller watch到，csi-provisioner 会结合 PVC 对象以及 PVC 对象中声明的 storageClass，通过 GRPC 调用 csi-controller-server\n  csi-controller-server，然后，到云存储服务这边去创建真正的存储，并最终创建出来 PV 对象\n  由集群中的 PV controller 将 PVC 和 PV 对象做 bound ， PV 就可以被使用了\n  用户在提交 pod 之后,首先会被调度器调度选中某一个合适的node\n  该 node 上面的 kubelet 在创建 pod 流程中会通过首先 csi-node-server 将我们之前创建的 PV 挂载到我们 pod 可以使用的路径\n  kubelet 开始 create \u0026amp;\u0026amp; start pod 中的所有 container\n  PV、PVC 以及通过 csi 使用存储流程 有三个阶段：\n第一: create 阶段，主要是创建存储\n 用户提交完 PVC，由 csi-provisioner 创建存储，并生成 PV 对象，之后 PV controller 将 PVC 及生 成的 PV 对象做 bound  第二: attach 阶段，就是将那块存储挂载到 node 上面(通常为将存储load到node的/dev下面)；\n用户在提交 pod yaml 的时候，首先会被调度选中某一个 合适的node，等 pod 的运行 node 被选出来之后， 会被 AD Controller watch 到 pod 选中的 node，它会去查找 pod 中使用了哪些 PV。然后它会生成一 个内部的对象叫 VolumeAttachment 对象，从而去触发 csi-attacher去调用csi-controller-server 去做真正的 attache 操作，attach操作调到云存储厂商OpenAPI。这个 attach 操作就是将存储 attach到 pod 将会运行的 node 上面  第三: mount 阶段，将对应的存储进一步挂载到 pod 可以使用的路径\nkubelet 创建 pod的过程中，它在创建 pod 的过程中，首先要去做一个 mount，这里的 mount 操作是为了将 已经attach到这个 node 上面那块盘，进一步 mount 到 pod 可以使用的一个具体路径，之后 kubelet 才 开始创建并启动容器  总结  介绍了 K8s Volume 的使用场景，以及本身局限性； 通过介绍 K8s 的 PVC 和 PV 体系，说明 K8s 通过 PVC 和 PV 体系增强了 K8s Volumes 在多 Pod 共享/迁移/存储扩展等场景下的能力的必要性以及设计思想； 通过介绍 PV（存储）的不同供给模式 (static and dynamic)，学习了如何通过不同方式为集群中的 Pod 供给所需的存储； 通过 PVC\u0026amp;PV 在 K8s 中完整的处理流程，深入理解 PVC\u0026amp;PV 的工作原理。  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"d6ccfa53c41dcc0ef15f7cbf7e1455e7","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/pv/pv/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/pv/pv/","section":"containertechnology","summary":"目标 k8s Volume 使用场景 PVC/PV/StorageClass 基本操作和概念呢解析 PVC/PV 设计与实现原理 存储基","tags":null,"title":"应用持久化","type":"docs"},{"authors":null,"categories":null,"content":"目标  介绍一些整体需求的来源； 介绍在 K8s 中 Liveness 和 Readiness 的使用方式； 介绍在 K8s 中常见问题的诊断； 应用的远程调试的方式； 课程的总结与实践；  需求背景 来源 应用迁移到 Kubernetes 之后，要如何去保障应用的健康与稳定呢？从两个方面\n 提高应用的可观测性； 提供应用可恢复能力；  可观测性，可在三个方面增强：\n 首先是应用的健康状态上面，可以实时地进行观测 可以获取应用的资源使用情况； 可以拿到应用的实时日志，进行问题的诊断与分析  应用可观测性的手段（Liveness/Readiness）探针 二者比较 应用监控探测方式 应用监控探测文件描述 K8s 中常见问题的诊断 状态机制 常见应用异常 Pod 停留在 Pending 第一个就是 pending 状态，pending 表示调度器没有进行介入。此时可以通过 kubectl describe pod 来查看相应的事件，如果由于资源或者说端口占用，或者是由于 node selector 造成 pod 无法调度的时候，可以在相应的事件里面看到相应的结果，这个结果里面会表示说有多少个不满足的 node，有多少是因为 CPU 不满足，有多少是由于 node 不满足，有多少是由于 tag 打标造成的不满足。\nPod 停留在 waiting 那第二个状态就是 pod 可能会停留在 waiting 的状态，pod 的 states 处在 waiting 的时候，通常表示说这个 pod 的镜像没有正常拉取，原因可能是由于这个镜像是私有镜像，但是没有配置 Pod secret；那第二种是说可能由于这个镜像地址是不存在的，造成这个镜像拉取不下来；还有一个是说这个镜像可能是一个公网的镜像，造成镜像的拉取失败。\nPod 不断被拉取并且可以看到 crashing 第三种是 pod 不断被拉起，而且可以看到类似像 backoff。这个通常表示说 pod 已经被调度完成了，但是启动失败，那这个时候通常要关注的应该是这个应用自身的一个状态，并不是说配置是否正确、权限是否正确，此时需要查看的应该是 pod 的具体日志。\nService 无法正常的工作 最后一种就是 service 无法正常工作的时候，该怎么去判断呢？那比较常见的 service 出现问题的时候，是自己的使用上面出现了问题。因为 service 和底层的 pod 之间的关联关系是通过 selector 的方式来匹配的，也就是说 pod 上面配置了一些 label，然后 service 通过 match label 的方式和这个 pod 进行相互关联。如果这个 label 配置的有问题，可能会造成这个 service 无法找到后面的 endpoint，从而造成相应的 service 没有办法对外提供服务，那如果 service 出现异常的时候，第一个要看的是这个 service 后面是不是有一个真正的 endpoint，其次来看这个 endpoint 是否可以对外提供正常的服务\n常见应用异常总结 应用远程调试 Pod 远程调试   进入一个正在运行的Pod\nkubectl exec -it pod-name /bin/bash    进入一个正在运行包含多容器的Pod\nkubectl exec -it pod-name -c container-name /bin/bash    Servic 远程调试 实现方案，两种\n  第一: 将一个服务暴露到远程的一个集群之内，让远程集群内的一些应用来去调用本地的一个服务，这是一条反向的一个链路；\n  第二: 让这个本地服务能够去调远程的服务，那么这是一条正向的链路。\n  开源工具 Telepresence Telepresence 将本地的应用代理到集群中的一个Service 上\n Telepresence -swap-deployment $DEPLOYMENT_NAME  当本地开发的应用需要调用集群中的服务时: 可以使用Port-Forward将远程的应用代理到本地的端口\n kubectl port-forward svc/app -n app-namespace  开源的调试工具 - kubectl-debug 总结与实践 ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"7987e009ee29448aff643567527d7dec","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/appobserver/appobserver/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/appobserver/appobserver/","section":"containertechnology","summary":"目标 介绍一些整体需求的来源； 介绍在 K8s 中 Liveness 和 Readiness 的使用方式； 介绍","tags":null,"title":"应用可观测性","type":"docs"},{"authors":null,"categories":null,"content":"目录  概览 WAL管理 WAL Read/Writer Log file 格式 Record 文件格式 格式的优缺点 WAL的实现  概览 Write ahead log (WAL) 以日志文件的形式将内存表的操作顺序的持久化到存储介质上。在失败的时候，WAL文件 通过利用这些日志文件重建内存表，来恢复数据库到之前一致的状态。当一个内存表被安全的刷写到持久化介质后，对应的WLA日志会逐步的归档和淘汰，最终 归档的日志经过一个时间后会从磁盘清理掉。\nWAL管理 WAL文件是有在WAL目录下递增的序列号生成，为了能恢复到database的状态，这些文件需要按照序列号来读。WAL管理器， 提供了将WAL文件作为单个单元读取的抽象。在内部，它使用Reader或Writer抽象来打开和读取文件。\nWAL Reader/Writer Writer 为向日志记录提供了一个抽象。存储媒介特定的内部细节，通过WriteableFile 接口屏蔽掉。 类似地，Reader提供了 从特定日志文件中顺序读取日志记录的抽象。内部存储媒介的详细信息有 SequentailFile 接口处理。\nLog File 格式 日志文件有一系列可变长度的记录组成。记录按kBlockSize分组。如果某个记录无法放入剩余空间，则剩余空间将填充null数据。 Writer 以kBlockSize为单位进行读/写\n +-----+-------------+--+----+----------+------+-- ... ----+ File | r0 | r1 |P | r2 | r3 | r4 | | +-----+-------------+--+----+----------+------+-- ... ----+ \u0026lt;--- kBlockSize ------\u0026gt;|\u0026lt;-- kBlockSize ------\u0026gt;| rn = variable size records P = Padding  Record 文件格式 Record 的布局格式有两种： Legacy 和 Recyclable\nThe Legacy Record Format +---------+-----------+-----------+--- ... ---+ |CRC (4B) | Size (2B) | Type (1B) | Payload | +---------+-----------+-----------+--- ... ---+ CRC = 32bit hash computed over the payload using CRC Size = Length of the payload data Type = Type of record (kZeroType, kFullType, kFirstType, kLastType, kMiddleType ) The type is used to group a bunch of records together to represent blocks that are larger than kBlockSize Payload = Byte stream as long as specified by the payload size  Recyclable Record Format +---------+-----------+-----------+----------------+--- ... ---+ |CRC (4B) | Size (2B) | Type (1B) | Log number (4B)| Payload | +---------+-----------+-----------+----------------+--- ... ---+ Same as above, with the addition of Log number = 32bit log file number, so that we can distinguish between records written by the most recent log writer vs a previous one.  Record Format Details For Legacy Format 日志文件内容是一个32KB块的序列。唯一的例外是文件的尾部可能包含部分块。\n每一个block 包含一系列记录组成:\n block := record* trailer? record := checksum: uint32\t// crc32c of type and data[] length: uint16 type: uint8\t// One of FULL, FIRST, MIDDLE, LAST data: uint8[length]  记录永远不会在块的最后6个字节内开始（因为它不适合）。这里的任何剩余字节都构成了尾部，它必须完全由零字节组成，并且必须被读者跳过。\n 如果当前的block 还剩下 7 个 bytes, 当新增一个非零长度的record时，写入的 writer 必须先发出一个记录(其中包含零字节 的用户数据)以填充block 尾部的 7 个 bytes ,然后在后续的block 中，发出用户的所有数据\n 用户的记录的数据类型如下：\nFULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4  以后可以有更多的数据类型，一些 Readers 可能会跳过他们不理解的recored, 也有一些Readers 或报告 这些数据被忽略。\nFull record 或包含整个用户记录。\nFIRST, MIDDLE, LAST, 是用于用户记录的类型，这些类型被分割为多个片段（通常是因为block 的边界）。 FIRST: user record 的第一个片段的类型 LAST: user record 的最后一个片段的类型 MID: 是用户记录的所有内部片段的类型。\n举例:\n一序列用户的 records\nA: length 1000\nB: length 97270\nC: length 8000\nA 会作为完整记录存储在第一个block中 B 会拆分成三个block. 第一个fragment 占据第一个block 剩余的部分 第二个 fragment 会占据第二个block 的所有 第三个 fragment 会占据第三个block 的前半部分，这将在第三个block中留下6个bytes 的空闲空间，该block 作为 尾部留空 C 作为完整的记录存在第四个block 中\n优势   不需要任何启发式 resyncing ， 只需要转到下一个block 边界 ，扫描。如果又损坏，请跳到 下一个block。作为一个附带的好处，当一个日志文件的部分内容作为recode 潜入到另一个日志文件中时，我们 不会感到困惑。\n  Splitting at approximate boundaries (e.g., for mapreduce) is simple:\n  找到下一个block 并跳过记录，直到找到完整的或第一个记录为止。\n对于大的记录，我们不需要额外的缓冲。  缺点:\n 不能打包小的 records. 这个可以通过添加新的类型来解决，这是当前实现的一个缺点 不能压缩。同样，这个可以通过增加记录的类型来解决这个问题  WAL的实现 WLA 文件格式 WAL 按编号和顺序的段操作\n·000000· ·000001· ·000002· ·000003·\n默认最大128M，\n以32KB的页数写入一个段。只有最近一段的最后一页可能是不完整的。WAL记录是一个不透明的字节片，如果超过当前页面的剩余空间， ,它被拆分成子记录。在 segment 边界，记录永远不会分开，如果单个记录超过默认段大小，则将创建更大尺寸的segment.\nPrometheus tsdb 的WAL格式如下：\n┌───────────┬──────────┬────────────┬──────────────┐ │ type \u0026lt;1b\u0026gt; │ len \u0026lt;2b\u0026gt; │ CRC32 \u0026lt;4b\u0026gt; │ data \u0026lt;bytes\u0026gt; │ └───────────┴──────────┴────────────┴──────────────┘  类型标签又如下几种状态：\n 0: rest of page will be empty 1: a full record encoded in a single fragment 2: first fragment of a record 3: middle fragment of a record 4: final fragment of a record  记录的编码格式 分三种类型\n 序列记录 样本记录 Tombstone 类型  Series records encode the labels that identifies a series and its unique ID.\n序列记录 一个序列记录，会包含该序列的标签和唯一ID\n┌────────────────────────────────────────────┐ │ type = 1 \u0026lt;1b\u0026gt; │ ├────────────────────────────────────────────┤ │ ┌─────────┬──────────────────────────────┐ │ │ │ id \u0026lt;8b\u0026gt; │ n = len(labels) \u0026lt;uvarint\u0026gt; │ │ │ ├─────────┴────────────┬─────────────────┤ │ │ │ len(str_1) \u0026lt;uvarint\u0026gt; │ str_1 \u0026lt;bytes\u0026gt; │ │ │ ├──────────────────────┴─────────────────┤ │ │ │ ... │ │ │ ├───────────────────────┬────────────────┤ │ │ │ len(str_2n) \u0026lt;uvarint\u0026gt; │ str_2n \u0026lt;bytes\u0026gt; │ │ │ └───────────────────────┴────────────────┘ │ │ . . . │ └────────────────────────────────────────────┘  采集样本记录 采样数据样本记录 主要包含了 三元组 （序列ID，时间戳，序列值Value） 序列的索引ID和时间戳 被编码为 w.r.t 第一个行存储了启始的ID 和启始的时间戳。\n┌──────────────────────────────────────────────────────────────────┐ │ type = 2 \u0026lt;1b\u0026gt; │ ├──────────────────────────────────────────────────────────────────┤ │ ┌────────────────────┬───────────────────────────┐ │ │ │ id \u0026lt;8b\u0026gt; │ timestamp \u0026lt;8b\u0026gt; │ │ │ └────────────────────┴───────────────────────────┘ │ │ ┌────────────────────┬───────────────────────────┬─────────────┐ │ │ │ id_delta \u0026lt;uvarint\u0026gt; │ timestamp_delta \u0026lt;uvarint\u0026gt; │ value \u0026lt;8b\u0026gt; │ │ │ └────────────────────┴───────────────────────────┴─────────────┘ │ │ . . . │ └──────────────────────────────────────────────────────────────────┘  Tombstone记录 Tombstone records encode tombstones as a list of triples (series_id, min_time, max_time) and specify an interval for which samples of a series got deleted.\n┌─────────────────────────────────────────────────────┐ │ type = 3 \u0026lt;1b\u0026gt; │ ├─────────────────────────────────────────────────────┤ │ ┌─────────┬───────────────────┬───────────────────┐ │ │ │ id \u0026lt;8b\u0026gt; │ min_time \u0026lt;varint\u0026gt; │ max_time \u0026lt;varint\u0026gt; │ │ │ └─────────┴───────────────────┴───────────────────┘ │ │ . . . │ └─────────────────────────────────────────────────────┘  WLA 顶层接口设计 数据结构设计\n // WAL is a write ahead log that stores records in segment files. // It must be read from start to end once before logging new data. // If an error occurs during read, the repair procedure must be called // before it's safe to do further writes. // // Segments are written to in pages of 32KB, with records possibly split // across page boundaries. // Records are never split across segments to allow full segments to be // safely truncated. It also ensures that torn writes never corrupt records // beyond the most recent segment. type WAL struct { dir string logger log.Logger segmentSize int mtx sync.RWMutex segment *Segment // Active segment. donePages int // Pages written to the segment. page *page // Active page. stopc chan chan struct{} actorc chan func() closed bool // To allow calling Close() more than once without blocking. compress bool snappyBuf []byte metrics *walMetrics }  段 Segment 默认128M，\n页 Page 页是内存的buffer , 用来向磁盘批量刷数据。 如果 Records 比一个页大，此记录会被分割，并单独的刷写到磁盘。\n磁盘的刷写动作在如下情况被触发：\n 当一个记录不适合当前的page 大小 或剩下的空间放不下下一个记录   // page is an in memory buffer used to batch disk writes. // Records bigger than the page size are split and flushed separately. // A flush is triggered when a single records doesn't fit the page size or // when the next record can't fit in the remaining free page space. type page struct { alloc int flushed int buf [pageSize]byte }  WAL 初始化  根据目录，获取segment列表 创建最后一个segment  // New returns a new WAL over the given directory. func New(logger log.Logger, reg prometheus.Registerer, dir string, compress bool) (*WAL, error) { return NewSize(logger, reg, dir, DefaultSegmentSize, compress) }\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"eaa505ff90f6ec4def01bc04cd9222c2","permalink":"https://meixinyun.github.io/programmertalk/courses/tsdb/chapter2/prometheustsdb/wal/wal/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/courses/tsdb/chapter2/prometheustsdb/wal/wal/","section":"courses","summary":"目录 概览 WAL管理 WAL Read/Writer Log file 格式 Record 文件格式 格式的优缺点 WAL的","tags":null,"title":"WAL","type":"docs"},{"authors":null,"categories":null,"content":"目录  Kubernetes 的调度过程； Kubernetes 的基础调度能力（资源调度、关系调度）； Kubernetes 高级调度能力（优先级、抢占）。  Kubernetes 调度过程 调度过程概览 调度过程解析  用户通过 kube-ApiServer 提交描述Pod的yaml  ApiServer 会先把这个待创建的请求路由给我们的 webhooks 的 Controlles 进行校验\n ApiServer 会在集群里面生成一个 pod，但此时生成的 pod，它的 nodeName 是空的，并且它的 phase 是 Pending 状态\n kube-Scheduler 以及 kubelet 都能 watch 到这个 pod 的生成事件，kube-Scheduler 发现这个 pod 的 nodeName 是空的之后，会认为这个 pod 是处于未调度状态\n kube-Scheduler 把这个 pod 拿到自己里面进行调度，通过一系列的调度算法，包括一系列的过滤和打分的算法后，Schedule 会选出一台最合适的节点，并且把这一台节点的名称绑定在这个 pod 的 spec 上，完成一次调度的过程\n 更新完 nodeName 之后，在 Node1 上的这台 kubelet 会 watch 到这个 pod 是属于自己节点上的一个 pod。它会把这个 pod 拿到节点上进行操作，包括创建一些容器 storage 以及 network，最后等所有的资源都准备完成，kubelet 会把状态更新为 Running\n  调度的关键点 调度的核心：就是把 pod 放到合适的 node 上。 何为合适？\n1、首先要满足 pod 的资源要求;\n2、其次要满足 pod 的一些特殊关系的要求;\n3、再次要满足 node 的一些限制条件的要求;\n4、最后还要做到整个集群资源的合理利用。\nKubernetes 是怎么做到满足这些 pod 和 node 的要求的？\nKubernetes基础调度力 调度能力划分   资源调度\nKubernetes 基本的一些 Resources 的配置方式，还有 Qos 的概念，以及 Resource Quota 的概念和使用方式\n  关系调度\n在关系调度上，介绍两种关系场景:\n2.1 pod 和 pod 之间的关系场景，包括怎么去亲和一个 pod，怎么去互斥一个 pod? 2.2 pod 和 node 之间的关系场景，包括怎么去亲和一个 node，以及有一些 node 怎么去限制 pod 调度上来。\n  如何满足Pod资源要求 资源配置方法 Pod资源类型  cpu memory ephemeral-storage extended-resource: nvidia.com/gpu  resources  request: 对这个 pod 基本保底的一些资源要求 limit: 代表的是对这个 pod 可用能力上限的一种限制  由 request/limit 来引出 Qos概念\n资源QoS类型 Qos: Quality of Service 其实是 Kubernetes 用来表达一个 pod 在资源能力上的服务质量的标准,Kubernetes 提供了三类的 Qos Class:\n 第一类是 Guaranteed:它是一类高的 Qos Class，一般用 Guaranteed 来为一些需要资源保障能力的 pod 进行配置 第二类是 Burstable，它其实是中等的一个 Qos label，一般会为一些希望有弹性能力的 pod 来配置 Burstable； 第三类是 BestEffort，通过名字我们也知道，它是一种尽力而为式的服务质量。  问题：K8s中用户没法指定自己的 pod 是属于哪一类 Qos，而是通过 request 和 limit 的组合来自动地映射上 Qos Class\nPod QoS配置 如何通过 request 和 limit 的组合来确定我们想要的 QoS Level\n Guaranteed: CPU/Memory 必须 request==limit，其他的资源可以不相等 Burstable: CPU/Memory request 和 limit 不相等 BestEffort: 所有资源request/limit 必须都不填  资源QoS用法 资源和QoS 关系  调度器只会使用 request 进行调度 不同的 QoS 表现不相同, 开启 kubelet cpu-manager-policy=static 特性时， 如果它的 request 是一个整数，它会对 Guaranteed Pod 进行绑核 非整数的 Guaranteed/Burstable/BestEffort，它们的 CPU 会放在一块，组成一个 CPU share pool，然后它们会根据不同的权重划分时间片来使用  另外，memory 上也会按照不同的 Qos 进行划分:OOMScore\n Guaranteed，它会配置默认的 -998 的 OOMScore Burstable,会根据内存设计的大小和节点的关系来分配 2-999 的 OOMScore BestEffort 会固定分配 1000 的 OOMScore  OOMScore 得分越高的话，在物理机出现 OOM 的时候会优先被 kill 掉\neviction:\n 会优先考虑驱逐 BestEffort 的 pod  资源 Quota 假如集群是由多个人同时提交的，或者是多个业务同时在使用，我们肯定要限制某个业务或某个人提交的总量，防止整个集群的资源都会被使用掉，导致另一个业务没有资源使用\n解析 ResourceQuota: 限制 namespace 资源用量 上图右侧的 yaml 所示,spec 包括了一个 hard 和 scopeSelector\nhard: 和 Resourcelist 很像,比 ResourceList 更丰富一点,可以填写一些 Pod，这样可以限制 Pod 数量能力\nscopeSelector:为这个 Resource 方法定义更丰富的索引能力, 比如上面的例子中，索引出非 BestEffort 的 pod，限制的 cpu 是 1000 个，memory 是 200G，Pod 是 10 个，然后 Scope 除了提供 NotBestEffort，它还提供了更丰富的索引范围，包括 Terminating/Not Terminating，BestEffort/NotBestEffort，PriorityClass。\n 用 ResourceQuota 方法来做到限制每一个 namespace 的资源用量，从而保证其他用户的资源使用  小结 基础资源的使用方式，做到了如何满足 Pod 资源要求。下面做一个小结：\n Pod 要配置合理的资源要求 CPU/Memory/EphemeralStorage/GPU  通过 Request 和 Limit 来为不同业务特点的 Pod 选择不同的 QoS\n Guaranteed：敏感型，需要业务保障 Burstable：次敏感型，需要弹性业务 BestEffort：可容忍性业务  为每个 NS 配置 ResourceQuota 来防止过量使用，保障其他人的资源可用\n如何满足Pod/Node 特殊关系/条件要求 如何满足Pod 与Pod 关系要求  podAffinity/podAntiAffinity required /preferred  如何满足Pod 与Node 关系要求 主要又两类:\n NodeSelector: 举例：必须要调度 Pod 到带了 k1: v1 标签的 Node 上，这时可以在 Pod 的 spec 中填写一个 nodeSelector 要求，比如 k1: v1。这样我的 Pod 就会强制调度到带了 k1: v1 标签的 Node 上 NodeAffinity:  小结 首先假如有需求是处理 Pod 与 Pod 的时候，比如 Pod 和另一个 Pod 有亲和的关系或者是互斥的关系，可以给它们配置下面的参数：\nPodAffinity\nPodAntiAffinity\n假如存在 Pod 和 Node 有亲和关系，可以配置下面的参数：\nNodeSelector\nNodeAffinity\n假如有些 Node 是限制某些 Pod 调度的，比如说一些故障的 Node，或者说是一些特殊业务的 Node，可以配置下面的参数：\nNode \u0026ndash; Taints\nPod \u0026ndash; Tolerations\nKubernetes 高级调度能力 优先级调度目的 在资源不够情况下，我们怎么做到集群的合理利用呢？  先到先得策略 (FIFO) -简单、相对公平，上手快 优先级策略 (Priority) - 符合日常公司业务特点  因为公司业务里面肯定是有高优先级的业务和低优先级的业务，所以优先级策略会比先到先得策略更能够符合日常公司业务特点\nPriority 和 Preemption  v 1.14 -stable Priority \u0026amp; Preemption default is On  优先级调度设置 设置解析  需要创建一个 priorityClass\n1.1 是创建名为 high 的 priorityClass，它是高优先级，得分为 10000；\n1.2 然后还创建了一个 low 的 priorityClass，它的得分是 100。\n 为每个 Pod 配置上不同的 priorityClassName\n2.1 给 Pod1 配置上了 high，Pod2 上配置了 low priorityClassName  内置优先级配置  Kubernetes 内置了默认的优先级,DefaultpriorityWhenNoDefaultClassExistis,默认所有Pod此项均为0 用户可配置最大优先级限制：HighestUserDefinablePriority = 10000000000(10 亿) 系统级别优先级：SystemCriticalPriority = 20000000000(20 亿)  优先级调度过程 只触发优先级调度但是没有触发抢占调度的流程 优先级抢占过程 优先级抢占策略   一个 Pod 进入抢占的时候，会判断 Pod 是否拥有抢占的资格，有可能上次已经抢占过一次，如果符合抢占资格，它会先对所有的节点进行一次过滤，过滤出符合这次抢占要求的节点，如果不符合就过滤掉这批节点\n  从过滤剩下的节点中，挑选出合适的节点进行抢占。这次抢占的过程会模拟一次调度，也就是把上面优先级低的 Pod 先移除出去，再把待抢占的 Pod 尝试能否放置到此节点上。然后通过这个过程选出一批节点，进入下一个过程叫 ProcessPreemptionWithExtenders。这是一个扩展的钩子，用户可以在这里加一些自己抢占节点的策略，如果没有扩展的钩子，这里面是不做任何动作的\n  接下来的流程叫做 PickOneNodeForPreemption，就是从上面 selectNodeForPreemption list 里面挑选出最合适的一个节点，这是有一定的策略的\n3.1 优先选择打破 PDB 最少的节点；\n3.2 其次选择待抢占 Pods 中最大优先级最小的节点；\n3.3 再次选择待抢占 Pods 优先级加和最小的节点；\n3.4 接下来选择待抢占 Pods 数目最小的节点；\n3.5 最后选择拥有最晚启动 Pod 的节点\n  小结 调度的高级策略，在集群资源紧张的时候也能合理调度资源\n 创建自定义的一些优先级类别 (PriorityClass)； 给不同类型 Pods 配置不同的优先级 (PriorityClassName)； 通过组合不同类型 Pods 运行和优先级抢占让集群资源和调度弹性起来。  参考阅读  ALi云原生课堂\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"f5ff16e9161ca151a7fdce3dce7db606","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/schedule/schedule/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/schedule/schedule/","section":"containertechnology","summary":"目录 Kubernetes 的调度过程； Kubernetes 的基础调度能力（资源调度、关系调度）； Kubernetes","tags":null,"title":"Kubernetes调度和资源管理","type":"docs"},{"authors":null,"categories":null,"content":"目标  Scheduler 架构 Scheduler 算法实现  调度流程 Predicates Priorities   如何配置调度器 Scheduler Extender Scheduler Framework  Scheduler 架构 架构图 架构图解析   调度器启动时会通过配置文件 File，或者是命令行参数，或者是配置好的 ConfigMap，来指定调度策略。指定要用哪些过滤器 (Predicates)、打分器 (Priorities) 以及要外挂哪些外部扩展的调度器 (Extenders)，和要使用的哪些 Schedule 的扩展点 (Plugins)\n  启动的时候会通过 kube-apiserver 去 watch 相关的数据，通过 Informer 机制将调度需要的数据 ：Pod 数据、Node 数据、存储相关的数据，以及在抢占流程中需要的 PDB 数据，和打散算法需要的 Controller-Workload 数据。\n  通过 Informer 去 watch 到需要等待的 Pod 数据，放到队列里面，通过调度算法流程里面，会一直循环从队列里面拿数据，然后经过调度流水线\n  调度流水线 (Schedule Pipeline) 主要有三个组成部分： 4.1 调度器的调度流程 4.2 Wait 流程 4.3 Bind 流程\n  从调度队列里面拿到一个 Pod 进入到 Schedule Theread 流程中，通过 Pre Filter\u0026ndash;Filter\u0026ndash;Post Filter\u0026ndash;Score(打分)-Reserve，最后 Reserve 对账本做预占用\n  基本调度流程结束后，会把这个任务提交给 Wait Thread 以及 Bind Thread，然后 Schedule Theread 继续执行流程，会从调度队列中拿到下一个 Pod 进行调度\n  调度完成后，会去更新调度缓存 (Schedule Cache)，如更新 Pod 数据的缓存，也会更新 Node 数据。以上就是大概的调度流程\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"63abccc859686872c96db136dae9066b","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/schedulealgorithem/schedulealgo/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/schedulealgorithem/schedulealgo/","section":"containertechnology","summary":"目标 Scheduler 架构 Scheduler 算法实现 调度流程 Predicates Priorities 如何配置调度器 Scheduler Extender Scheduler Framework Scheduler 架构","tags":null,"title":"Kubernetes调度和资源管理","type":"docs"},{"authors":null,"categories":null,"content":"目标  Pod 网络联通性 负载均衡 服务发现  Pod 网络联通性 ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"86fab641a3a0d6fe4ca36fd6516ade08","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network01/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/network/network01/","section":"containertechnology","summary":"目标 Pod 网络联通性 负载均衡 服务发现 Pod 网络联通性","tags":null,"title":"网络模型基础","type":"docs"},{"authors":null,"categories":null,"content":"目标  kubernetes 网络的演化历史 Pod 如何上网 Service 如何工作 负载均衡 思考问题 总结  kubernetes 网络的演化历史 早期Docker网络的由来及弊端 容器网络发端于 Docker 的网络。Docker 使用了一个比较简单的网络模型，即内部的网桥加内部的保留 IP. 这种设计的好处在于: 容器的网络和外部世界是解耦的，无需占用宿主机的 IP 或者宿主机的资源，完全是虚拟的。 它的设计初衷是: 当需要访问外部世界时，会采用 SNAT 这种方法来借用 Node 的 IP 去访问外面的服务。 比如容器需要对外提供服务的时候，所用的是 DNAT 技术，也就是在 Node 上开一个端口，然后通过 iptable 或者别的某些机制，把流导入到容器的进程上以达到目的。\n该模型的问题在于，外部网络无法区分哪些是容器的网络与流量、哪些是宿主机的网络与流量。比如，如果要做一个高可用的时候，172.16.1.1 和 172.16.1.2 是拥有同样功能的两个容器，此时我们需要将两者绑成一个 Group 对外提供服务，而这个时候我们发现从外部看来两者没有相同之处，它们的 IP 都是借用宿主机的端口，因此很难将两者归拢到一起。\nkubernetes 的解决方案 Kubernetes 提出了这样一种机制:即每一个 Pod，也就是一个功能聚集小团伙应有自己的“身份证”，或者说 ID。在 TCP 协议栈上，这个 ID 就是 IP。 这个 IP 是真正属于该 Pod 的，外部世界不管通过什么方法一定要给它。对这个 Pod IP 的访问就是真正对它的服务的访问，中间拒绝任何的变造。比如以 10.1.1.1 的 IP 去访问 10.1.2.1 的 Pod，结果到了 10.1.2.1 上发现，它实际上借用的是宿主机的 IP，而不是源 IP，这样是不被允许的。Pod 内部会要求共享这个 IP，从而解决了一些功能内聚的容器如何变成一个部署的原子的问题。\nKubernetes 对怎么实现这个模型其实是没有什么限制的，用 underlay 网络来控制外部路由器进行导流是可以的；如果希望解耦，用 overlay 网络在底层网络之上再加一层叠加网，这样也是可以的。总之，只要达到模型所要求的目的即可。\nPod 如何上网 容器网络的网络包究竟是怎么传送的？可以从以下两个维度来看：\n 协议层次 网络拓扑  协议层次 它和 TCP 协议栈的概念是相同的，需要从两层、三层、四层一层层地摞上去，发包的时候从右往左，即先有应用数据，然后发到了 TCP 或者 UDP 的四层协议，继续向下传送，加上 IP 头，再加上 MAC 头就可以送出去了。收包的时候则按照相反的顺序，首先剥离 MAC 的头，再剥离 IP 的头，最后通过协议号在端口找到需要接收的进程。\n网络拓扑 一个容器的包所要解决的问题分为两步：第一步，如何从容器的空间 (c1) 跳到宿主机的空间 (infra)；第二步，如何从宿主机空间到达远端。\n容器网络的方案可以通过接入、流控、通道这三个层面来考虑\n 第一个是接入，就是说我们的容器和宿主机之间是使用哪一种机制做连接，比如 Veth + bridge、Veth + pair 这样的经典方式，也有利用高版本内核的新机制等其他方式（如 mac/IPvlan 等）,来把包送入到宿主机空间； 第二个是流控，就是说我的这个方案要不要支持 Network Policy，如果支持的话又要用何种方式去实现。这里需要注意的是，我们的实现方式一定需要在数据路径必经的一个关节点上。如果数据路径不通过该 Hook 点，那就不会起作用； 第三个是通道，即两个主机之间通过什么方式完成包的传输。我们有很多种方式，比如以路由的方式，具体又可分为 BGP 路由或者直接路由。还有各种各样的隧道技术等等。最终我们实现的目的就是一个容器内的包通过容器，经过接入层传到宿主机，再穿越宿主机的流控模块（如果有）到达通道送到对端。  路由方案分析 一个最简单的路由方案：Flannel-host-gw\n这个方案采用的是每个 Node 独占网段，每个 Subnet 会绑定在一个 Node 上，网关也设置在本地，或者说直接设在 cni0 这个网桥的内部端口上。 该方案的好处是管理简单，坏处就是无法跨 Node 迁移 Pod-这个 IP、网段已经是属于这个 Node 之后就无法迁移到别的 Node 上\n这个方案的精髓在于 route 表的设置，如上图所示。接下来为大家一一解读一下。\n  第一条很简单，我们在设置网卡的时候都会加上这一行。就是指定我的默认路由是通过哪个 IP 走掉，默认设备又是什么\n  第二条是对 Subnet 的一个规则反馈。就是说我的这个网段是 10.244.0.0，掩码是 24 位，它的网关地址就在网桥上，也就是 10.244.0.1。这就是说这个网段的每一个包都发到这个网桥的 IP 上；\n  第三条是对对端的一个反馈。如果你的网段是 10.244.1.0（上图右边的 Subnet），我们就把它的 Host 的网卡上的 IP (10.168.0.3) 作为网关。也就是说，如果数据包是往 10.244.1.0 这个网段发的，就请以 10.168.0.3 作为网关。后面的跟本次讲解没有什么关系。\n  再来看一下这个数据包到底是如何跑起来的？\n假设容器 (10.244.0.2) 想要发一个包给 10.244.1.3，那么它在本地产生了 TCP 或者 UDP 包之后，再依次填好对端 IP 地址、本地以太网的 MAC 地址作为源 MAC 以及对端 MAC。一般来说本地会设定一条默认路由，默认路由会把 cni0 上的 IP 作为它的默认网关，对端的 MAC 就是这个网关的 MAC 地址。然后这个包就可以发到桥上去了。如果网段在本桥上，那么通过 MAC 层的交换即可解决。\n这个例子中我们的 IP 并不属于本网段，因此网桥会将其上送到主机的协议栈去处理。主机协议栈恰好找到了对端的 MAC 地址。使用 10.168.0.3 作为它的网关，通过本地 ARP 探查后，我们得到了 10.168.0.3 的 MAC 地址。即通过协议栈层层组装，我们达到了目的，将 Dst-MAC 填为右图主机网卡的 MAC 地址，从而将包从主机的 eth0 发到对端的 eth0 上去。\n所以大家可以发现，这里有一个隐含的限制，上图中的 MAC 地址填好之后一定是能到达对端的，但如果这两个宿主机之间不是二层连接的，中间经过了一些网关、一些复杂的路由，那么这个 MAC 就不能直达，这种方案就是不能用的。当包到达了对端的 MAC 地址之后，发现这个包确实是给它的，但是 IP 又不是它自己的，就开始 Forward 流程，包上送到协议栈，之后再走一遍路由，刚好会发现 10.244.1.0/24 需要发到 10.244.1.1 这个网关上，从而到达了 cni0 网桥，它会找到 10.244.1.3 对应的 MAC 地址，再通过桥接机制，这个包就到达了对端容器。\n大家可以看到，整个过程总是二层、三层，发的时候又变成二层，再做路由，就是一个大环套小环。这是一个比较简单的方案，如果中间要走隧道，则可能会有一条 vxlan tunnel 的设备，此时就不填直接的路由，而填成对端的隧道号。\nService 如何工作 Service 其实是一种负载均衡 (Load Balance) 的机制。\n我们认为它是一种用户侧(Client Side) 的负载均衡，也就是说 VIP 到 RIP 的转换在用户侧就已经完成了，并不需要集中式地到达某一个 NGINX 或者是一个 ELB 这样的组件来进行决策。\n它的实现是这样的:\n 首先是由一群 Pod 组成一组功能后端 在前端上定义一个虚 IP 作为访问入口,会附赠一个 DNS 的域名，Client 先访问域名得到虚 IP 之后再转成实 IP Kube-proxy 则是整个机制的实现核心，它隐藏了大量的复杂性。它的工作机制是通过 apiserver 监控 Pod/Service 的变化(比如是不是新增了 Service、Pod）并将其反馈到本地的规则或者是用户态进程。并将其反馈到本地的规则或者是用户态进程  三步实现一个LVS 的Service 我们来实际做一个 LVS 版的 Service。LVS 是一个专门用于负载均衡的内核机制。它工作在第四层，性能会比用 iptable 实现好一些。\n假设我们是一个 Kube-proxy，拿到了一个 Service 的配置，如下图所示：它有一个 Cluster IP，在该 IP 上的端口是 9376，需要反馈到容器上的是 80 端口，还有三个可工作的 Pod，它们的 IP 分别是 10.1.2.3, 10.1.14.5, 10.1.3.8。\n它要做的事情就是：\n过程详解：\n 第 1 步，绑定 VIP 到本地（欺骗内核）  首先需要让内核相信它拥有这样的一个虚 IP，这是 LVS 的工作机制所决定的，因为它工作在第四层，并不关心 IP 转发，只有它认为这个 IP 是自己的才会拆到 TCP 或 UDP 这一层。在第一步中，我们将该 IP 设到内核中，告诉内核它确实有这么一个 IP。实现的方法有很多，我们这里用的是 ip route 直接加 local 的方式，用 Dummy 哑设备上加 IP 的方式也是可以的。\n 第 2 步，为这个虚 IP 创建一个 IPVS 的 virtual server；  告诉它我需要为这个 IP 进行负载均衡分发，后面的参数就是一些分发策略等等。virtual server 的 IP 其实就是我们的 Cluster IP。\n 第 3 步，为这个 IPVS service 创建相应的 real server。  我们需要为 virtual server 配置相应的 real server，就是真正提供服务的后端是什么。比如说我们刚才看到有三个 Pod，于是就把这三个的 IP 配到 virtual server 上，完全一一对应过来就可以了。Kube-proxy 工作跟这个也是类似的。只是它还需要去监控一些 Pod 的变化，比如 Pod 的数量变成 5 个了，那么规则就应变成 5 条。如果这里面某一个 Pod 死掉了或者被杀死了，那么就要相应地减掉一条。又或者整个 Service 被撤销了，那么这些规则就要全部删掉。所以它其实做的是一些管理层面的工作。\n三步实现LVS 服务全景图\n负载均衡还分内部外部 Service 的类型，可以分为以下 4 类：\n ClusterIP 集群内部的一个虚拟 IP，这个 IP 会绑定到一堆服务的 Group Pod 上面，这也是默认的服务方式。它的缺点是这种方式只能在 Node 内部也就是集群内部使用。 NodePort 供集群外部调用。将 Service 承载在 Node 的静态端口上，端口号和 Service 一一对应，那么集群外的用户就可以通过 : 的方式调用到 Service。 LoadBalancer 给云厂商的扩展接口。像阿里云、亚马逊这样的云厂商都是有成熟的 LB 机制的，这些机制可能是由一个很大的集群实现的，为了不浪费这种能力，云厂商可通过这个接口进行扩展。它首先会自动创建 NodePort 和 ClusterIP 这两种机制，云厂商可以选择直接将 LB 挂到这两种机制上，或者两种都不用，直接把 Pod 的 RIP 挂到云厂商的 ELB 的后端也是可以的。 ExternalName 摈弃内部机制，依赖外部设施，比如某个用户特别强，他觉得我们提供的都没什么用，就是要自己实现，此时一个 Service 会和一个域名一一对应起来，整个负载均衡的工作都是外部实现的。  下图是一个实例。它灵活地应用了 ClusterIP、NodePort 等多种服务方式，又结合了云厂商的 ELB，变成了一个很灵活、极度伸缩、生产上真正可用的一套系统。\n  用 ClusterIP 来做功能 Pod 的服务入口 大家可以看到，如果有三种 Pod 的话，就有三个 Service Cluster IP 作为它们的服务入口。这些方式都是 Client 端的，如何在 Server 端做一些控制呢？\n  起一些 Ingress 的 Pod（Ingress 是 K8s 后来新增的一种服务，本质上还是一堆同质的 Pod），然后将这些 Pod 组织起来，暴露到一个 NodePort 的 IP，K8s 的工作到此就结束了\n  任何一个用户访问 23456 端口的 Pod 就会访问到 Ingress 的服务，它的后面有一个 Controller，会把 Service IP 和 Ingress 的后端进行管理，最后会调到 ClusterIP，再调到我们的功能 Pod。\n  ELB 去监听所有集群节点上的 23456 端口，只要在 23456 端口上有服务的，就认为有一个 Ingress 的实例在跑。\n  整个的流量经过外部域名的一个解析跟分流到达了云厂商的 ELB，ELB 经过负载均衡并通过 NodePort 的方式到达 Ingress，Ingress 再通过 ClusterIP 调用到后台真正的 Pod。\n思考问题  容器层的网络究竟如何与宿主机网络共存？选择 underlay 网络还是 overlay 网络？是都变成 overlay 还是根据场景去区分？大家在工作中一般是怎么思考的？ Service 还可以有怎样的实现？ 为什么一个容器编排系统要大力搞服务发现和负载均衡  总结  要从根本上理解 Kubernetes 网络模型的演化来历，理解 PerPodPerIP 的用心在哪里； 网络的事情万变不离其宗，按照模型从 4 层向下就是发包过程，反正层层剥离就是收包过程，容器网络也是如此； Ingress 等机制是在更高的层次上（服务\u0026lt;-\u0026gt;端口）方便大家部署集群对外服务，通过一个真正可用的部署实例，希望大家把 Ingress+Cluster IP + PodIP 等概念联合来看，理解社区出台新机制、新资源对象的思考。  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"89c8d6628e797533ea2b53335fa985b8","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network02/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/network/network02/","section":"containertechnology","summary":"目标 kubernetes 网络的演化历史 Pod 如何上网 Service 如何工作 负载均衡 思考问题 总结","tags":null,"title":"网络模型进阶","type":"docs"},{"authors":null,"categories":null,"content":"目标  CNI 定义 CNI如何选择 开发自己的CNI插件  什么是CNI CNI: Container Network Interface，即容器网络的 API 接口\nCNI Plugin : 一系列实现 CNI API 接口的网络插件 CNI 插件主要解决的问题：\n 配置Pod网络空间 打通Pod之间网络连同  Kubelet 通过这个标准的 API 来调用不同的网络插件以实现不同的网络配置方式。\n常见的CNI接口包括： Calico Flannel Terway Weave Net Contiv\nKubernetes 中如何使用 kubernetes CNI 插件启用流程  配置CNI配置文件（/etc/cni/net.d/XXXnet.conf） 安装CNI 二进制插件(/opt/cni/bin/xxnet) 在这个节点上创建Pod Kubelet 会根据CNI 配置文件执行CNI插件 Pod网络配置初始化  如图所示：\nCNI如何选择 CNI 的实现类型，可分为三类：\n Overlay, 隧道 路由 Underlay, 底层网络  如图：\n如何选择自己的插件呢？\n环境限制 不同环境中所支持的底层能力是不同的。\n 虚拟化环境（例如 OpenStack）中的网络限制较多，比如不允许机器之间直接通过二层协议访问，必须要带有 IP 地址这种三层的才能去做转发，限制某一个机器只能使用某些 IP 等。在这种被做了强限制的底层网络中，只能去选择 Overlay 的插件，常见的有 Flannel-vxlan, Calico-ipip, Weave 等等； 物理机环境中底层网络的限制较少，比如说我们在同一个交换机下面直接做一个二层的通信。对于这种集群环境，我们可以选择 Underlay 或者路由模式的插件。Underlay 意味着我们可以直接在一个物理机上插多个网卡或者是在一些网卡上做硬件虚拟化；路由模式就是依赖于 Linux 的路由协议做一个打通。这样就避免了像 vxlan 的封包方式导致的性能降低。这种环境下我们可选的插件包括 clico-bgp, flannel-hostgw, sriov 等等； 公有云环境也是虚拟化，因此底层限制也会较多。但每个公有云都会考虑适配容器，提升容器的性能，因此每家公有云可能都提供了一些 API 去配置一些额外的网卡或者路由这种能力。在公有云上，我们要尽量选择公有云厂商提供的 CNI 插件以达到兼容性和性能上的最优。比如 Aliyun 就提供了一个高性能的 Terway 插件。  功能需求   安全需求； K8s 支持 NetworkPolicy，就是说我们可以通过 NetworkPolicy 的一些规则去支持“Pod 之间是否可以访问”这类策略。但不是每个 CNI 插件都支持 NetworkPolicy 的声明，如果大家有这个需求，可以选择支持 NetworkPolicy 的一些插件，比如 Calico, Weave 等等。\n  是否需要集群外的资源与集群内的资源互联互通； 大家的应用最初都是在虚拟机或者物理机上，容器化之后，应用无法一下就完成迁移，因此就需要传统的虚拟机或者物理机能跟容器的 IP 地址互通。为了实现这种互通，就需要两者之间有一些打通的方式或者直接位于同一层。此时可以选择 Underlay 的网络，比如 sriov 这种就是 Pod 和以前的虚拟机或者物理机在同一层。我们也可以使用 calico-bgp，此时它们虽然不在同一网段，但可以通过它去跟原有的路由器做一些 BGP 路由的一个发布，这样也可以打通虚拟机与容器。\n  K8s 的服务发现与负载均衡的能力。 K8s 的服务发现与负载均衡就是我们前面所介绍的 K8s 的 Service，但并不是所有的 CNI 插件都能实现这两种能力。比如很多 Underlay 模式的插件，在 Pod 中的网卡是直接用的 Underlay 的硬件，或者通过硬件虚拟化插到容器中的，这个时候它的流量无法走到宿主机所在的命名空间，因此也无法应用 kube-proxy 在宿主机配置的规则。\n  这种情况下，插件就无法访问到 K8s 的服务发现。因此大家如果需要服务发现与负载均衡，在选择 Underlay 的插件时就需要注意它们是否支持这两种能力。\n性能需求 我们可以从 Pod 的创建速度和 Pod 的网络性能来衡量不同插件的性能。\n  Pod 的创建速度。当我们创建一组 Pod 时，比如业务高峰来了，需要紧急扩容，这时比如说我们扩容了 1000 个 Pod，就需要 CNI 插件创建并配置 1000 个网络资源。Overlay 和路由模式在这种情况下的创建速度是很快的，因为它是在机器里面又做了虚拟化，所以只需要调用内核接口就可以完成这些操作。但对于 Underlay 模式，由于需要创建一些底层的网络资源，所以整个 Pod 的创建速度相对会慢一些。因此对于经常需要紧急扩容或者创建大批量的 Pod 这些场景，我们应该尽量选择 Overlay 或者路由模式的网络插件。\n  Pod 的网络性能。主要表现在两个 Pod 之间的网络转发、网络带宽、PPS 延迟等这些性能指标上。Overlay 模式的性能较差，因为它在节点上又做了一层虚拟化，还需要去封包，封包又会带来一些包头的损失、CPU 的消耗等，如果大家对网络性能的要求比较高，比如说机器学习、大数据这些场景就不适合使用 Overlay 模式。这种情形下我们通常选择 Underlay 或者路由模式的 CNI 插件。\n  如何开发自己的CNI插件 CNI 插件的实现通常包含两个部分：\n 一个二进制的 CNI 插件去配置 Pod 网卡和 IP 地址。这一步配置完成之后相当于给 Pod 上插上了一条网线，就是说它已经有自己的 IP、有自己的网卡了； 一个 Daemon 进程去管理 Pod 之间的网络打通。这一步相当于说将 Pod 真正连上网络，让 Pod 之间能够互相通信。  给 Pod 插上网线 步骤如下：\n  给 Pod 准备一个网卡。通常我们会用一个 \u0026ldquo;veth\u0026rdquo; 这种虚拟网卡，一端放到 Pod 的网络空间，一端放到主机的网络空间，这样就实现了 Pod 与主机这两个命名空间的打通。\n  给 Pod 分配IP地址。这个 IP 地址在集群里需要是唯一的。如何保障集群里面给 Pod 分配的是个唯一的 IP 地址呢？ 一般来说我们在创建整个集群的时候会指定 Pod 的一个大网段，按照每个节点去分配一个 Node 网段。比如说上图右侧创建的是一个 172.16 的网段，我们再按照每个节点去分配一个 /24 的段，这样就能保障每个节点上的地址是互不冲突的。然后每个 Pod 再从一个具体的节点上的网段中再去顺序分配具体的 IP 地址，比如 Pod1 分配到了 172.16.0.1，Pod2 分配到了 172.16.0.2，这样就实现了在节点里面 IP 地址分配的不冲突，并且不同的 Node 又分属不同的网段，因此不会冲突。这样就给 Pod 分配了集群里面一个唯一的 IP 地址。\n  配置 Pod 的 IP 和路由\n   第一步，将分配到的 IP 地址配置给 Pod 的虚拟网卡； 第二步，在 Pod 的网卡上配置集群网段的路由，令访问的流量都走到对应的 Pod 网卡上去，并且也会配置默认路由的网段到这个网卡上，也就是说走公网的流量也会走到这个网卡上进行路由； 最后在宿主机上配置到 Pod 的 IP 地址的路由，指向到宿主机对端 veth1 这个虚拟网卡上。这样实现的是从 Pod 能够到宿主机上进行路由出去的，同时也实现了在宿主机上访问到 Pod 的 IP 地址也能路由到对应的 Pod 的网卡所对应的对端上去。  给 Pod 连上网络 刚才我们是给 Pod 插上网线，也就是给它配了 IP 地址以及路由表。那怎么打通 Pod 之间的通信呢？也就是让每一个 Pod 的 IP 地址在集群里面都能被访问到。\n一般我们是在 CNI Daemon 进程中去做这些网络打通的事情。通常来说是这样一个步骤：\n 首先 CNI 在每个节点上运行的 Daemon 进程会学习到集群所有 Pod 的 IP 地址及其所在节点信息。学习的方式通常是通过监听 K8s APIServer，拿到现有 Pod 的 IP 地址以及节点，并且新的节点和新的 Pod 的创建的时候也能通知到每个 Daemon； 拿到 Pod 以及 Node 的相关信息之后，再去配置网络进行打通。  首先 Daemon 会创建到整个集群所有节点的通道。这里的通道是个抽象概念，具体实现一般是通过 Overlay 隧道、阿里云上的 VPC 路由表、或者是自己机房里的 BGP 路由完成的； 第二步是将所有 Pod 的 IP 地址跟上一步创建的通道关联起来。关联也是个抽象概念，具体的实现通常是通过 Linux 路由、fdb 转发表或者OVS 流表等完成的。Linux 路由可以设定某一个 IP 地址路由到哪个节点上去。fdb 转发表是 forwarding database 的缩写，就是把某个 Pod 的 IP转发到某一个节点的隧道端点上去（Overlay 网络）。OVS 流表是由Open vSwitch 实现的，它可以把 Pod 的 IP 转发到对应的节点上。    课后思考实践  在自己公司的网络环境中，选择哪种网络插件最适合？ 尝试自己实现一个 CNI 插件。  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ba9b30d793dc5d2e8faabee64c278479","permalink":"https://meixinyun.github.io/programmertalk/containertechnology/k8s/network/network03/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/containertechnology/k8s/network/network03/","section":"containertechnology","summary":"目标 CNI 定义 CNI如何选择 开发自己的CNI插件 什么是CNI CNI: Container","tags":null,"title":"理解CNI 和 CNI插件","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://meixinyun.github.io/programmertalk/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/programmertalk/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":"Basic Concept of Concurrency Race Conditions “A race condition occurs when two or more operations must execute in the correct order, but the program has not been written so that this order is guaranteed to be maintained.”\nMost of the time, this shows up in what’s called a data race, where one concurrent operation attempts to read a variable while at some undetermined time another concurrent operation is attempting to write to the same variable.\nAtomicity When something is considered atomic, or to have the property of atomicity, this means that within the context that it is operating, it is indivisible, or uninterruptible.\nSomething may be atomic in one context, but not another.\nWhen thinking about atomicity, very often the first thing you need to do is to define the context, or scope, the operation will be considered to be atomic in Everything follows from this.\n “indivisible” and “uninterruptible”  These terms mean that within the context you’ve defined, something that is atomic will happen in its entirety without anything happening in that context simultaneously\nDeadlocks, Livelocks, and Starvation” Deadlocks A deadlocked program is one in which all concurrent processes are waiting on one another. In this state, the program will never recover without outside intervention.\nLivelocks Livelocks are programs that are actively performing concurrent operations, but these operations do nothing to move the state of the program forward.\nHave you ever been in a hallway walking toward another person? She moves to one side to let you pass, but you’ve just done the same. So you move to the other side, but she’s also done the same. Imagine this going on forever, and you understand livelocks.  Livelocks are a subset of a larger set of problems called starvation.\nStarvation Starvation is any situation where a concurrent process cannot get all the resources it needs to perform work.\nKeep in mind that starvation can also apply to CPU, memory, file handles, database connections: any resource that must be shared is a candidate for starvation\n","date":1590851869,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590851869,"objectID":"0193ff480aa3d2816db061c18d10cc3d","permalink":"https://meixinyun.github.io/programmertalk/post/concurrency/","publishdate":"2020-05-30T23:17:49+08:00","relpermalink":"/programmertalk/post/concurrency/","section":"post","summary":"Basic Concept of Concurrency Race Conditions “A race condition occurs when two or more operations must execute in the correct order, but the program has not been written so that this order is guaranteed to be maintained.","tags":[],"title":"Concurrency","type":"post"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://meixinyun.github.io/programmertalk/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/programmertalk/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":null,"categories":null,"content":"什么是云原生 在云环境构建和运行 云环境包括：私有云，公有云，或混合云\n技术点  容器 服务网格 微服务 不可变基础设施 声明式API  目的 构建\n 容错性好 易于管理 便于观测 松耦合 的系统  云原需要关注的几个问题  如何让应用能充分利用云计算基础设施 如何具有快平台的移植性 如何让应用更具有可预见性，更健壮 如何实现运维自动化 如何实现微服务化  持续交付 交付的边界  交付内容决定了最终交付的结果 交付动作决定了交付过程的可靠性，连续性，稳定性 一次交付过程，包含: 网络的变更，服务载体的变更，而发布策略就是保证交付过程  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"9527a479947e854812a2dbc2bcaf55af","permalink":"https://meixinyun.github.io/programmertalk/devops/setup/%E5%AE%B9%E5%99%A8%E5%8C%96%E4%BA%A4%E4%BB%98/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/devops/setup/%E5%AE%B9%E5%99%A8%E5%8C%96%E4%BA%A4%E4%BB%98/","section":"devops","summary":"什么是云原生 在云环境构建和运行 云环境包括：私有云，公有云，或","tags":null,"title":"云原生-容器化交付","type":"docs"},{"authors":null,"categories":null,"content":"目标 本文定义了研发效能，它指的是一个组织持续快速交付价值的能力，可以从流动效率、资源效率和质量三个方面来衡量。其中流动效率是改进研发效能的核心抓手，它带来系统和全局的改进。\n研发效能质量度量 衡量指标  持续发布能力 需求响应周期 交付吞吐率 交付过程质量 对外交付质量  如下图:\n效能目标设定  部分团队的2-1-1 目标  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"87b603329b484083832cb700a9f34442","permalink":"https://meixinyun.github.io/programmertalk/ee/engineeringefficiency/%E7%A0%94%E5%8F%91%E6%95%88%E8%83%BD%E5%BA%A6%E9%87%8F/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/programmertalk/ee/engineeringefficiency/%E7%A0%94%E5%8F%91%E6%95%88%E8%83%BD%E5%BA%A6%E9%87%8F/","section":"ee","summary":"目标 本文定义了研发效能，它指的是一个组织持续快速交付价值的能","tags":null,"title":"研发效能-指标度量","type":"docs"},{"authors":["Ace Mei"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://meixinyun.github.io/programmertalk/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/programmertalk/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic","开源"],"title":"常见的面试问题","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bd75a5accee2be70c890e5c42f5df9e7","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/chapter7/collector/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/courses/golang/chapter7/collector/","section":"courses","summary":"","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"07510c1a546ae33275f93a29e8487c8d","permalink":"https://meixinyun.github.io/programmertalk/courses/golang/chapter7/mutator/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/courses/golang/chapter7/mutator/","section":"courses","summary":"","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Referrence  阿里专家带你玩转DevOps企业最佳实践\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c263d1d47d7bedb01d840229de39adb5","permalink":"https://meixinyun.github.io/programmertalk/devops/referrence/courses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/devops/referrence/courses/","section":"devops","summary":"Referrence 阿里专家带你玩转DevOps企业最佳实践","tags":null,"title":"","type":"devops"},{"authors":null,"categories":null,"content":"开源框架 Derrick ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8ce164cd9943be09c270b73a6b10444a","permalink":"https://meixinyun.github.io/programmertalk/devops/setup/cdtools/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/devops/setup/cdtools/","section":"devops","summary":"开源框架 Derrick","tags":null,"title":"","type":"devops"},{"authors":null,"categories":null,"content":"从零搭建CICD系统标准化交付流程 基础背景知识 Jenkins CI 选型及优势  成熟度高，周边生态丰富，1000+ 插件体系 Java开发工具调查占比 60%，CI市场占比70% 社区活跃度高  基于Jenkins CI 构建自动化交付流程 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6aff5ad7b8612c80722f4f2775a2d3cf","permalink":"https://meixinyun.github.io/programmertalk/devops/setup/cicdsetupfrom0to1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/devops/setup/cicdsetupfrom0to1/","section":"devops","summary":"从零搭建CICD系统标准化交付流程 基础背景知识 Jenkins CI 选型及优势","tags":null,"title":"","type":"devops"},{"authors":null,"categories":null,"content":"开源 CI/CD 构建框架 TekTon 的深入剖析 九辩 DevOps时代 5月13日\n简介 Tekton 是一个功能强大且灵活的 Kubernetes 原生 CI/CD 构建框架，用于创建持续集成和交付（CI/CD）系统。关于 Tekton ，网上可以搜到很多很多介绍文档，本文主要阐述我对 Tekton 的实现原理和背后的技术逻辑的一点理解。 Tekton 定义了 Task、TaskRun、Pipeline、PipelineRun、PipelineResource 五类核心对象，通过对 Task 和 Pipeline 的抽象，我们可以定义出任意组合的 pipeline 模板来完成各种各样的 CI/CD 任务，再通过 TaskRun、PipelineRun和PipelineResource 可以将这些模板套用到各个实际的项目中。 实现原理 高度抽象的结构化设计使得 Tekton 具有非常灵活的特性，那么 Tekton 是如何实现 workflow 的流转的呢？ Tekton 利用 Kubernetes 的 List-Watch 机制，在启动时初始化了 2 个 Controller、PipelineRunController 和 TaskRunController 。 PipelineRunController 监听 PipelineRun 对象的变化。在它的 reconcile 逻辑中，将 pipeline 中所有的 Task 构建为一张有向无环图(DAG)，通过遍历 DAG 找到当前可被调度的 Task 节点创建对应的 TaskRun 对象。\nDAG 支持 Tekton 对 DAG 的支持相对比较简单。在 Tekton 中一个 Pipeline 就是一张 DAG ，Pipeline 中的多个Task可是DAG中的节点。Task 默认并发执行，可以通过 RunAfter 和 From 关键字控制执行顺序。 示例：\n name: lint-repo taskRef: name: pylint resources: inputs: - name: workspace resource: my-repo name: test-app taskRef: name: make-test resources: inputs: - name: workspace resource: my-repo name: build-app taskRef: name: kaniko-build-app runAfter:  test-app resources: inputs:  name: workspace resource: my-repo outputs: name: image resource: my-app-image     name: build-frontend taskRef: name: kaniko-build-frontend runAfter:  test-app resources: inputs:  name: workspace resource: my-repo outputs: name: image resource: my-frontend-image     name: deploy-all taskRef: name: deploy-kubectl resources: inputs: - name: my-app-image resource: my-app-image from: - build-app - name: my-frontend-image resource: my-frontend-image from: - build-frontend 渲染出的执行顺序为： | | v v test-app lint-repo / v v build-app build-frontend \\ / v v deploy-all 相比于 Argo 等专注在 workflow 的项目而言， Tekton 支持的任务编排方式是非常有限的。常见的循环，递归，重试，超时等待等策略都是没有的。 条件判断 Tekton 支持 condition 关键字来进行条件判断。Condtion 只支持判断当前Task是否执行，不能作为 DAG 的分支条件来进行动态 DAG 的渲染。 condition： https://github.com/tektoncd/pipeline/blob/e2755583d52ae46907790d40ba4886d55611cd23/docs/conditions.md   condition检查失败(exitCode != 0)，task不会被执行，pipelineRun状态不会因为condition检查失败而失败。 多个条件之间 “与” 逻辑关系 PipelineResource 在 Task 间数据交换 作为 CI/CD 的工具，代码在什么时候 Clone 到 WorkSpace 中，如何实现的？Tekton 中抽象了 PipelineResource 进行任务之间的数据交换， GitResource 是其中最基础的一种。用法如下。 声明一个 Git 类型的 PipelineResource : kind: PipelineResource metadata: name: skaffold-git-build-push-kaniko spec: type: git params:  name: revision value: v0.32.0 name: url value: https://github.com/GoogleContainerTools/skaffold 在 Task 中引用这个 Resource 做为输入： kind: Task metadata: name: build-push-kaniko spec: inputs: resources:  name: workspace type: git steps:   name: build-and-push image: registry.cn-shanghai.aliyuncs.com/kaniko-project-edas/executor:v0.17.1 代码会被 clone 在 /workspace 目录。 Tekton 是如何处理这些 PipelineResource 的呢，这就要从 Taskrun Controller 如何创建 Pod 说起。 Tekton 中一个 TaskRun 对应一个 Pod ，每个 Pod 有一系列 init-containers 和 step-containers 组成。init-container 中完成认证信息初始化， workspace 目录初始化等初始化工作。 在处理 step-container 时，会根据这个 Task 引用的资源 Append 或者 Insert 一个 step-container 来处理对应的输和输出，如下图所示。    Task中Step执行顺序控制 Tekton 源自 Knative Build ，在 Knative Build 中使用 Init-container 来串联 Steps 保证 Steps 顺序执行，在上面的分析中我们知道 Tekton 是用 Containers 来执行 Steps ， Pod 的 Containers 是并行执行的， Tekton 是如何保证 Steps 执行顺序呢？\n这是一个 TaskRun 创建的 Pod 的部分描述信息，可以看到所有的 Step 都是被 /tekton/tools/entrypoints 封装起来执行的。 -wait_file 指定一个文件，通过监听文件句柄，在探测到文件存在时执行被封装的 Step 任务。 -post_file 指定一个文件，在Step任务完成后创建这个文件。通过文件序列 /tekton/tools/${index} 来对 Step 进行排序。\n args:  -wait_file /tekton/tools/0 -post_file /tekton/tools/1 -termination_path /tekton/termination -entrypoint /ko-app/git-init    -url https://github.com/GoogleContainerTools/skaffold -revision v0.32.0 -path /workspace/workspace command: /tekton/tools/entrypoint image: registry.cn-shanghai.aliyuncs.com/kaniko-project-edas/git-init:v0.10.2 name: step-git-source-skaffold-git-build-push-kaniko-rz765 args:  -wait_file /tekton/tools/1 -post_file /tekton/tools/2 -termination_path /tekton/termination -entrypoint /kaniko/executor    \u0026ndash;dockerfile=Dockerfile \u0026ndash;destination=localhost:5000/leeroy-web \u0026ndash;context=/workspace/workspace/examples/microservices/leeroy-web \u0026ndash;oci-layout-path=$(inputs.resources.builtImage.path) command: /tekton/tools/entrypoint image: registry.cn-shanghai.aliyuncs.com/kaniko-project-edas/executor@sha256:565d31516f9bb91763dcf8e23ee161144fd4e27624b257674136c71559ce4493 name: step-build-and-push   args:  -wait_file /tekton/tools/2 -post_file /tekton/tools/3 -termination_path /tekton/termination -entrypoint /ko-app/imagedigestexporter    -images \u0026lsquo;[{\u0026ldquo;name\u0026rdquo;:\u0026ldquo;skaffold-image-leeroy-web-build-push-kaniko\u0026rdquo;,\u0026ldquo;type\u0026rdquo;:\u0026ldquo;image\u0026rdquo;,\u0026ldquo;url\u0026rdquo;:\u0026ldquo;localhost:5000/leeroy-web\u0026rdquo;,\u0026ldquo;digest\u0026rdquo;:\u0026quot;\u0026quot;,\u0026ldquo;OutputImageDir\u0026rdquo;:\u0026quot;/workspace/output/builtImage\u0026rdquo;}]\u0026rsquo; command: /tekton/tools/entrypoint image: registry.cn-shanghai.aliyuncs.com/kaniko-project-edas/imagedigestexporter:v0.10.2 name: step-image-digest-exporter-lvlj9 实践 使用 Tekton 构建代码并部署到 SAE Serverless 应用引擎（ SAE ） 是阿里云上一款面向应用的 Serverless PaaS 平台，帮助 PaaS 层用户免运维 IaaS，按需使用，按量计费，实现低门槛微服务应用上云，有效解决成本及效率问题。支持 Spring Cloud、Dubbo 和 HSF 等流行的开发框架，真正实现了 Serverless 架构和微服务架构的完美融合。      接下来将使用 Tekton 部署一个 Spring Cloud 微服务应用到 SAE 平台。\n示例中的演示代码地址：https://github.com/alicloud-demo/spring-cloud-demo 1、前置条件 在 Kubernetes 集群上安装 Tekton ： https://github.com/tektoncd/pipeline/blob/master/docs/install.md\n创建一个 SAE 应用： https://help.aliyun.com/document_detail/122439.html\n2、定义一个 Git 资源 apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: spring-cloud-demo spec: type: git params:\n name: url value: https://github.com/alicloud-demo/spring-cloud-demo 3、定义构建和部署 Task 根据 SAE 官方文档进行部署，详情参考： https://help.aliyun.com/document_detail/110639.html  apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: build-deploy-sae spec: inputs: resources: - name: source type: git steps:\n name: build-and-deploy image: maven:3.3-jdk-8 command: [\u0026ldquo;mvn\u0026rdquo;, \u0026ldquo;clean\u0026rdquo;, \u0026ldquo;package\u0026rdquo;, \u0026ldquo;-f\u0026rdquo;, \u0026ldquo;source\u0026rdquo;, \u0026ldquo;toolkit:deploy\u0026rdquo;, \u0026ldquo;-Dtoolkit_profile=toolkit_profile.yaml\u0026rdquo;, \u0026ldquo;-Dtoolkit_package=toolkit_package.yaml\u0026rdquo;, \u0026ldquo;-Dtoolkit_deploy=toolkit_deploy.yaml\u0026rdquo;] securityContext: runAsUser: 0 4、定义 TaskRun 运行任务 apiVersion: tekton.dev/v1alpha1 kind: TaskRun metadata: name: build-deploy-sae spec: taskRef: name: build-deploy-sae inputs: resources:  name: source resourceRef: name: spring-cloud-demo 5、导入到kubernetes中运行 kubectl apply -f source-2-service-taskrun.yaml    6、查看日志 kubectl logs build-deploy-sae-pod-85xdk step-build-and-deploy 构建日志：\n部署日志：\n[INFO] Start to upload [provider3-1.0-SNAPSHOT.jar] using [Sae uploader]. [INFO] [##################################################] 100.0% [INFO] Upload finished in 3341 ms, download url: [https://edas-hz.oss-cn-hangzhou.aliyuncs.com/apps/K8S_APP_ID/37adb12b-5f0c-4711-98ec-1f1e91e6b043/provider3-1.0-SNAPSHOT.jar] [INFO] Begin to trace change order: e2499b9a-6a51-4904-819c-1838c1dd62cb [INFO] PipelineName: Batch: 1, PipelineId:f029314a-88bb-450b-aa35-7cc550ff1329 [INFO] Waiting\u0026hellip; [INFO] Waiting\u0026hellip; [INFO] Waiting\u0026hellip; [INFO] Waiting\u0026hellip; [INFO] Waiting\u0026hellip; [INFO] Waiting\u0026hellip; [INFO] Waiting\u0026hellip; [INFO] Waiting\u0026hellip; [INFO] Deploy application successfully! [INFO] \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; [INFO] BUILD SUCCESS [INFO] \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; [INFO] Total time: 32:41 min [INFO] Finished at: 2020-04-15T10:09:39+00:00 [INFO] Final Memory: 47M/190M [INFO] \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; 7、验证部署结果 在 SAE 控制台查看变更记录：\n验证应用访问：\n总结 区别于传统的 CI/CD 工具（Jenkins），Tekton 是一套构建 CICD 系统的框架。Tekton 不能使你立即获得 CI/CD 的能力。但是基于 Tekton 可以设计出各种花式的构建部署流水线。\n得益于 Tekton 良好的抽象，这些设计出的流水线可以作为模板在多个组织，项目间共享。Tekton 源自 Knative 的 Build-Template 项目，设计之初的一个重要目标就是使人们能够共享和重用构成 pipeline 的组件，以及 Pipeline 本身。在 Tekton的RoadMap 中 Tekton Catelog 就是为了实现这一目标而提出的。 区别于 Argo 这种基于 Kubernetes 的 Workflow 工具， Tekton 在工作流控制上的支持是比较弱的。一些复杂的场景比如循环，递归等都是不支持的。更不用说 Argo 在高并发和大集群调度下的性能优化。这和 Tekton 的定位有关， Tekton 定位于实现 CICD 的框架，对于 CICD 不需要过于复杂的流程控制。 大部分的研发流程可以被若干个最佳实践来覆盖。而这些最佳实践应该也必须可以在不同的组织间共享，为此 Tekton 设计了 PipelineResource 的概念。PipelineResource 是 Task 间交互的接口，也是跨平台跨组织共享重用的组件，在 PipelineResource 上还可以有很多想象空间。 作者信息： 九辩，阿里巴巴高级开发工程师，负责阿里云EDAS(企业级分布式应用服务)应用生命周期研发工作，长期关注云时代微服务的部署和治理工作。\n来源：本文转自公众号阿里巴巴中间件，点击查看原文。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d99387109fca410f733a7dbed3b84c5a","permalink":"https://meixinyun.github.io/programmertalk/ee/cicd/z/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/ee/cicd/z/","section":"ee","summary":"开源 CI/CD 构建框架 TekTon 的深入剖析 九辩 DevOps时代 5月13日 简介","tags":null,"title":"","type":"ee"},{"authors":null,"categories":"c++ 随笔","content":" apr\n APR 是什么  apr 运行时库目标是： 提供一个可移植的运行时支持库，是Apache HTTP服务器的支持库，提供了一组映射到下层操作系统的API。 可以屏蔽平台相关性，使程序员可以叫方便的写出在不同平台上移植的程序。\t 功能特性 1. 内存管理和内存池功能 2. 原子操作（Linearizability） 3. 动态库处理 4. 文件I/O 5. 命令参数解析 6. 锁机制（Locking） 7. 散列表和数组 8. Mmap（mmap）功能 9. 网络套接字和协议 10. 线程，进程和互斥锁功能 11. 共享内存功能 12. 时间子程序 13. 用户和组ID服务  应用背景  由于公司的设备品种繁多，staragent 作为运维工具的基础设施，广泛部署在公司的各个服务器上，考虑代码的维护性，和可移植性，我们考虑采用 apr 运行库来实现staragent的客户端的核心功能。其中主要涉及APR 库的 1, 5, 6, 9 10,11 等功能。本次我们重点了解如何搭建 apr 库的基本环境，编译构建；项目集成。以及其网络库存，进程间处理库。  apr 运行环境搭建  为方便工程测试，我们将依赖的源码包编译为静态连接库，在项目中设置依赖变量，可以保证只用编译 一次apr库； 1. 下载源码包： https://apr.apache.org/\t2. 执行构建 为了方便测试，我创建了一个编译目录， 如下 是创建的目录结构： ├── build_dep.sh ├── build.sh ├── CMakeLists.txt ├── depend │ └── compiled ├── log └── src ├── apr-1.5.2 ├── common └── proc 下载的源码，解压后，放在src/apr-1.5.2目录下； 在当前路径下执行，buil_dep.sh 脚本，会将apr库编译安装在 depend/compiled/ubuntu_64/目录下。 以下时编译安装脚本。 #!/bin/sh set -x curDir=`pwd` mkdir -p depend/compiled/ubuntu_64/ echo \u0026quot;build apr-1.5.2\u0026quot; cd src/apr-1.5.2/ ./configure --prefix=${curDir}/depend/compiled/ubuntu_64 make \u0026amp;\u0026amp; make install 3. 设置项目依赖路径 编译安装完毕后，可以设置路径依赖，我们采用cmake 构建项目工程； 需要在构建是指定依赖路径： -DDEP_ROOT=/XXX/depend/compiled/ubuntu_64 4. 添加引用 需要在项目中设置apr 引用和依赖 include_directories(${DEP_ROOT}/include/apr-1) link_direcoteries(${DEP_ROOT}/lib) target_link_libraries(xxx apr-1) 5. 执行构建 mkdir -p build \u0026amp;\u0026amp; cd build \u0026amp;\u0026amp; cmake .. \u0026amp;\u0026amp; make  apr 库使用的基本结构及编程风格 apr 并不是一个框架库，优点是 可以很方便的和其它库联合使用  apr库 编程的代码基本结构（skeleton code）  一个基本的apr库骨架代码如下： 会调用apr_initialize（） 初始化， 调研apr_terminate() 终结。 /* pseudo code of libapr. error checks omitted */ apr_status_t rv; apr_foo_t *foo; rv = apr_foo_create(\u0026amp;foo, args...);/* create a @foo object by @args */ rv = apr_foo_do_something(foo, args...); /* do something with @foo */ apr_foo_destroy(foo); /* destroy the @foo object. Sometimes, this is done implicitly by * destroying related memory pool. Please see below */  apr 的编程风格  ＊ 命名规则很简单，清晰 ＊ 数据类型大部分对用户隐藏，（可通过API操作属性；未实现完全的类型） ＊ 大部分的返回结果为 apr_status_t. 返回结果会作为参数适用。 ＊ 基于内存池  apr 内存池 大部分的libapr API 依赖于内存池。通过内存池，可以很方便的管理内存块。 如果你有很多内存快，需要申请和释放，管理起来就会比较麻烦，可能导致内存泄漏。apr 的内存池库可以解决此问题。apr 所有的内存申请，都是有内存池负责申请，调用 apr_pool_destroy() 即可释放掉所有的内存。好处有二： 其一： 预防内存泄漏 其二： 分配内存块的开销相对降低。 内存池迫使我们基于会话编程；一个内存池，可以看成一个会话上下文，一些具有相同生命周期的对象集合。你可以使用一个会话上下文来管理一个对象集合。 基本思想是： 在会话开始的时候，创建一个内存池； 你不需要关心他们的生命周期，最后，会话结束的时候，释放掉内存池即可。 备注：  内存池操作的基本API  /* excerpted from apr_pools.h */ /* 内存创建 */ APR_DECLARE(apr_status_t) apr_pool_create(apr_pool_t **newpool, apr_pool_t *parent); /* 内存申请 */ APR_DECLARE(void *) apr_palloc(apr_pool_t *p, apr_size_t size); /* 内存池销毁 */ APR_DECLARE(void) apr_pool_destroy(apr_pool_t *p);  申请，释放 一块内存空间  /* excerpted from mp-sample.c */ apr_pool_t *mp; /* create a memory pool. */ apr_pool_create(\u0026amp;mp, NULL); /* allocate memory chunks from the memory pool */ char *buf1; buf1 = apr_palloc(mp, MEM_ALLOC_SIZE); /* destroy the memory pool. These chunks above are freed by this */ apr_pool_destroy(mp);  几个内存池使用建议：  1. apr_palloc() 并没有最大的内存限制，内存池主要是基与小的内存块设计的，初始化的大小是 8kbyte; 若需要使用较大的内存块，比如几M ，建议不要使用内存池。 2. 默认，内存池管理 不会主动将申请的内存返回给操作系统，如果程序长时间运行，可能会有问题。建议限定内存池的上限。 #define YOUR_POOL_MAX_FREE_SIZE 32 apr_pool_t *mp; apr_pool_create(\u0026amp;mp, NULL); apr_allocator_t* pa = apr_pool_allocator_get(mp); if（pa）｛ apr_allocator_max_free_set(pa, YOUR_POOL_MAX_FREE_SIZE); ｝ 3. 以上例子 给出了如何申请和释放内存；libapr 封装了内存的申请和释放实现逻辑。 apr_pool_create（）／ apr_pool_destroy(mp); 成对出现。 rv = apr_initialize();／ apr_terminate(); 成对出现。  内存池的清理及销毁相关API  场景1: apr_pool_clear() 与apr_pool_destroy()类似，但内存池依然可以使用；一个典型的场景如下： /* sample code about apr_pool_clear() */ apr_pool_t *mp; apr_pool_create(\u0026amp;mp, NULL); for (i = 0; i \u0026lt; n; ++i) { do_operation(..., mp); apr_pool_clear(mp); } 场景2: 我们需要在内存池 clear/destroy 时执行特定的操作，可以通过注册callback函数实现。在回调函数里，你可以实现任何收尾处理代码。 apr_pool_cleanup_register()  apr内存池构建结构  apr的内存池，是基于树结构的。每个内存池拥有一个父的内存池， apr_pool_create() API，第二个参数，是其父内存池的地址，传递为NLL，则当前的内存池，最为根节点。 当调用apr_pool_destroy()时，内存池的子节点也会被销毁， 当调用apr_pool_clear() 时，内存池可用，但其子节点的内存池被销毁。当一个内存池的子节点 销毁时，其 cleanup 函数会被调用。  使用建议  传递 NULL 作为内存池的 cleanup 回调函数 （ BUG ） 应 像下面的代码 传递 apr_pool_cleanup_null /* pseudo code about memory pool typical bug */ /* apr_pool_cleanup_register(mp, ANY_CONTEXT_OF_YOUR_CODE, ANY_CALLBACK_OF_YOUR_CODE, NULL); THIS IS A BUG */ /* FIXED */ apr_pool_cleanup_register(mp, ANY_CONTEXT_OF_YOUR_CODE, ANY_CALLBACK_OF_YOUR_CODE, apr_pool_cleanup_null);  apr 进程处理库  为了便于理解，我们基于一个功能来学习apr 进程库。 我们的任务是： 通过apr进程库，创建一个子进程，在子进程中执行一个shell命令，然后 在父进程中，将该子进程执行的结果，在终端输出。 我们需要学习的知识： 如何利用apr 库 创建一个子进程； 子进程执行完毕后，父进程如何获取子进程执行的结果。  apr 进程库： 进程属性对象  /* excerpted from apr_thread_proc.h */ APR_DECLARE(apr_status_t) apr_procattr_create(apr_procattr_t **new_attr, apr_pool_t *cont); 第一个参数： 结果参数； 第二个参数： 进程使用的内存池对象 apr_procattr_t 结构比较复杂，提供了get/set方法； 在 apr_thread_proc.h 中定义；  detach API  APR_DECLARE(apr_status_t) apr_threadattr_detach_set(apr_threadattr_t *attr, apr_int32_t on); 通过此API，可以设置子进程为分离的进程（detached） ; 不同的操作系统(detached)的含义不同； 在windows 系统里，当子进程为命令行应用，并且不需要看控制台的时候，设置为分离模式；在 linux 操作系统里，如果子进程为一个服务进程则建议设置为分离模式。 默认的模式是： non-detachable; typedef enum { APR_SHELLCMD, /**\u0026lt; use the shell to invoke the program */ APR_PROGRAM, /**\u0026lt; invoke the program directly, no copied env */ APR_PROGRAM_ENV, /**\u0026lt; invoke the program, replicating our environment */ APR_PROGRAM_PATH, /**\u0026lt; find program on PATH, use our environment */ APR_SHELLCMD_ENV /**\u0026lt; use the shell to invoke the program, replicating our environment */ } apr_cmdtype_e; apr_cmdtype_e 主要作用： （1）设置执行进程是否是使用shell; APR_SHELLCMD 和 APR_SHELLCMD_ENV 两种模式，是使用 shell 来 创建一个进程；可与system(3)做比较。 与system(3) 和 fork 或 exec 比较，有时被认为一种容易的交互方式； 在 libapr 建议 不用 shell 来执行一个进程。理由是： 虽然利用 shell 来执行任务 好像是易于扩展，但实际存在安全漏洞。 除非 你很清楚自己在执行什么，否则不要用 APR_SHELLCMD 或 APR_SHELLCMD_ENV。 （2）另一个不同 主要是，环境变量。 传递环境变量有两种方式： （a）使用 APR_PROGRAM_ENV 或 APR_SHELLCMD_ENV. 子进程会获得父进程环境变量的一个副本；子进程修改任务环境变量的值并不会影响父进程。 （b）通过设置 apr_proc_create() 函数的 参数来传递。 （3）最后一个不同主要是和 PATH 环境变量有关。 APR_PROGRAM_PATH 是唯一的，只用通过使用 APR_PROGRAM_PATH 这种模式，才可以用一个 程序的名称 代替 程序的路径。 比如： 可以使用 \u0026quot;ls\u0026quot; 启动一个子进程。libapr 会 从PATH的环境变量里搜索 其确切的路径。否则我们就传递一个绝对路径，如 \u0026quot;/bin/ls\u0026quot; ; 作者不推荐使用 可能会存在安全隐患。  进程创建API 创建进程函数  /* excerpted from apr_thread_proc.h */ APR_DECLARE(apr_status_t) apr_proc_create(apr_proc_t *new_proc, const char *progname, const char * const *args, const char * const *env, apr_procattr_t *attr, apr_pool_t *pool); 函数说明： 第一个参数： 子进程返回值，内存由apr 库创建； 第二个参数： 子进程程序的名称，建议传递绝对路径。 第三个参数： 子进程运行的参数列表；构造子进程参数列表方法如下： /* pseudo code of args to apr_proc_create() */ int argc = 0; const char* argv[32]; /* 32 is a magic number. enough size for the number of arguments list */ argv[argc++] = progname; /* program path of the command to run */ argv[argc++] = \u0026quot;-i\u0026quot;; argv[argc++] = \u0026quot;foo\u0026quot;; argv[argc++] = \u0026quot;--longopt\u0026quot;; argv[argc++] = \u0026quot;bar\u0026quot;; argv[argc++] = NULL; /* The final element should be NULL as sentinel */ 该参数列表，第一个参数 为 程序运行的路径，最后一个参数最为哨兵，传递NULL。 第四个参数： 传递给子进程环境变量列表， 和第三个参数类似，参数列表的最后一个值为NULL，最为哨兵。 第五个参数： apr_procattr_t; 进程属性对象，在1中介绍；通过apr_proattr_create()API创建 第六个参数： 内存池地址；  获取子进程执行状态。  /* excerpted from apr_thread_proc.h */ APR_DECLARE(apr_status_t) apr_proc_wait(apr_proc_t *proc, int *exitcode, apr_exit_why_e *exitwhy, apr_wait_how_e waithow); 函数说明： 第一个参数： 之前通过apr_proc_create()创建的进程属性对象； 第二，三个参数： 结果参数； 第四个参数：等待子进程的方式： APR_WAIT 当前进程会阻塞，直到子进程终止； APR_NOWAIT 非阻塞；  父子进程间通信的一种方式 pipe 管道时一种内部进程通信的方式。在父子进程间通信，比较方便。通过管道，父进程可以传递字符流到子进程的标准输入；同样，父进程也可以通过管道收到子进程发的字符流。子进程可以将输出结果写入标准输出或标准错误。子进程可以不 care 管道，只需要从标准 输入／输出／错误里读写； 相对的，从父进程的视角来看，管道就像一个文件对象。父进程只需要调用 apr_file_read() 或 apr_file_write() 从其管道 来发送或接收 数据  管道处理函数 /* excerpted from apr_thread_proc.h */ APR_DECLARE(apr_status_t) apr_procattr_io_set(apr_procattr_t *attr, apr_int32_t in, apr_int32_t out, apr_int32_t err); 函数参数说明： 第一个参数：进程属性对象； 第2，3，4 参数 分别对应改进程的标准 输入/输出/错误。  后记 apr库 涉及内容比较多，是一个循序渐进的过程。我是刚入门学习，水平有限，欢迎大家交流指证。\n我的计划如下： 熟悉apr库的使用方法 -\u0026gt; 应用实战 －\u0026gt; 研究其源代码 －\u0026gt; 自己编写类似的工具箱\n有兴趣的同学欢迎加我，相互交流。\n未完待续。。。。\n相关链接： ［1］ apr官网 ［2］ apr 使用教程\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"41f18e6a32ff0444b1c81947dc95f19d","permalink":"https://meixinyun.github.io/programmertalk/post/2016/2016-02-23-apr-note/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/post/2016/2016-02-23-apr-note/","section":"post","summary":"apr APR 是什么 apr 运行时库目标是： 提供一个可移植的运行时支持库，是","tags":null,"title":"LibApr 阅读笔记","type":"post"},{"authors":null,"categories":"运维体系建设 CMDB 学习笔记","content":"以CMDB为基础构建DevOps平台 目标 CMDB是运维其它系统的基础。通过对业界CMDB系统相关资料的学习整理，结合自身实践的经验，尝试总结一些CMDB建设的观点和思路。\n本文概述  基础概念（CMDB相关的概念） CMDB的模型 CMDB在运维体系中的地位（全局观） CMDB的实践 CMDB的经验总结 CMDB相关的产品介绍  基础概念 配置管理数据库( Configuration Management Database,CMDB) :\n是一个逻辑数据库，包含了配置项全生命周期的信息以及配置项之间的关系(包括物理关系、实时通信关系、 非实时通信关系和依赖关系). - 来自百度  也称配置管理，配置管理一直被认为是 ITIL 服务管理的核心，因为其他所有流程均需要使用配置管理数据库 (CMDB).\n举例说明： 比如说变更系统发起了一个部署请求，要部署某个版本到现网，部署完成之后，上层的变更系统会把变更的 结果写到CMDB中，对配置进行归档；在某个机器down机，此时可以快速的知道该机器的具体用途，确定影响的业务；当 机器需要重新恢复的时候，可以快速的根据CMDB中的信息进行恢复  配置管理和配置文件管理\n ITIL所讲的配置管理是从软件工程管理角度出发的，把一切对象都当做配置，，比如说源代码、文档、人员、服务器 甚至硬盘和内存等等。所以说他和业务程序的配置管理有着本质的不同，为了有效区分，我们又习惯说业务程序的配 置管理叫配置文件管理。但又有着一定的联系，在ITIL中，业务程序的配置可能会以一个配置项存在， 附属在应用程序上。  配置管理和资产管理\n区别 核心的区别点就是导向，配置管理是面向业务管理，而非成本，这个会决定配置管理的粒度。当前如果业务非常简单，不需要对服务器端口进行管理，此时则不需要考虑纳入端口的管理，否则增加管理的代价\n配置项\n配置项是指要在配置管理控制下的资产、人力、服务组件或者其他逻辑资源。\n 从整个服务或系统来说，包括硬件、软件、文档、支持人员到单独软件模块或硬件组件（CPU、内存、SSD、硬盘等等。 配置项需要有整个生命周期（状态）的管理和追溯（日志）。对配置项的分类，我一般从逻辑资源和物理资源两个角度 来分解，然后层次化分解，这个思路会让你特别清晰，不会混乱。  属性\n一个配置项就是一个对象，有对象便有属性，属性是一个配置项的具体描述。\n 比如说服务器这个配置项，他的具体描述有在哪个机房、哪个机柜的哪个位置、现在是否有业务运行、具体谁负责等等。在后面的模型篇里会对属性做全面的梳理，完成现实世界到模型世界的转换。另外配置项和属性可以转换，比如说IP地址，他肯定是一个资源对象存在。但是从服务器的角度来说，它作为一个属性存在，更准确的说是网卡的属性  模型 模型的核心职责，就是把配置项和属性逐条的梳理出来。模型整理，重点做了四个方面工作：\n1、配置管理系统的角色\n第一、应用运维，负责服务器上的业务信息维护； 第二、基础运维，负责机房、机柜及其服务器物理信息的准确性； 第三、配置管理员，负责基础信息的维护，比如说业务分类，人员信息； 第四、查询类角色，比如研发。CMDB是核心的资源信息管理系统，一般不轻易开放权限。  2、配置项识别与定义\n 这是重点工作，没有简单的方法可循，细致活，基于上图的【配置项】的物理资源和逻辑资源的不断分解，根据业务需要最终识别出要管理的配置项。然后对每一个配置项进行整理，确定要管理的属性。形成类似的下表：  就拿最核心的服务器资源来说，会形成如下表的信息整理\n逐个进行整理，在上表中有几个方面需要注意：\n第一、每个配置项目确定了维护角色，他在后续的过程中，需要对这个准确性负责，确定维护的职责边界。 第二、要整理出配置项的关联，比如说上表中的所属机房、所属机柜。 第三、这个表不是数据库的设计表，具体数据库的设计表是开发人员根据这个模型参考实现。  3、基于场景的配置管理规范\n配置管理的核心目的是为了确保配置信息集中管理，并且是准确的管理。在这个里面需要做两个核心的工作。\n第一、配置项的规范化管理； 第二、面向配置项的流程规范化管理，没有一套与之匹配的配置流程，最后配置管理都会混乱不堪，这个系统也就形同虚设。  4、状态变迁图 用一张图来说明资源状态的变化，便于更好的基于场景和变更来控制配置项状态的变更，其实也就是它的生命周期管理。\nCMDB在运维生态中的地位（全局观） CMDB 是 Devops的重要元数据基础，建立基础设施层，平台层和持续交付系统，持续反馈系统的联系纽带。 互联网时代，公司快速发展，软件研发的速度和质量是永恒的追求目标。敏捷研发体系 离不开持续交付体系的支撑，持续交付依托于 持续集成，持续部署，持续反馈。这些系统的构建，需要通过统一的元数据 平台，来管理底层基础资源，从而建立起：服务，资源，人员的关系。  该图的几个核心概念如下：\n 基础设施服务： 计算，存储，网络的抽象服务 平台服务： 存储服务，MR服务，Cache缓存服务等 资源管理层次：基础设施层元信息，Pass层元信息，应用资源元信息 云管控： 私有云，公有云，混合云 Devops : 服务构建，交付，运行 和运营  可以看出， CMDB 承载了底层资源的元信息的沉淀，作为支撑系统，为上层的软件生命周期管理系统提供元数据服务。 为了更清楚的了解CMDB的地位以及和运维其他系统直接的关系，我们看下DevOps 的全景图，\n在Devops 全景图中，反应了持续交付，及敏捷体系的建设核心要素。持续交付离不开 CI/CD，部署流水线的支撑； 完成服务的持续优化与升级，需要持续反馈。(在 有效构建海量运营的持续反馈能力文章中，详细讲述了如何实现海量运维体系中的持续反馈。)\n从运维平台化的角度，来看ITIL，CMDB和Devops与之间关系。\n如图：\n其中：ITIL ：包括运维体系建设中的NOC，事件管理，应急响应（ONCall），变更，发布管理，知识问答，运维知识库等。 这些系统的构建依托中间层：自动化运维 和 数据化运维的支撑。 而实现运维的自动化和数据化，则需要通过 CMDB 的元数据层，建立各个支撑系统 和 底层基础设施直接的映射关系。\nCMDB 建设中的演化 面向应用的IT资源模型框架 IT资源模型框架，反应组织，部门，人员 和 基础设施层，PaaS 层以及业务层的关系，如图： 以应用为中心的CMDB模型 从应用视角，来看应用的管理会包含应用环境，服务资源，部署资源，以及运维动作。不同的数据，由不同的团队或系统负责。可以从资源，运维动作，状态三个维度构建应用的管理体系。\n 应用管理：明确应用管理的边界，统一术语和名称 IP资源管理：IT资源管理，实现全局分层配置 应用支撑系统管理：结合应用管理需求，纳管应用描述，环境配置，制品标准，管理规程，维护手段的等应用信息  CMDB的实践 构建CMDB的建设的原则 1. 应用CMDB必须提供统一的应用元数据管理能力，和应用类型无关 2. 应用CMDB建设的核心诉求是应用生命周期管理 3. 应用CMDB必须以应用为中心，而非以基础资源为中心 4. 应用CMDB必须要从应用的角度构建起与IT资源的弹性关系 5. 应用CMDB是为应用资源、动作、状态的统一管理提供支撑 6. 应用CMDB要有统一的基础资源层CMDB作为基础 7. 应用CMDB的核心场景就是持续交付  构建CMDB的过程，是一个渐进式的迭代过程 CMDB 的构建不是一蹴而就，是一个渐进式的过程。从CMDB的微内核和弹性CMDB模型库开始，通过 “自动发现+标准流程+人工维护”的来完善CMDB数据库。\n所谓的CMDB的微内核，是指以应用、集群和主机三个概念就可以构建起一个CMDB，基于这三个概念，可以不断去向周边扩散。 主机可以在其关联或者拥有的资源上不断去扩展，比如说主机所在的机柜、机柜所在的机房、机器关联的交换机等等。\n所谓的 弹性模型 是指： 由对象的弹性和对象CI及其关联的弹性定义实现的。\n下图的案例： 对象的弹性定义： 对象CI及其关联的弹性定义:\n自动发现 是降低维护成本的一种有效方式, 但确保一个CMDB库信息的有效性，还需要标准化的流程 和人工维护。\n标准化的流程 是运维资源信息变更的场景化流程梳理，比如说机房搬迁，服务器搬迁，服务器下架等等，这个流程需要识别出来，并确定相应的CMDB配置项状态变更过程。\n人工维护，在有些流程没有构建起来的情况下，则需要通过人工变更的能力把CMDB信息维护准确，比如说主机所属负责人变更，这个时候不建议流程了，可以通过人工直接变更完成。那如何确保维护准确呢？通过外围系统来控制，比如说告警信息，如果负责人没有变更告警是直达原有负责人，导致告警不准确。\nCMDB的经验总结 其实CMDB真的是非常简单的系统，至少在两家公司做的CMDB都是非常短的时间完成，最多两个月。但是其实施的过程很多经验可以分享。\n1、导致CMDB失败的因素\nA、缺少管理层承诺----没有管理层的承诺，CMDB不可能成功。 B、在复杂流程上消耗太多的时间---我们是创建一个CMDB库，不是一个流程系统。 C、没有创建相应的工作指导文档---指导如何管理和维护CMDB。 D、没有指定配置项负责人----确保配置项有人专职维护。 E、目标过大，涵盖太多的功能----比如说IT采购和预算管理等等。 F、颗粒度不合适----配置合理的CMDB的配置项层次和粒度非常重要。 G、存在组织隔阂----CMDB是一个集成体系，靠流程中的每一个人通力协作，而不是某个人。  2、导致CMDB成功的因素\nA、业务导向。比如说我们在CMDB的新的系统中实时加入QR码技术，为了降低资产盘点的工作量。 B、能自动发现就自动发现，降低配置管理的成本，但自动发现的信息不能用来做告警。 C、配置项的管理员必须全程参与，需求定型、测试及验收等等。 D、CMDB系统建设完成之后，其他系统必须和他联动。比如说监控、质量、容量等等，用场景驱动配置项的管理。 E、流程一定要平台化，不要让流程脱离CMDB存在，比如说搞一个OA流程，这个是很致命的。 F、CMDB要持续演进，特别是云端资源的管理。 G、配置项和流程必须要文档化，后期要进行CMDB培训。  参考文献  【平台篇】运维平台之CMDB系统建设 \n 长文 | 重构CMDB，避免运维之耻 \n  T资源管理系统Sm@rtCMDB\n 蓝鲸CMDB解决方案\n 优维科技老王：与其说建设CMDB，不如说建设IT资源图谱\n 优维科技王津银：创新赋能 银行业新一代CMDB的落地实践\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"772b4ddfd11170384160acae8d02d7ab","permalink":"https://meixinyun.github.io/programmertalk/post/2019/2019-10-16-cmdb%E5%BB%BA%E8%AE%BE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/post/2019/2019-10-16-cmdb%E5%BB%BA%E8%AE%BE/","section":"post","summary":"以CMDB为基础构建DevOps平台 目标 CMDB是运维其它系","tags":null,"title":"以CMDB为基础构建DevOps平台","type":"post"},{"authors":null,"categories":"系统架构 微服务","content":"概念篇 微服务相关资源收集，从是什么，为什么，怎么样，以及以后的演化几个纬度。\n什么是微服务 （是什么）  什么是服务网格以及为什么我们需要服务网格\n Service Mesh的诞生,从分布式到微服务\n解决什么问题 （为什么）  原理解析Service Mesh与ESB、API管理与消息代理的关系\n 下一代微服务\n解决方案 （怎么样） 业界解决方案对比，各自的优缺点  解读2017之Service Mesh：群雄逐鹿烽烟起\n Service Mesh 时代的选边与站队\n 面向Kubernetes的新开源Service Mesh : Conduit登场\n原理篇 istio 各个组件的介绍  官网-中文文档\n 重新定义Service Mesh的新生服务网格Istio\n 服务网格新生代Istio进化，与传统模式相较5大特性更助容器扩展\n 聊聊ServiceMesh 数据面板 Envoy\n工程实践（如何做）  蚂蚁金服大规模微服务架构下的Service Mesh探索之路\n Service Mesh 数据平面 SOFAMosn 深层揭秘\n 微博Mesh服务化改造如何支撑\n案例实战  使用 Istio治理微服务入门\n展望未来（如何发展）  \u0026ldquo;Introduction to Service Meshes\u0026rdquo; ServiceMesh 中文网 Service Mesh Meetup\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0801adacf52ba63e90a92206da6f672d","permalink":"https://meixinyun.github.io/programmertalk/post/2018/2018-09-11-%E5%BE%AE%E6%9C%8D%E5%8A%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/post/2018/2018-09-11-%E5%BE%AE%E6%9C%8D%E5%8A%A1/","section":"post","summary":"概念篇 微服务相关资源收集，从是什么，为什么，怎么样，以及以后","tags":null,"title":"微服务 ServiceMesh学习资源","type":"post"},{"authors":null,"categories":"运维体系建设 学习笔记","content":"有效构建海量运营的持续反馈能力 本文来自对大梁先生[DevOps最后一棒，有效构建海量运营的持续反馈能力]一文的学习和理解。\n本文的思路梳理 服务的产品螺旋式升级和快速迭代，离不开Devops 体系的支撑。在Devops 体系的最后一个环节，包含两个维度：\n 1. 运维活动的质量运营与终结 2. 产品的技术运营和生命周期的终结  本文的讨论的焦点是：在产品生命周期结束前，如何通过在技术运营阶段构建的质量体系，来实现（对服务的）持续反馈和优化。\n首先，对 Devops 体系定义（计划，需求，设计，开发，测试，部署，运营，EOL）； 通过监控，告警，运营来做持续反馈。\n其次：提出来监控，告警，运营完成质量体系的核心要素； 通过做立体监控+ 舆情监控实现360无死角监控；\n再次，立体监控带来的海量的报警，需要解决告警泛滥问题\n 1. “理清监控对象与指标的关系”，通过指标分类（低层次，高层次），监控的数据归一化（将采集和 计算的数据）转换为值和率 后，通过表报和告警的形式输出，达到分析状态和发现异常的目标。 2. 提出了通过三种技巧（溯源，根源，优先）来实现对海量告警事件分析处理 3. 介绍了常用的动态算法和告警收敛算法来对监控告警收敛  最后： 介绍了织云监控的指标体系，和织云监控的质量体系\n背景介绍 Devops体系介绍 DevOps的体系，它最后的一个环节，就是做运营和终结的环节,它应该包含两个纬度:\n 一是这次运维活动的质量运营与终结； 二是产品的技术运营和生命周期的终结  本文讨论的焦点： 在产品生命周期结束前，我们在技术运营阶段构建的质量体系，以实现持续反馈和优化的目标\n运维如何对产品做持续反馈？ 通过监控，告警，运营实现（对服务的）持续反馈和优化 。\n监控，告警，运营 核心要素   监控——覆盖率、状态反馈、指标度量\n 监控要做到360度无死角，业务出现了什么问题都能发现，有了监控的反馈，可以看到实时监控的状态，同时，当指标发生变化的时候也需要看到一些反馈    告警——时效性、准确性、触及率\n 业务越来越复杂，层次越来越多，每一个监控点都会产生数据指标、状态异常，会收到越来越多的告警。未看到或者看到未处理都需要承担责任，因为收到的并非都是误告警。最重要还要有触及率，告警由谁发现与处理    运营——RCA、事件管理、报表/考核\n 问题再三出现、必须从根源优化。通过事件管理机制保证 RCA 可以落地，最后通过报表和考核去给运维赋予权利推动相关优化活动的开展，包括架构和代码的优化等等    如何构建360无死角立体监控 立体监控 + 舆情监控\n立体监控 业务按照不同的层级进行管理，从下向上，有服务器层、数据库、逻辑层、中间计算的这一层，有接入层、负载 均衡，有我们的机房，DNS服务、客户端、用户端，为了做到无死角，我们规划与建设了很多监控点，美其名曰 立体化监控。 在2014年实现用户舆情监控能力后，我们的监控点做到了100%的覆盖  舆情监控 舆情监控 实现原理图 用户舆情监控顾名思义就是监控用户的声音和反馈。用户的意见反馈来源可以分几部分，一是appstore的入口，另一个是app内嵌的反馈入口，还有的就是腾讯的用户反馈论坛，所有的数据都会被汇集到织云舆情监控平台上，然后通过机器学习实现自动分类。系统会把类似“QQ空间打不开”、“QQ空间不用好”等这些词汇进行语意分析和归类，然后统一告警成“QQ空间异常”，时间间隔是15分钟颗粒度。（技术细节可以参考我在2016年在北京TOP100大会上的分享主题。）\n这套监控先天就有门槛，因为要基于用户的主动反馈行为，同时需要较多的用户反馈数据量，因为腾讯的用户量基数很大，用户主动反馈的量也很大。同时，舆情监控可以用于监控技术上的质量问题，也能用于监控产品的体验或交互问题\n持续运营阶段面临的挑战和解决方案 （监控全面覆盖后，必然带来海量的告警压力，如何解决告警泛滥，是下面的主要议题）  运营阶段解决的难题   繁——简\n 在具体生产过程中会产生运维的事件或者是故障，经常会有死机，以及各层监控告警，这些繁琐的告警、故障，改如何简单化？    泛——精\n 举个案例，在一台核心的交换机下，假设其下联有1000台的机器分布到数据层、逻辑层、接入层等等，当这台交换机故障不可用时，由于有立体化监控的存在，每个监控点都会产生大量的告警信息，我们要如何发现这些告警是由于这台核心交换机故障引起的呢？    乱——序\n 由于指标采集方式和计数据量的不同，直接导致了监控的流处理效率是不一样的，告警收到的次序不一样，我们要如何排序，如何有效识别优先级？    解决方案 理清监控对象与指标的关系 腾讯业务要监控的对象如图，按照业务逻辑从下往上，下面是通用的监控层面，网络、服务器、虚拟化还有应用，应用包括了组件的一些监控。\n基于立体化的监控，假设用组件的监控，无论是QQ还是QQ空间、QQ音乐，都有一些通用的指标可以衡量。如，打开的内存是多少？长连接数是多少？用户进程、吞吐量、流量、CPU，业务层面返回码的分别是什么？省市连接的成功率、请求量的分布是什么？这都与具体的业务逻辑无关\n为了理清海量的监控数据，我们把指标划分成两大类\n  指标划分两大类\n  低层次指标\n 把公共的、基础设施等在业务逻辑之下的指标称之为低层次的指标，网络、硬件、虚拟化等。    高层次指标\n 高层次的指标要能更直接的反馈业务可用性的情况，如成功率、延时、请求率等    遵循的原则\n  在规划监控处理或者优化监控策略时，要尽量把低层次的指标自动化处理或收敛掉，尽量以高纬度的指标来告警\n  高层次的指标，是要能够实时反馈业务的真实状况的，在海量规模的业务运维场景下，靠人没办法看到单机的层面，必须要看到集群的层面\n  在高层次指标中，还要有效的区分开单服务的高层次的指标，和业务功能的高层次的指标\n 善用这些低层次的指标能够帮助运维快速的定位高层次指标的故障原因    可靠性和可用性\n  可靠性是指单个服务失败的次数\n 微服务自身有失败重试的逻辑，需要在可靠性和可用性之间做出一定取舍          监控的本质（指标的归一化处理）\n   收集和计算得到一些值和率，通过一定的分析策略或算法，然后把结论以不同的形式展现，最终达到分析状态和发现异常的目标  海量运营监控需要强调信息的有效性  原因：立体化的监控，会带来监控指标的爆炸，更有可能带来告警数据的失控 如何解决：有效的解决告警多、误告警多  关联分析  把一些真正重要的，需要通过事件、活动、指标提取出来。希望不要把什么事情都告警出来，而过多的消耗告警的诚信   无误告警  怎么样把收敛策略、屏蔽的策略用到极致，必要时可以将两者组合，以达到更强化的效果   持续运营  做好持续运营就是做好跟进，为了保证重要的事情有人跟、有人度量，防止问题再三出现，要在流程上有保障的机制 质量体系来闭环管理  当监控发现业务架构不合理、代码不合理等问题，能够通过该质量体系，推动业务、开发、运维去将优化措施落地，这也是为了最终的商业价值        海量数据收敛的手段 溯源、根因、优选\n溯源分析实践简介   高纬与降维打击。高维与降维打击，把一个指标的结果值或率以不同的纬度展开，要把每一个纬度的指标组合的状态异常都变成告警，这是很不现实的，因为压根处理不过来。反而多维度的指标异常能通过日常的报表汇总分析就能发现的异常，然后通过考核去持续的推动，把异常指标给理顺、优化掉，这是就是高维、降维的打击。\n  **级联分析。**网络有一个词叫微突发，网络突然拥塞了，导致一大波低层次和高层次的告警产生。举个案例，一个交换机异常，引发下联的服务器爆炸式的告警，当此类情况发生，我们的统一告警平台全部不理，做好全局的收敛，以保证监控告警的有效性不受影响。\n  **逆向思维。**意思是不能只看结果数据，要回到原始数据来看。如果要做到逆向思维可生效，那流处理集群在真正加工完，存储的结果数据之前要做最基础的清晰，把那部分日志备份到大数据平台做离线的计算，然后结果数据再走正常的流，去做告警也好，异常波动也好，因为很多异常的东西必须要看到原始数据。我们曾经深入分析相册的日上传照片流水日志，找到了大量异常的用户照片，从而节省了大量的运营成本，这些都是结果数据无法做到的效果。\n  根因分析实践简介   **用高层次的告警收敛低层次的告警。**同一个集群下既产生了低层次的告警，又产生了高层次的告警，低层次的告警不用发。\n  用主调的告警收敛被调的告警。模块A调用模块B，B挂了，A受不受影响？从保障业务可用性的角度，如果A没有产生告警，证明该场景只是B的可靠性告警，告警通知开发而不是运维。但如果B挂了，A也产生了告警，运维就应该收到A的告警，B还是告警给开发。推进告警分级（分值、分级、分人、分通道）的机制，其实是慢慢把一些运维要做的事情分给开发，运维只看核心的，软件可靠性这些开发来看，可靠性是开发的问题，可用性是运营质量的问题。\n  **用原因告警收敛现象告警。**在业务逻辑的调用联调中，用原因告警收敛掉现象告警。（具体可参考2016年3月深圳运维大会上，我关于监控的分享PPT）。\n  **用主动触发的活动去屏蔽一个对象的告警。**有些告警是由于变更的行为引起的，要收敛掉。如正在做升级引发了告警，运维系统要能关联这些事件与告警。有高层次的告警、低层次的告警，还有运维的活动事件，都把这些集中在一起，通过权重的算法，有一个排序决策说告警应该是告这条链路，而不是每一个对象都重复的告警。\n  优选指标实践简介   **核心指标论。**腾讯内部的系统代号叫DLP，是一种通过人工来筛选核心指标的方法。举例，一个模块可能有300－400个指标，这300－400个指标中，包含有低层次的指标，高层次的指标，但当这个模块出问题的时候，这300－400个指标可能都会产生告警，那么应该怎么样收敛呢？倘若我们提前已经对该模块进行过核心指标的人工筛选，这个指标能代表模块最真实的指标。\n  **监控的相关性。**监控与监控之间是相关的，例如300个指标告警了，最核心的那个也会告警，最核心的告警了这300个指标可以不告警，只看核心的就可以了，为什么要人手选核心指标，因为暂时没有办法人工识别。\n  告警分级管理。 可以基于核心的指标对告警做分级，非核心的开发自己收，核心的运维收，高规格保障。\n  降低流试监控的计算量。 监控点越多，流的数据越大，整个监控流处理集群规模很大，10万台机器光是流处理的集群都是接近1500台，当运营成本压力大时，我们也可以重点的保障DLP的指标的优先计算资源，保证优先给予容量的支持。\n  常用（动态阈值）算法与策略简介 在定义指标状态异常时，我们的经验是尽量不要用固定阀值，要用也是动态阀值，否则在监控对象的阀值管理上就会有大量的人工管理的成本。其他的推荐策略如图。 常用收敛算法简介  毛刺收敛  持续多长时间报警，一般建议采集周期的2～3倍   同类收敛  一个模块有300个监控实力，产生了300条的告警，只要有一条告给运维，对于运维同类收敛掉了。   时间收敛  生产环境中有很多定时的任务，如定时跑批会引起I/O的陡增等异常，这种可以针对性的收敛掉。   昼夜收敛  有一些告警，在分布式服务的高可用架构下，晚上不需要告警出来，可以等白天才告警，更人性化的管理。   变更收敛  如果告警的时间点有运维的活动，就要收敛掉它。怎么做到的？取决于要把运维的活动都收口在标准化运维的平台，运维平台对生产环节都要将变更日志写入在变更记录中心那里，然后统一告警系统能够关联变更记录来决策是收敛还是发出告警    织云监控指标体系及质量体系 织云监控的指标体系  织云监控构建的质量体系，分成用户端、客户端、服务端、基础端，定义核心指标 DLP，并且善用分级告警、分渠道告警，结合短信、QQ、微信和电话等渠道实现告警通知，整个质量监控体系都是围绕预警、自愈、分析、排障碍的能力构建  织云监控的质量体系 小结织云监控的质量体系，我们希望打造一个闭环，能够实现持续反馈、度量、优化，让团队间能够有效的协同工作，更高效更有效。\n监控能力。全局的看、需要什么样的监控能力和监控点，同时要理清指标是怎么分层的，哪些指标是重要的？最终把它转成业务看得懂的高层次指标。\n业务可用性。运维要看什么，要看可靠性还是可用性，如果规模不大看可靠性可以，但是在海量的场景下可靠性要太细，要抽象核心指标来度量，用于衡量可用性。可靠性则可以通过考核体系去度量与管理，结合QA和老板的力量来推动开发团队的投入与优化。\n用户体验。做技术运营会有视角的盲点，会经常迷恋可用性的数据是4个9、5个9，但这并不完全代表了我们服务质量好，当用户连接不上我们的服务端时，几个9的意义都不大。这是一个很现实的问题，因此用户体验监控一定要做，因为内部的可用性再高都不代表用户用得好。\n技术解决。要有技术解决的方案，要有自动化的工具，有协助用户排障的工具，有根因分析的算法平台等等。\n统计分析。最终形成可度量的指标、可考核的、可展示的，最好是DIY展示的，监控数据的统计/报表能力服务化，应发挥更多的角色来使用监控数据，而非仅限于运维角色。\n持续改进。最终持续的改进无论是架构的问题、代码的问题，还是因为标准化的问题或非功能落地推进不了的问题，都是需要数字来度量和推动。最好，这个数字要能间接的反馈商业的价值，也就是DevOps提倡的思路。\n相关链接  IT大咖说\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2a2870b032dc5dba4d4a1dd5bbc8770b","permalink":"https://meixinyun.github.io/programmertalk/post/2019/2019-04-14-%E6%9C%89%E6%95%88%E6%9E%84%E5%BB%BA%E6%B5%B7%E9%87%8F%E8%BF%90%E8%90%A5%E7%9A%84%E6%8C%81%E7%BB%AD%E5%8F%8D%E9%A6%88%E8%83%BD%E5%8A%9B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/post/2019/2019-04-14-%E6%9C%89%E6%95%88%E6%9E%84%E5%BB%BA%E6%B5%B7%E9%87%8F%E8%BF%90%E8%90%A5%E7%9A%84%E6%8C%81%E7%BB%AD%E5%8F%8D%E9%A6%88%E8%83%BD%E5%8A%9B/","section":"post","summary":"有效构建海量运营的持续反馈能力 本文来自对大梁先生[DevOp","tags":null,"title":"有效构建海量运营的持续反馈能力","type":"post"},{"authors":null,"categories":"监控体系建设","content":"监控体系建设-全链路监控 定义 全链路监控： 从业务视角出发，能直观反映整个业务流程的健康状态，无需切换系统，即可贯穿全局和上下游，能快速发现，定位问题。\n实现关键 健康状态评估  通过统一数据模型，和查询接口，完成监控对象数据模型的抽象， 通过对指标的归一化处理，抽象出黄金指标，完成对象对象状态特性的抽象 通过对服务稳定性抽象，如SLA，MTTR等为服务打分，完成对服务，系统的抽象。  链路拓扑生成 人工筛选+自动生成    人工筛选\n 类似黄金指标，可以把影响业务的核心流程通过人工的形式梳理出来，形成黄金流程。关注这个黄金流程上的业务活动， 以及核心这些业务活动关联以来有什么（系统，服务）    自动生成\n从三个维度：\n 服务 与 资源的关系： 来自CMDB，服务树。辅助确定故障影响面； 服务 与 服务 的关系： 来自中间件埋点 网络拓扑关系： 来自网络流量分析，网络拓扑    异常检测  智能基线预警 专家规则预警  问题及解决方案 跨部门或者跨BU的链路监控问题 业务全链路路监控通过“黄金指标”+“业务维度”来解决这个难题\n首先每个部门把直接对外提供服务的核心业务梳理出来，并设定它的“黄金指标”，我们设定这样的黄金指标 为这个部门的SLA指标，其它部门的系统调用这个业务时，就看这些SLA的指标是否正常，而不再关心这个业 务的内部调用细节。  每个部门都定义自己的对外核心业务和SLA指标，形成一种标准化的服务能力评价体系，业务全链路可以把各个部门的业务节点串联起来，组成一个更大的大网，在这个里面每个部门都对外暴露的自己的关键业务，每个关键业务通过“黄金指标”量化服务能力，这样就可以快速定位到底是谁的问题，建立起一个正在的全景式监控\n业务全链路的精确的定位 精准定位两类：\n 鹰眼系统，原理和Dapper类似，主要做系统Trace链路排查 阿里的A3系统： 日志的异常诊断的一个工具，  业务全链路的异常检测 业务活动的监控指标分为三层\n 第一层是这个业务活动的黄金指标，对第一层的指标我们会追求它的准确率，会为每一个指标建立单独的预测模型，然后通过时间序列等算法，尽可能做到对这个指标的准确判断以及报警。 第二层是这个业务活动依赖各系统服务的监控指标，对于这一层指标我们采用的准确性与成本和效率均衡的策略，通过一些轻量级波动检测算法来实现异常检测，而且这一层次的检测是由上一层触发，只有在上一层检测发现异常时才触发，不会一直定时执行。 第三层是这个业务活动所在应用的监控指标，包括他整个系统调用链路上的各种应用指标，包括系统指标、各类中间件、缓存、数据库等等，这一层的数据量是最大的，我们不会直接用智能算法分析这些指标，而是分析它们产生的各类报警事件和变更事件  异常检测分析流程：\n AI时代的全链路监控，阿里工程师怎么做\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd46de947ebeb5eb5ec0b4e9555abaca","permalink":"https://meixinyun.github.io/programmertalk/post/2019/2019-11-02-%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE-%E5%85%A8%E9%93%BE%E8%B7%AF%E7%9B%91%E6%8E%A7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/post/2019/2019-11-02-%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE-%E5%85%A8%E9%93%BE%E8%B7%AF%E7%9B%91%E6%8E%A7/","section":"post","summary":"监控体系建设-全链路监控 定义 全链路监控： 从业务视角出发，能直","tags":null,"title":"监控体系建设-全链路监控","type":"post"},{"authors":null,"categories":"监控体系建设 演化历程 标准化","content":"监控体系建设-演化历程及标准化 目标 本文，结合工作的实践以及他人的分享，整理了监控系统建设中的若干问题以及对应的解决方案。\n本文逻辑 监控系统进化一般路径 监控系统如何标准化 小结  监控系统进化的一般路径 如同人类的进化史，大部分公司都会经历 工具时代，平台化，标准化，智能化的过程。\n工具化时代 聚焦的问题： 解决有没有的问题。\n代表： 基于开源的监控系统如，Nagios+Cacti / zabbix 等。\n存在的问题：\n 1. 无法定制。投入人力和资源都比较少，无法对这些组件和系统深入的二次开发或定制。和 公司的运维体系不能融为一体。 2. 不能规模化. 随着公司业务体量的增加，监控的规模上来后，性能，稳定性，产品易用性 等问题表现突出，无法系统的解决。  平台化时代 聚焦的问题：解决性能，稳定性，以及和运维体系整合联动。\n代表：： 如小米开源的open-falcon, 阿里的 alimonitor，鹰眼 等\n存在的问题：\n 1. 监控的全面覆盖，必然带来监控指标的爆炸性增长，带来了监控告警的泛滥。 2. 监控自定义的东西就很多，标准化的程度低，监控的运维管控就很困难，进一步的数据分析挖掘就更难做了 3. 产品的专业性比较强，用户的要求比苛刻，复杂程度高  标准化时代 聚焦的问题：解决监控指标爆炸，监控指标的标准化，业务全链路，以及服务质量评估体系建立。\n代表： 阿里的Sunfire监控平台，prometheus+thanos ， 更多的是结合公司的业务层场景， 平台+标准+人工标注。\n智能化 聚焦的问题：\n 发现问题： 异常捕获（时间序列的异常，操作日志异常等） 定位问题： RCA，根因溯源，推荐，事件关联分析等  代表：阿里的智能诊断平台，skyline, Prophet, 腾讯Metis, 以及裴丹老师的相关理论研究\n存在的问题：\n 这部分还处于探索阶段，大部分公司都处于弱智能时代。  监控系统如何标准化 监控系统的标准化的最终目的，是为业务全链路监控打好基础。全链路监控的目的是为了快速发现问题，定位问题。\n标准化的目标及意义 我理解的标准化，主要来自三个维度：数据模型层和指标规范层，业务规范层\n数据模型层： 指的是建立一种通用的数据模型，能完全覆盖各个监控层级的业务需求，同时能提供一种标准的监控查询语法，满足 各个业务的定制化分析，聚合等需求。 典型的解决方案： 如prometheus 的PROMQL, 和 阿里基于HiTsdb  指标规范： 四个黄金指标，资源类型的通用指标 USE，业务类型的：RED 等 指的是对 监控指标的归一化处理。比如，google SRE 解密中提出的四大黄金指标， 流量 ：业务在单位时间内的调用量，如：服务的QPS、每秒订单笔数等。 耗时 ：业务的具体处理时长，需区分成功耗时和失败耗时。 错误 ：调用出错数量、成功率、错误码。 饱和度 ：应用已使用资源的占比。 Weave Cloud在基于Google的“4个黄金指标”的原则下结合Prometheus以及Kubernetes容器实践，细 化和总结 得出的：RED方法 (请求)速率：服务每秒接收的请求数。 (请求)错误：每秒失败的请求数。 (请求)耗时：每个请求的耗时。  USE方法全称”Utilization Saturation and Errors Method”，主要用于分析系统性能问题，可以指导用户快速识别资源瓶颈以及错误的方法。\n使用率：关注系统资源的使用情况。 这里的资源主要包括但不限于：CPU，内存，网络，磁盘等等。100% 的使用率通常是系统性能瓶颈的标志。 饱和度：例如CPU的平均运行排队长度，这里主要是针对资源的饱和度(注意，不同于4大黄金信号)。任何 资源在某种程度上的饱和都可能导致系统性能的下降。 错误：错误计数。例如：“网卡在数据包传输过程中检测到的以太网网络冲突了14次”。  业务规范 为解决业务全链路分析而建立业务监控模型\n业务域：一个完整的业务或产品称为“业务域”，如电商的“交易域”、“营销域”、“支付域”等。 业务活动：业务域中的的核心业务用例叫做“业务活动”，如交易域的“下单确认”、“创建订单”等，业务活 动是整个监控模型的核心，每个业务活动都会有标准的【黄金指标】来反应自身的健康状况，业务活动之 间建立上下游关系就形成了业务链路。 系统服务：业务活动中的依赖的关键方法称作“系统服务”，如“下单确认”包含：查询会员、查询商品、查询 优惠等关键方法，每个系统服务也通过【黄金指标】来表示其健康状况。  小结 本文梳理了监控体系建设的进化历程，以及如何实现标准化。后续会介绍如何给予标准化，实现全链路业务监控。  参考文献  阿里业务全链路智能监控探索\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"76bdcf702c33712460253b32db8c4e3f","permalink":"https://meixinyun.github.io/programmertalk/post/2019/2019-11-01-%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE-%E8%BF%9B%E5%8C%96%E5%8E%86%E7%A8%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/post/2019/2019-11-01-%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE-%E8%BF%9B%E5%8C%96%E5%8E%86%E7%A8%8B/","section":"post","summary":"监控体系建设-演化历程及标准化 目标 本文，结合工作的实践以及他","tags":null,"title":"监控体系建设-演化历程及标准化","type":"post"},{"authors":null,"categories":"运维体系建设 学习笔记","content":"目标  通过学习蓝鲸体系架构及设计思想，扩展自己在运维体系建设领域的认知。  学习重点 蓝鲸体系的背景，设计理念，和落地方式  设计背景 运维转型历程 十年前 ,需求驱动型的, 业务运维忙于:\n 服务器、网络、OS、DB、发布、变更、监控、故障处理、运营环境信息维护提取等等。  五年前, 使我们的运维团队从**“操作服务输出”**，转型为**“解决方案服务输出” **\n三年前，2012年，作为落地承载运维转型方案的蓝鲸体系同时开始构建\n 为什么要转型：  原因1：业务红海化\n 1. 行业竞争很激烈，精细化运营越来越重要，希望业能将务可用性保持在无限接近7*24 2. 产品策划运营等其它岗位提供各类运营工具以提高“产品运营”的效率，甚至能为运营决策提供准确的 数据依据  原因2：“传统运维”生存空间塌缩\n原因3：我们太累了\n运维转型的长远目标  将基础运维服务（发布变更、监控处理、数值调整、数据提取等）尽可能做到运维无人 值守，运维提供解决方案（工具） 同时负责随时调整解决方案，但不提供重复性的操作服务，由使用者自助或者外包团队操作  设计理念 面临的挑战  业务类型复杂 技术框架多样 发布变更、故障处理等运维操作场景和操作流程是没有直观规律 服务器数量，操作单元量庞大，增长速度很快  需求分析  不能侵入业务架构，不能依赖业务架构，不能依赖业务所使用的技术，不能依赖 有统一的运维操作流程 甚至，也最好别指望开发商为你做什么改造，还得支持海量场景（最好能支持十万 级操作单元并发）  总结出来的共同点是：\n运维通过linux命令，可以搞定所有“发布变更故障处理等”运维操作流程。\n 虽然只有这一点，但也足够了，这至少说明，运维的基础服务，不论是发布变更还是告警处理， 都是可以分步骤的，步骤可能是串行的，也可能有分支结构  设计思路 尽可能将单个步骤抽象为原子，再将原子自动化，而后通过任务引擎连接成“串”或者“树状分支结构”实现全自动化**\n 设计思路的优点：不依赖业务类型，不依赖架构，不依赖场景，只要运维手工能做的，都可以做 成无人值守  落地方式 将原子自动化 运维通过命令可以做的步骤，在蓝鲸作业平台上封装个脚本，就变成了可集成的自动化原子，而运维 通过其他运营系统页面操作的步骤，由蓝鲸集成平台中的ESB平台与其对接好接口之后，也变成了可 集成的自动化原子  将原子集成为工具 运维/运营工具的开发对传统运维是有一定障碍的，蓝鲸通过几方面的工作来解决这个问题\n 1. 在“蓝鲸集成平台”（蓝鲸体系目前有6个平台）中构建了一个PaaS模块，业务运维（devops）无需关 注找服务器、部署环境（各种包、mysql、nginx等）等步骤，只需要写好工具本身的逻辑代码上装 到PaaS容器就行了，同时还免除了工具的运维成本（高可用、故障修复等）。基于docker技术，工具 的部署也是一键式的 2. 其次是开发了一套工具代码框架，内置了统一登录、权限、tag等通用功能，更重要的是，不需要一个一个 去对接各个系统的接口（原子），因为ESB模块都封装好了，只要写个函数就可以调用这些原子 3. 再有就是解决运维的前端开发难题——前端样例库。提供了“从各种页面元素到不同类型的运维工具的页面组合 套餐”，减少了运维消耗在前端开发上的时间 4. 最后，还为蓝鲸开发者提供培训，一般来说，新进毕业生在通过四周以内的培训之后，就可以独立在蓝鲸集 成平台中构建APP工具  通过以上方案，基本解决了运维构建工具高门槛的问题，而且可以随时低成本的根据业务变化（例如新增了模块，导致发布变更、告警处理流程都变了）调整工具\n进化方向 在这种设计模式下，蓝鲸团队的建设方向就很清晰了：\n1. 继续降低工具本身的开发成本，提高PaaS模块的可靠性； 2. 扩展原子服务，找出运维海量操作流程中，重复度最高的一些原子，构建成平台，封装接口提供给PaaS作 为自动化原子，让运维更轻松的调度更多节点，提升单个节点功能密度，升级拓展更多的流程，直到把流程升 级到运维无人值守，升级到对产品、策划等岗位的闭环服务为止。  进化成果   蓝鲸集成平台：包含PaaS、ESB、开发框架、web样例等模块，是运维制作工具APP的平台。\n  蓝鲸移动平台：蓝鲸体系的移动端操作入口。\n  蓝鲸作业平台：各种大小文件传输，含参脚本执行类的动作，可以在蓝鲸作业平台封装，通过接口操控。\n  蓝鲸配置平台：从业务的各层分级结构到子节点的各类属性，都可以直观的存储于蓝鲸配置平台，通过接口存取。\n  蓝鲸管控平台：一套基于海量标准设计的管控系统，为作业平台提供文件管道和任务管道，为数据平台提供数据管 道等，整个蓝鲸体系对OS及容器单元、大数据的所有管控，只依赖管控平台的一个智能agent。\n  蓝鲸数据平台：基于kafka、storm构建的供应用运维使用的实时计算平台，为上层蓝鲸集成平台上的智能决策 类工具族、数据视图类工具族、辅助决策类工具族提供大数据处理及实时计算能力\n  运维团队的进化历程 运维基础工作自动化 辅助产品运营自动化 数据化运维 蓝鲸服务 蓝鲸的服务可以分成两类：PaaS和SaaS。\n对应用运维来说，PaaS服务是万能的，几乎没有场景限制，只要是原子能覆盖的流程，都能做得出来，非常灵活\n蓝鲸大力发展PaaS服务，也印证了运维发展的理念：\n依靠运维，武装运维，使其能提供更高维度的服务，而不是取代运维\n蓝鲸平台系统架构 腾讯蓝鲸智云体系由原子平台和通用的一级 SaaS 服务组成，平台包括 管控平台、配置平台、作业平台、数据平台、容器管理、PaaS 平台、移动平台 等，通用 SaaS 包括节点管理、标准运维、日志检索、蓝鲸监控、故障自愈等，为各种云（公有云、私有云、混合云）的用户提供不同场景、不同需求的一站式技术运营解决方案。\n参考文章  腾讯蓝鲸体系架构及设计思想\n 蓝鲸文档中心 \u0026gt; 蓝鲸体系 \u0026gt; 体系架构\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"42ca74848b31d275ab3e6e7ead8cd64e","permalink":"https://meixinyun.github.io/programmertalk/post/2020/2020-04-13-note/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/post/2020/2020-04-13-note/","section":"post","summary":"目标 通过学习蓝鲸体系架构及设计思想，扩展自己在运维体系建设领","tags":null,"title":"蓝鲸体系架构及设计思想学习笔记","type":"post"},{"authors":null,"categories":"学习笔记","content":"进程，线程，协程详解 目标 彻底弄清楚进程，线程，协程。\n本文规划  概念简述 进程，线程的联系 ，区别 （是什么） 为什么引入多线程模型（为什么） 多线程的实现模型（怎么样） 协程是什么？为什么引入？如何实现？ 总结  概念简述 并发：某个时刻，同时处理N个任务 并行：N个Worker，同时执行N个任务 注意： 在没有并行能力机器上使用并发，实际上会降低你的性能。 比如 只有1个核，同时处理N个计算任务，存在上下文切换的损耗  进程：正在执行程序的实例。操作系统资源调度的最小单位。每个进程都有一个地址空间，和控制线程。 线程: 是进程的一个实体,是CPU调度和分派的基本单位。和进程共享地址空间，准并行的运行多个控制线程 进程，线程的区别与联系 进程：进程模型基于两种独立的概念：资源分组处理和执行。有些情况，将两种概念分开会更有益，这也是引入“线程”这一概念的原因。\n 理解进程的另一个角度，用某种方法把相关的资源集中在一起。  进程与线程的联系 进程与线程的关系模型如图(Figure2-11)\n图（a）, 三个进程，每个进程有一个线程； 图（b）一个进程中，有三个线程。 从图上可以清晰看出，进程和线程之间的联系。\n进程与线程的区别 进程与线程的区别，表（Figure 2-12） 为什么引入多线程模型 为什么在一个进程引入多个线程\n有若干理由，如下\n1. 许多应用中同时并发着多种活动，某些活动随着时间推移会被阻塞。将应用程序分解成可以准并行运行的多个顺序线程，会使得 程序设计模型简单。 2. 在有了多进程模型的抽象之后，我们才不必考虑中断，定时器和上下文切换，只需考察并行进程。类似，在有了多线程模型后，才 能引入一种新的元素：并行实体共享同一个地址空间，和所有可用数据的能力。对某些应用而言，这种能力是必须的，而这正是多 进程模型（具有不同地址空间）所无法表达的 3. 线程比进程更轻量级，比进程更容易创建，销毁。在许多系统中，创建一个线程较创建一个进程要快（10～100）倍。 对于IO密集型的，拥有多个线程，可以让彼此并发执行 4. 在多核（CPU）系统中，多线程模型是有益的。  多线程模型要解决什么问题？\n 多线程使得顺序进程的思想得以保留，这种顺序进程阻塞了系统调用，但是仍然实现了并行性。对系统调用进行阻塞时程序设计变 得简单，且并行性改善了性能。 单线程服务器虽然保留了阻塞系统调用的简单性，但却放弃了性能。通过有限状态机的方法，运用非阻塞调用和中断，通过并行 性实现高性能，但给编程增加了困难。  多线程的实现模型（怎么样） 有两种主要方法实现线程包：用户空间中和在内核中。由于这两种方法，互有利弊，以至出现了混合实现方式。为了便于区分，我们可分为三种线程模型：\n 用户级线程模型 内核级线程模型 混合线程模型  用户级线程模型 与 内核级线程模型 用户空间中实现线程： 把整个线程包放在用户空间中，内核对线程包一无所知。从内核的角度考虑，是按照正常的方式管理，即单线程进程。 用户空间管理线程如图 2-16（a), 每个进程需要有其专属的的线程表（thread table）,用来追踪该进程中的线程。这些线程表和内核中 的进程表类似，不过他仅仅记录各个线程的属性，如每个线程的程序计数器，堆栈指针，寄存器和状态等。该线程表由运行时系统管理。 用户空间中实现线程的优点： 1. 用户级线程包可以在不支持线程的操作系统上实现 2. 用户空间的线程切换比陷入内核切换要快一个数量级（或更多） 当某个线程引起本地阻塞后，它调用一个运行时系统过程，该过程检查该线程是否必须进入阻塞状态。如果是，它在线程表中保存 该线程的寄存器（即它本身的），查看表中可运行的就绪线程，并把新的线程的保存值重新装入机器的寄存器中。只要堆栈指针 和程序计数器一被切换，新的线程就又自动投入运行。如果机器有一条保存所有寄存器的指令和另一条装入全部寄存器的指令， 那么整个线程的切换可以在几条指令内完成。 3. 允许每个进程有自己定制的调度算法。 用户级线程具有较好的扩展性，因为在内核空间中内核线程需要一些固定表格空间和堆栈空间，如果内核线程数量非常大，就会 出问题。 用户级线程的问题： 第一：如何实现阻塞系统调用。 由于用户线程包对内核是透明的，用户的任何一个线程进行阻塞系统调用，会导致该进程下的所有线程被阻塞。使用线程的一个主要目标 是，首先要允许每个线程使用阻塞调用，但是还要避免被阻塞的线程影响其他的线程。 第二：如果一个线程开始运行，那么在该进程中的其他线程就不能运行，除非第一个线程自动放弃CPU。 单独的进程内部，没有时钟中断，不能用轮转调度的方式调度进程。 第三：与阻塞系统调用问题类似的缺页故障  内核级线程模型：在内核中记录系统中所有线程的线程表。当某个线程希望创建或销毁一个线程时，进行一个系统调用，这个系统调用通过对线程表的更新完成线程的创建和销毁。如图2-16 (b) 内核线程的实现原理：内核的线程表保存了每个线程的寄存器，状态和其他信息。这些信息和在用户空间中（运行时系统）的线程是一样的，但是现 在保存在内核中。这些信息是传统内核所维护的每个单线程进程信息的子集。另外，还维护了传统的进程信息。 内核线程的优势： 1. 所有能够阻塞线程的调用都以系统调用的形式出现。 当一个线程阻塞时，内核可以选择调用同一个进程的另一个线程（若有一个就绪线程），或者运行另外一个进程中的线程。相比，在用户空 线程中，运行时系统只能运行自己进程内的线程，知道内核剥夺它的CPU（或没有可运行的线程）为止。 2. 内核中不需要任何新的，非阻塞的系统调用。当出现阻塞和缺页故障时候，可以很方便的检查该进程是否有任何其他可以运行的线程。 内核线程的问题： 在内核中创建和销毁现场的代价比较大。 （线程池，控制线程的回收和循环利用，是一种有效降低线程开销的方法） 内核线程模型业引入了其他更多的复杂问题： 在一个多线程进程创建新进程时，新进程是否拥有和原进程相同数量的线程还是只有一个线程？ 如果多个线程都注册了一个信号，当信号到达时，如何调度线程？  混合线程模型 为了结合用户级线程和内核级线程的优势，就出现了混合线程模型。该模型如图 2-17  该模型，是用内核级线程，将用户级线程与某些或着全部内核线程多路复用起来。编程人员可以决定有多少个内核级线程和多少个用户级线程彼此多路复用。\n 优势：内核只识别内核级线程，对其调度。每个内核级线程，有一个可以轮流使用的用户基级线程集合。  混合线程模型需要解决一个核心的问题是：如果线程阻塞在某个系统调用或页面故障上，只要同一个进程中有任何就绪的线程，就应该有可能运行其他的线程。避免用户空间和内核空间不必要的切换，从而提升效率。\n实现机制： 调度程序激活机制 内核给每个进程安排一定数量的虚拟处理器，并且让（用户空空间）运行时系统将线程分配到处理器上 当内核了解到一个线程被阻塞之后（例如：由于执行了一个阻塞系统调用或产生了一个页面故障），内核通知该进程的运行时系统，并且在 堆栈中以参数形式传递有问题的线程编号和发生事件的一个描述。内核通过一个已知的起始地址启动运行时系统，从而发出了通知，这是 对UNIX中信号的一种粗略模拟。这个机制称之为 上行调用 一旦如此激活，运行时系统就重新调度其线程，这个过程通常是这样： 把当前线程标记为阻塞并从就绪表中取出另一个线程，设置其寄存器，然后在启动之。稍后，当内核知道原来的线程有可云心时（例如：原 先试图读取的管道中又了数据，或已从磁盘中读入了故障的页面），内核就一次又上行调用运行时系统，通知它这一件事情。此时，该运行 时系统按照自己的判断，或者立即重启被组赛的线程，或者把它放入就绪表中稍后运行。  协程 协程定义 V1: 协同程序是一种计算机程序组件，它通过允许暂停和恢复执行，将子程序泛化以实现非抢占式的多任务处理。\nCoroutines are computer program components that generalize subroutines for non-preemptive multitasking, by allowing execution to be suspended and resumed  V2: 协程是一个函数，它可以暂停执行以在以后恢复，是无堆栈的。它通过返回调用方来暂停执行，恢复执行需要的数据与堆栈分开存储。使得可以进行异步执行顺序代码\nA coroutine is a function that can suspend execution to be resumed later. Coroutines are stackless: they suspend execution by returning to the caller and the data that is required to resume execution is stored separately from the stack. This allows for sequential code that executes asynchronously (e.g. to handle non-blocking I/O without explicit callbacks), and also supports algorithms on lazy-computed infinite sequences and other uses  我理解 协程 本质是一种 通过顺序代码来实现异步执行的函数机制。在降低线程切换带来开销的同时，通过系统并行执行来提升执行效率。\n参考文章：\n 现在操作系统\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4816c623ce4cb5f69d107555bbb776c7","permalink":"https://meixinyun.github.io/programmertalk/post/2016/2016-06-01-%E8%B0%83%E5%BA%A6%E5%99%A8%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/programmertalk/post/2016/2016-06-01-%E8%B0%83%E5%BA%A6%E5%99%A8%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","section":"post","summary":"进程，线程，协程详解 目标 彻底弄清楚进程，线程，协程。 本文规划","tags":["计算机编程 操作系统"],"title":"进程，线程，协程详解","type":"post"}]